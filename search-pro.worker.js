const nt="ENTRIES",V="KEYS",T="VALUES",p="";class D{constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=z(this._path);if(z(s)===p)return{done:!1,value:this.result()};const n=t.get(z(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=z(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>z(t)).filter(t=>t!==p).join("")}value(){return z(this._path).node.get(p)}result(){switch(this._type){case T:return this.value();case V:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const z=e=>e[e.length-1],ot=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return W(e,t,s,n,i,1,o,""),n},W=(e,t,s,n,o,u,i,r)=>{const h=u*i;t:for(const c of e.keys())if(c===p){const d=o[h-1];d<=s&&n.set(r,[e.get(c),d])}else{let d=u;for(let l=0;l<c.length;++l,++d){const m=c[l],f=i*d,g=f-i;let a=o[f];const F=Math.max(0,d-s-1),y=Math.min(i-1,d+s);for(let _=F;_<y;++_){const b=m!==t[_],E=o[g+_]+ +b,A=o[g+_+1]+1,w=o[f+_]+1,L=o[f+_+1]=Math.min(E,A,w);L<a&&(a=L)}if(a>s)continue t}W(e.get(c),t,s,n,o,d,i,r+c)}};class C{constructor(t=new Map,s=""){this._size=void 0,this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=M(n);for(const i of o.keys())if(i!==p&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ut(this._tree,t)}entries(){return new D(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ot(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(p):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(p)}keys(){return new D(this,V)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(p,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(p,s(n.get(p))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let o=n.get(p);return o===void 0&&n.set(p,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==p&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==p&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==p&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const h=e.get(u);if(r===u.length)e=h;else{const c=new Map;c.set(u.slice(r),h),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ut=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(p),s.size===0)$(n);else if(s.size===1){const[o,u]=s.entries().next().value;R(n,o,u)}}},$=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)$(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==p&&R(e.slice(0,-1),n,o)}},R=(e,t,s)=>{if(e.length===0)return;const[n,o]=M(e);n.set(o+t,s),n.delete(o)},M=e=>e[e.length-1],it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,B="or",q="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},N=({score:e},{score:t})=>t-e,lt=()=>new Map,k=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[B]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),P(n.terms,u)}}return e},[q]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);P(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d:h}=u;return Math.log(1+(s-t+.5)/(t+.5))*(h+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},ft={k:1.2,b:.7,d:.5},gt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof(console==null?void 0:console[e])=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:B,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:ft},Ft={combineWith:q,prefix:(e,t,s)=>t===s.length-1},mt={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},pt={...mt,...U};class _t{constructor(t){if((t==null?void 0:t.fields)==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?pt:t.autoVacuum;this._options={...gt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const yt=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},At=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},K=(e,t=B)=>{if(e.length===0)return new Map;const s=t.toLowerCase();return e.reduce(ht[s])||new Map},S=(e,t,s,n,o,u,i,r,h=new Map)=>{if(o==null)return h;for(const c of Object.keys(u)){const d=u[c],l=e._fieldIds[c],m=o.get(l);if(m==null)continue;let f=m.size;const g=e._avgFieldLength[l];for(const a of m.keys()){if(!e._documentIds.has(a)){At(e,l,a,s),f-=1;continue}const F=i?i(e._documentIds.get(a),s,e._storedFields.get(a)):1;if(!F)continue;const y=m.get(a),_=e._fieldLength.get(a)[l],b=dt(y,f,e._documentCount,_,g,r),E=n*d*F*b,A=h.get(a);if(A){A.score+=E,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else h.set(a,{score:E,terms:[t],match:{[s]:[c]}})}}return h},Ct=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((a,F)=>({...a,[F]:G(n.boost,F)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:h}=n,{fuzzy:c,prefix:d}={...J.weights,...i},l=e._index.get(t.term),m=S(e,t.term,t.term,1,l,o,u,h);let f,g;if(t.prefix&&(f=e._index.atPrefix(t.term)),t.fuzzy){const a=t.fuzzy===!0?.2:t.fuzzy,F=a<1?Math.min(r,Math.round(t.term.length*a)):a;F&&(g=e._index.fuzzyGet(t.term,F))}if(f)for(const[a,F]of f){const y=a.length-t.term.length;if(!y)continue;g==null||g.delete(a);const _=d*a.length/(a.length+.3*y);S(e,t.term,a,_,F,o,u,h,m)}if(g)for(const a of g.keys()){const[F,y]=g.get(a);if(!y)continue;const _=c*a.length/(a.length+y);S(e,t.term,a,_,F,o,u,h,m)}return m},X=(e,t,s={})=>{if(typeof t!="string"){const d={...s,...t,queries:void 0},l=t.queries.map(m=>X(e,m,d));return K(l,d.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:h}=i,c=r(t).flatMap(d=>h(d)).filter(d=>!!d).map(at(i)).map(d=>Ct(e,d,i));return K(c,i.combineWith)},Y=(e,t,s={})=>{const n=X(e,t,s),o=[];for(const[u,{score:i,terms:r,match:h}]of n){const c=r.length,d={id:e._documentIds.get(u),score:i*c,terms:Object.keys(h),match:h};Object.assign(d,e._storedFields.get(u)),(s.filter==null||s.filter(d))&&o.push(d)}return o.sort(N),o},zt=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Y(e,t,s)){const r=i.join(" "),h=n.get(r);h!=null?(h.score+=u,h.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:h}]of n)o.push({suggestion:u,terms:r,score:i/h});return o.sort(N),o},Et=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:h,serializationVersion:c},d)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const l=new _t(d);l._documentCount=t,l._nextId=s,l._documentIds=k(n),l._idToShortId=new Map,l._fieldIds=o,l._fieldLength=k(u),l._avgFieldLength=i,l._storedFields=k(r),l._dirtCount=h||0,l._index=new C;for(const[m,f]of l._documentIds)l._idToShortId.set(f,m);for(const[m,f]of e){const g=new Map;for(const a of Object.keys(f)){let F=f[a];c===1&&(F=F.ds),g.set(parseInt(a,10),k(F))}l._index.set(m,g)}return l},Q=Object.entries,wt=Object.fromEntries,j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,d=!1)=>{let l="";i===0?l=c.length>20?`… ${c.slice(-20)}`:c:d?l=c.length+i>100?`${c.slice(0,100-i)}… `:c:l=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,l&&o.push(l),i+=l.length,d||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let h=s.indexOf(n,u);if(h===-1)return null;for(;h>=0;){const c=h+n.length;if(r(e.slice(u,h)),u=c,i>100)break;h=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},Z=/[\u4e00-\u9fa5]/g,tt=(e={})=>({fuzzy:.2,prefix:!0,processTerm:t=>{const s=t.match(Z)||[],n=t.replace(Z,"").toLowerCase();return n?[n,...s]:[...s]},...e}),et=(e,t,s={})=>{const n={};return Y(t,e,tt({boost:{h:2,t:1,c:4},...s})).forEach(o=>{const{id:u,terms:i,score:r}=o,h=u.includes("@"),c=u.includes("#"),[d,l]=u.split(/[#@]/),{contents:m}=n[d]??={title:"",contents:[]};if(h)m.push([{type:"customField",key:d,index:l,display:i.map(f=>o.c.map(g=>j(g,f))).flat().filter(f=>f!==null)},r]);else{const f=i.map(g=>j(o.h,g)).filter(g=>g!==null);if(f.length&&m.push([{type:c?"heading":"title",key:d,...c&&{anchor:l},display:f},r]),"t"in o)for(const g of o.t){const a=i.map(F=>j(g,F)).filter(F=>F!==null);a.length&&m.push([{type:"text",key:d,...c&&{anchor:l},display:a},r])}}}),Q(n).sort(([,o],[,u])=>u.contents.reduce((i,[,r])=>i+r,0)-o.contents.reduce((i,[,r])=>i+r,0)).map(([o,{title:u,contents:i}])=>{if(!u){const r=yt(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},st=(e,t,s={})=>zt(t,e,tt(s)).map(({suggestion:n})=>n),v=wt(Q(JSON.parse("{\"/en/\":{\"documentCount\":114,\"nextId\":114,\"documentIds\":{\"0\":\"v-2d0a870d\",\"1\":\"v-2d0a870d@2\",\"2\":\"v-5aa3d8ba\",\"3\":\"v-367b840a\",\"4\":\"v-367b840a@2\",\"5\":\"v-395cd082\",\"6\":\"v-395cd082#catalog\",\"7\":\"v-395cd082@0\",\"8\":\"v-395cd082@2\",\"9\":\"v-70eda030\",\"10\":\"v-70eda030@0\",\"11\":\"v-70eda030@1\",\"12\":\"v-70eda030@2\",\"13\":\"v-3777b6d3\",\"14\":\"v-3777b6d3@0\",\"15\":\"v-3777b6d3@1\",\"16\":\"v-4a2a37eb\",\"17\":\"v-4a2a37eb#markdown-introduction\",\"18\":\"v-4a2a37eb#markdown-config\",\"19\":\"v-4a2a37eb#markdown-extension\",\"20\":\"v-4a2a37eb#vuepress-enhancement\",\"21\":\"v-4a2a37eb#theme-enhancement\",\"22\":\"v-4a2a37eb#custom-container\",\"23\":\"v-4a2a37eb#tabs\",\"24\":\"v-4a2a37eb#code-tabs\",\"25\":\"v-4a2a37eb#superscript-and-subscript\",\"26\":\"v-4a2a37eb#align\",\"27\":\"v-4a2a37eb#attrs\",\"28\":\"v-4a2a37eb#footnote\",\"29\":\"v-4a2a37eb#mark\",\"30\":\"v-4a2a37eb#tasklist\",\"31\":\"v-4a2a37eb#image-enhancement\",\"32\":\"v-4a2a37eb#card\",\"33\":\"v-4a2a37eb#chart\",\"34\":\"v-4a2a37eb#echarts\",\"35\":\"v-4a2a37eb#flowchart\",\"36\":\"v-4a2a37eb#mermaid\",\"37\":\"v-4a2a37eb#tex\",\"38\":\"v-4a2a37eb#include-files\",\"39\":\"v-4a2a37eb#code-demo\",\"40\":\"v-4a2a37eb#stylize\",\"41\":\"v-4a2a37eb#playground\",\"42\":\"v-4a2a37eb#vue-playground\",\"43\":\"v-4a2a37eb#presentation\",\"44\":\"v-4a2a37eb@0\",\"45\":\"v-4a2a37eb@1\",\"46\":\"v-4a2a37eb@2\",\"47\":\"v-0e4acecb\",\"48\":\"v-0e4acecb#page-information\",\"49\":\"v-0e4acecb#page-content\",\"50\":\"v-0e4acecb#page-structure\",\"51\":\"v-0e4acecb@0\",\"52\":\"v-0e4acecb@1\",\"53\":\"v-0e4acecb@2\",\"54\":\"v-fb852992\",\"55\":\"v-fb852992#heading-2\",\"56\":\"v-fb852992#heading-3\",\"57\":\"v-fb852992@0\",\"58\":\"v-fb852992@1\",\"59\":\"v-4fd051a1\",\"60\":\"v-4fd051a1#heading-2\",\"61\":\"v-4fd051a1#heading-3\",\"62\":\"v-4fd051a1@0\",\"63\":\"v-4fd051a1@1\",\"64\":\"v-57615dc1\",\"65\":\"v-57615dc1#heading-2\",\"66\":\"v-57615dc1#heading-3\",\"67\":\"v-57615dc1@0\",\"68\":\"v-57615dc1@1\",\"69\":\"v-285adf66\",\"70\":\"v-285adf66#heading-2\",\"71\":\"v-285adf66#heading-3\",\"72\":\"v-285adf66@0\",\"73\":\"v-285adf66@1\",\"74\":\"v-58aa03b4\",\"75\":\"v-58aa03b4#heading-2\",\"76\":\"v-58aa03b4#heading-3\",\"77\":\"v-58aa03b4@0\",\"78\":\"v-58aa03b4@1\",\"79\":\"v-55405276\",\"80\":\"v-55405276#heading-2\",\"81\":\"v-55405276#heading-3\",\"82\":\"v-55405276@0\",\"83\":\"v-55405276@1\",\"84\":\"v-51d6a138\",\"85\":\"v-51d6a138#heading-2\",\"86\":\"v-51d6a138#heading-3\",\"87\":\"v-51d6a138@0\",\"88\":\"v-51d6a138@1\",\"89\":\"v-4e6ceffa\",\"90\":\"v-4e6ceffa#heading-2\",\"91\":\"v-4e6ceffa#heading-3\",\"92\":\"v-4e6ceffa@0\",\"93\":\"v-4e6ceffa@1\",\"94\":\"v-e748286e\",\"95\":\"v-e748286e#heading-2\",\"96\":\"v-e748286e#heading-3\",\"97\":\"v-e748286e@0\",\"98\":\"v-e748286e@1\",\"99\":\"v-e3de7730\",\"100\":\"v-e3de7730#heading-2\",\"101\":\"v-e3de7730#heading-3\",\"102\":\"v-e3de7730@0\",\"103\":\"v-e3de7730@1\",\"104\":\"v-e074c5f2\",\"105\":\"v-e074c5f2#heading-2\",\"106\":\"v-e074c5f2#heading-3\",\"107\":\"v-e074c5f2@0\",\"108\":\"v-e074c5f2@1\",\"109\":\"v-dd0b14b4\",\"110\":\"v-dd0b14b4#heading-2\",\"111\":\"v-dd0b14b4#heading-3\",\"112\":\"v-dd0b14b4@0\",\"113\":\"v-dd0b14b4@1\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2,30],\"1\":[null,null,2],\"2\":[2,7],\"3\":[2],\"4\":[null,null,2],\"5\":[2],\"6\":[1,8],\"7\":[null,null,1],\"8\":[null,null,2],\"9\":[4,40],\"10\":[null,null,1],\"11\":[null,null,1],\"12\":[null,null,4],\"13\":[2,10],\"14\":[null,null,1],\"15\":[null,null,1],\"16\":[2,32],\"17\":[2,19],\"18\":[2,26],\"19\":[2,18],\"20\":[2,16],\"21\":[2,18],\"22\":[2,24],\"23\":[1,2],\"24\":[2,2],\"25\":[3,4],\"26\":[1,7],\"27\":[1,6],\"28\":[1,7],\"29\":[1,7],\"30\":[1,6],\"31\":[2,8],\"32\":[1,24],\"33\":[1,2],\"34\":[1,2],\"35\":[1,2],\"36\":[1,2],\"37\":[1,11],\"38\":[2,10],\"39\":[2,2],\"40\":[1,9],\"41\":[1,2],\"42\":[2,2],\"43\":[1,7],\"44\":[null,null,1],\"45\":[null,null,1],\"46\":[null,null,2],\"47\":[2,10],\"48\":[2,27],\"49\":[2,49],\"50\":[2,34],\"51\":[null,null,1],\"52\":[null,null,3],\"53\":[null,null,2],\"54\":[1],\"55\":[2,5],\"56\":[2,5],\"57\":[null,null,1],\"58\":[null,null,3],\"59\":[2],\"60\":[2,5],\"61\":[2,5],\"62\":[null,null,2],\"63\":[null,null,2],\"64\":[1],\"65\":[2,5],\"66\":[2,5],\"67\":[null,null,2],\"68\":[null,null,2],\"69\":[1],\"70\":[2,5],\"71\":[2,5],\"72\":[null,null,1],\"73\":[null,null,2],\"74\":[2],\"75\":[2,5],\"76\":[2,5],\"77\":[null,null,1],\"78\":[null,null,3],\"79\":[2,6],\"80\":[2,5],\"81\":[2,5],\"82\":[null,null,1],\"83\":[null,null,3],\"84\":[2],\"85\":[2,5],\"86\":[2,5],\"87\":[null,null,2],\"88\":[null,null,3],\"89\":[2],\"90\":[2,5],\"91\":[2,5],\"92\":[null,null,2],\"93\":[null,null,3],\"94\":[2],\"95\":[2,5],\"96\":[2,5],\"97\":[null,null,2],\"98\":[null,null,3],\"99\":[2,9],\"100\":[2,5],\"101\":[2,5],\"102\":[null,null,2],\"103\":[null,null,3],\"104\":[2],\"105\":[2,5],\"106\":[2,5],\"107\":[null,null,1],\"108\":[null,null,3],\"109\":[2],\"110\":[2,5],\"111\":[2,5],\"112\":[null,null,1],\"113\":[null,null,3]},\"averageFieldLength\":[1.7944673065512011,12.516996012689509,1.589501752224544],\"storedFields\":{\"0\":{\"h\":\"Blog Home\",\"t\":[\"This is a blog home page demo.\",\"To use this layout, you should set both layout: BlogHome and home: true in the page front matter.\",\"For related configuration docs, please see blog homepage.\"]},\"1\":{\"c\":[\"Blog Home\"]},\"2\":{\"h\":\"Intro Page\",\"t\":[\"Place your introduction and profile here.\"]},\"3\":{\"h\":\"Slide page\"},\"4\":{\"c\":[\"Slide page\"]},\"5\":{\"h\":\"Features demo\"},\"6\":{\"h\":\"Catalog\",\"t\":[\"Markdown Enhance\",\"Page Config\",\"Function Disable\",\"Encryption Demo\"]},\"7\":{\"c\":[\"Guide\"]},\"8\":{\"c\":[\"Features demo\"]},\"9\":{\"h\":\"Disabling layout and features\",\"t\":[\"You can disable some function and layout on the page by setting the Frontmatter of the page.\",\"This page is an demo that disables the following features:\",\"Navbar\",\"Sidebar\",\"Breadcrumb\",\"Page information\",\"Contributors\",\"Edit link\",\"Update time\",\"Prev/Next link\",\"Comment\",\"Footer\",\"Back to top button\"]},\"10\":{\"c\":[\"Guide\"]},\"11\":{\"c\":[\"disable\"]},\"12\":{\"c\":[\"Disabling layout and features\"]},\"13\":{\"h\":\"Encryption Article\",\"t\":[\"The actual article content.\",\"Paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text.\",\"Paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text.\"]},\"14\":{\"c\":[\"Guide\"]},\"15\":{\"c\":[\"encryption\"]},\"16\":{\"h\":\"Markdown Enhance\",\"t\":[\"VuePress basically generate pages from Markdown files. So you can use it to generate documentation or blog sites easily.\",\"You should create and write Markdown files, so that VuePress can convert them to different pages according to file structure.\"]},\"17\":{\"h\":\"Markdown Introduction\",\"t\":[\"If you are a new learner and don't know how to write Markdown, please read Markdown Intro and Markdown Demo.\"]},\"18\":{\"h\":\"Markdown Config\",\"t\":[\"VuePress introduce configuration for each markdown page using Frontmatter.\",\"Info\",\"Frontmatter is a important concept in VuePress. If you don't know it, you need to read Frontmatter Introduction.\"]},\"19\":{\"h\":\"Markdown Extension\",\"t\":[\"The Markdown content in VuePress will be parsed by markdown-it, which supports syntax extensions via markdown-it plugins.\"]},\"20\":{\"h\":\"VuePress Enhancement\",\"t\":[\"To enrich document writing, VuePress has extended Markdown syntax.\",\"For these extensions, please read Markdown extensions in VuePress.\"]},\"21\":{\"h\":\"Theme Enhancement\",\"t\":[\"By using vuepress-plugin-md-enhance, the theme extends more Markdown syntax and provides richer writing functions.\"]},\"22\":{\"h\":\"Custom Container\",\"t\":[\"Safely use {{ variable }} in Markdown.\",\"Custom Title\",\"A custom information container with code, link.\",\"const a = 1; \",\"Custom Title\",\"A custom tip container\",\"Custom Title\",\"A custom warning container\",\"Custom Title\",\"A custom danger container\",\"Custom Title\",\"A custom details container\",\"View Detail\"]},\"23\":{\"h\":\"Tabs\",\"t\":[\"View Detail\"]},\"24\":{\"h\":\"Code Tabs\",\"t\":[\"View Detail\"]},\"25\":{\"h\":\"Superscript and Subscript\",\"t\":[\"19th H2O\",\"View Detail\"]},\"26\":{\"h\":\"Align\",\"t\":[\"I am center\",\"I am right align\",\"View Detail\"]},\"27\":{\"h\":\"Attrs\",\"t\":[\"A word having id.\",\"View Detail\"]},\"28\":{\"h\":\"Footnote\",\"t\":[\"This text has footnote[1].\",\"View Detail\"]},\"29\":{\"h\":\"Mark\",\"t\":[\"You can mark important words .\",\"View Detail\"]},\"30\":{\"h\":\"Tasklist\",\"t\":[\" Plan A\",\" Plan B\",\"View Detail\"]},\"31\":{\"h\":\"Image Enhancement\",\"t\":[\"Support setting color scheme and size\",\"View Detail\"]},\"32\":{\"h\":\"Card\",\"t\":[\"title: Mr.Hope desc: Where there is light, there is hope logo: https://mrhope.site/logo.svg link: https://mrhope.site color: rgba(253, 230, 138, 0.15) \",\"View Detail\"]},\"33\":{\"h\":\"Chart\",\"t\":[\"View Detail\"]},\"34\":{\"h\":\"Echarts\",\"t\":[\"View Detail\"]},\"35\":{\"h\":\"Flowchart\",\"t\":[\"View Detail\"]},\"36\":{\"h\":\"Mermaid\",\"t\":[\"View Detail\"]},\"37\":{\"h\":\"Tex\",\"t\":[\"∂ωr∂r​(ωyω​)=(ωyω​){(logy)r+i=1∑r​ωi(−1)ir⋯(r−i+1)(logy)r−i​}\",\"View Detail\"]},\"38\":{\"h\":\"Include files\",\"t\":[\"Markdown Enhance\",\"Page Config\",\"Function Disable\",\"Encryption Demo\",\"View Detail\"]},\"39\":{\"h\":\"Code Demo\",\"t\":[\"View Detail\"]},\"40\":{\"h\":\"Stylize\",\"t\":[\"Donate Mr.Hope a cup of coffee. \",\"View Detail\"]},\"41\":{\"h\":\"Playground\",\"t\":[\"View Detail\"]},\"42\":{\"h\":\"Vue Playground\",\"t\":[\"View Detail\"]},\"43\":{\"h\":\"Presentation\",\"t\":[\"View Detail\",\"This is footnote content ↩︎\"]},\"44\":{\"c\":[\"Guide\"]},\"45\":{\"c\":[\"Markdown\"]},\"46\":{\"c\":[\"Markdown Enhance\"]},\"47\":{\"h\":\"Page Config\",\"t\":[\"Content before more comment is regarded as page excerpt.\"]},\"48\":{\"h\":\"Page Information\",\"t\":[\"You can set page information in Markdown's Frontmatter.\",\"The author is Ms.Hope.\",\"The writing date is January 1, 2020\",\"Category is \\\"Guide\\\"\",\"Tags are \\\"Page Config\\\" and \\\"Guide\\\"\"]},\"49\":{\"h\":\"Page Content\",\"t\":[\"You are free to write your Markdown here.\",\"Assets\",\"You can place images besides your Markdown files, but you should use relative links (i.e.: starting with ./) for them.\",\"For images in .vuepress/public directory, please use absolute links (i.e.: starting with /) for them.\",\"The theme contains a custom badge:\",\"A dark blue badge text badge at the end of line. \"]},\"50\":{\"h\":\"Page Structure\",\"t\":[\"This page should contain:\",\"BreadCrumb\",\"Title and information\",\"TOC (Table of Contents)\",\"Meta information including update time and contributors\",\"Comments\",\"Navbar\",\"Sidebar\",\"Footer\",\"Back to top button\",\"You can customize them in theme options and page frontmatter.\"]},\"51\":{\"c\":[\"Guide\"]},\"52\":{\"c\":[\"Page config\",\"Guide\"]},\"53\":{\"c\":[\"Page Config\"]},\"54\":{\"h\":\"Cherry\"},\"55\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"56\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"57\":{\"c\":[\"Cherry\"]},\"58\":{\"c\":[\"red\",\"small\",\"round\"]},\"59\":{\"h\":\"Dragon Fruit\"},\"60\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"61\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"62\":{\"c\":[\"Dragon Fruit\",\"Fruit\"]},\"63\":{\"c\":[\"red\",\"big\"]},\"64\":{\"h\":\"Strawberry\"},\"65\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"66\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"67\":{\"c\":[\"Fruit\",\"Strawberry\"]},\"68\":{\"c\":[\"red\",\"small\"]},\"69\":{\"h\":\"Tomato\"},\"70\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"71\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"72\":{\"c\":[\"Vegetable\"]},\"73\":{\"c\":[\"red\",\"round\"]},\"74\":{\"h\":\"Apple 1\"},\"75\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"76\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"77\":{\"c\":[\"Apple\"]},\"78\":{\"c\":[\"red\",\"big\",\"round\"]},\"79\":{\"h\":\"Apple 2\",\"t\":[\"A apple article being stared.\"]},\"80\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"81\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"82\":{\"c\":[\"Apple\"]},\"83\":{\"c\":[\"red\",\"big\",\"round\"]},\"84\":{\"h\":\"Apple 3\"},\"85\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"86\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"87\":{\"c\":[\"Apple\",\"Fruit\"]},\"88\":{\"c\":[\"red\",\"big\",\"round\"]},\"89\":{\"h\":\"Apple 4\"},\"90\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"91\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"92\":{\"c\":[\"Apple\",\"Fruit\"]},\"93\":{\"c\":[\"red\",\"big\",\"round\"]},\"94\":{\"h\":\"Banana 1\"},\"95\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"96\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"97\":{\"c\":[\"Banana\",\"Fruit\"]},\"98\":{\"c\":[\"yellow\",\"curly\",\"long\"]},\"99\":{\"h\":\"Banana 2\",\"t\":[\"A banana article being stared with number 10.\"]},\"100\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"101\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"102\":{\"c\":[\"Banana\",\"Fruit\"]},\"103\":{\"c\":[\"yellow\",\"curly\",\"long\"]},\"104\":{\"h\":\"Banana 3\"},\"105\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"106\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"107\":{\"c\":[\"Banana\"]},\"108\":{\"c\":[\"yellow\",\"curly\",\"long\"]},\"109\":{\"h\":\"Banana 4\"},\"110\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"111\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"112\":{\"c\":[\"Banana\"]},\"113\":{\"c\":[\"yellow\",\"curly\",\"long\"]}},\"dirtCount\":0,\"index\":[[\"yellow\",{\"2\":{\"98\":1,\"103\":1,\"108\":1,\"113\":1}}],[\"your\",{\"1\":{\"2\":1,\"49\":2}}],[\"you\",{\"1\":{\"0\":1,\"9\":1,\"16\":2,\"17\":1,\"18\":2,\"29\":1,\"48\":1,\"49\":3,\"50\":1}}],[\"4\",{\"0\":{\"89\":1,\"109\":1}}],[\"3\",{\"0\":{\"56\":1,\"61\":1,\"66\":1,\"71\":1,\"76\":1,\"81\":1,\"84\":1,\"86\":1,\"91\":1,\"96\":1,\"101\":1,\"104\":1,\"106\":1,\"111\":1}}],[\"january\",{\"1\":{\"48\":1}}],[\"↩︎\",{\"1\":{\"43\":1}}],[\"−1\",{\"1\":{\"37\":1}}],[\"ωyω​\",{\"1\":{\"37\":2}}],[\"∂ωr∂r​\",{\"1\":{\"37\":1}}],[\"0\",{\"1\":{\"32\":1}}],[\"=\",{\"1\":{\"22\":1,\"37\":1}}],[\"round\",{\"2\":{\"58\":1,\"73\":1,\"78\":1,\"83\":1,\"88\":1,\"93\":1}}],[\"r−i​\",{\"1\":{\"37\":1}}],[\"r−i+1\",{\"1\":{\"37\":1}}],[\"r+i=1∑r​ωi\",{\"1\":{\"37\":1}}],[\"rgba\",{\"1\":{\"32\":1}}],[\"right\",{\"1\":{\"26\":1}}],[\"richer\",{\"1\":{\"21\":1}}],[\"red\",{\"2\":{\"58\":1,\"63\":1,\"68\":1,\"73\":1,\"78\":1,\"83\":1,\"88\":1,\"93\":1}}],[\"relative\",{\"1\":{\"49\":1}}],[\"related\",{\"1\":{\"0\":1}}],[\"regarded\",{\"1\":{\"47\":1}}],[\"read\",{\"1\":{\"17\":1,\"18\":1,\"20\":1}}],[\"meta\",{\"1\":{\"50\":1}}],[\"mermaid\",{\"0\":{\"36\":1}}],[\"ms\",{\"1\":{\"48\":1}}],[\"mrhope\",{\"1\":{\"32\":2}}],[\"mr\",{\"1\":{\"32\":1,\"40\":1}}],[\"more\",{\"1\":{\"21\":1,\"47\":1}}],[\"md\",{\"1\":{\"21\":1}}],[\"mark\",{\"0\":{\"29\":1},\"1\":{\"29\":1}}],[\"markdown\",{\"0\":{\"16\":1,\"17\":1,\"18\":1,\"19\":1},\"1\":{\"6\":1,\"16\":2,\"17\":3,\"18\":1,\"19\":3,\"20\":2,\"21\":1,\"22\":1,\"38\":1,\"48\":1,\"49\":2},\"2\":{\"45\":1,\"46\":1}}],[\"matter\",{\"1\":{\"0\":1}}],[\"vegetable\",{\"2\":{\"72\":1}}],[\"vue\",{\"0\":{\"42\":1}}],[\"vuepress\",{\"0\":{\"20\":1},\"1\":{\"16\":2,\"18\":2,\"19\":1,\"20\":2,\"21\":1,\"49\":1}}],[\"view\",{\"1\":{\"22\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"27\":1,\"28\":1,\"29\":1,\"30\":1,\"31\":1,\"32\":1,\"33\":1,\"34\":1,\"35\":1,\"36\":1,\"37\":1,\"38\":1,\"39\":1,\"40\":1,\"41\":1,\"42\":1,\"43\":1}}],[\"via\",{\"1\":{\"19\":1}}],[\"variable\",{\"1\":{\"22\":1}}],[\"where\",{\"1\":{\"32\":1}}],[\"which\",{\"1\":{\"19\":1}}],[\"words\",{\"1\":{\"29\":1}}],[\"word\",{\"1\":{\"27\":1}}],[\"warning\",{\"1\":{\"22\":1}}],[\"with\",{\"1\":{\"22\":1,\"49\":2,\"99\":1}}],[\"will\",{\"1\":{\"19\":1}}],[\"writing\",{\"1\":{\"20\":1,\"21\":1,\"48\":1}}],[\"write\",{\"1\":{\"16\":1,\"17\":1,\"49\":1}}],[\"know\",{\"1\":{\"17\":1,\"18\":1}}],[\"generate\",{\"1\":{\"16\":2}}],[\"guide\",{\"1\":{\"48\":2},\"2\":{\"7\":1,\"10\":1,\"14\":1,\"44\":1,\"51\":1,\"52\":1}}],[\"2020\",{\"1\":{\"48\":1}}],[\"230\",{\"1\":{\"32\":1}}],[\"253\",{\"1\":{\"32\":1}}],[\"2\",{\"0\":{\"55\":1,\"60\":1,\"65\":1,\"70\":1,\"75\":1,\"79\":1,\"80\":1,\"85\":1,\"90\":1,\"95\":1,\"99\":1,\"100\":1,\"105\":1,\"110\":1},\"1\":{\"13\":14}}],[\"10\",{\"1\":{\"99\":1}}],[\"15\",{\"1\":{\"32\":1}}],[\"138\",{\"1\":{\"32\":1}}],[\"19th\",{\"1\":{\"25\":1}}],[\"1\",{\"0\":{\"74\":1,\"94\":1},\"1\":{\"13\":12,\"22\":1,\"28\":1,\"48\":1}}],[\"number\",{\"1\":{\"99\":1}}],[\"need\",{\"1\":{\"18\":1}}],[\"new\",{\"1\":{\"17\":1}}],[\"next\",{\"1\":{\"9\":1}}],[\"navbar\",{\"1\":{\"9\":1,\"50\":1}}],[\"using\",{\"1\":{\"18\":1,\"21\":1}}],[\"use\",{\"1\":{\"0\":1,\"16\":1,\"22\":1,\"49\":2}}],[\"update\",{\"1\":{\"9\":1,\"50\":1}}],[\"long\",{\"2\":{\"98\":1,\"103\":1,\"108\":1,\"113\":1}}],[\"logy\",{\"1\":{\"37\":2}}],[\"logo\",{\"1\":{\"32\":2}}],[\"line\",{\"1\":{\"49\":1}}],[\"links\",{\"1\":{\"49\":2}}],[\"link\",{\"1\":{\"9\":2,\"22\":1,\"32\":1}}],[\"light\",{\"1\":{\"32\":1}}],[\"learner\",{\"1\":{\"17\":1}}],[\"layout\",{\"0\":{\"9\":1},\"1\":{\"0\":2,\"9\":1},\"2\":{\"12\":1}}],[\"e\",{\"1\":{\"49\":2}}],[\"excerpt\",{\"1\":{\"47\":1}}],[\"extends\",{\"1\":{\"21\":1}}],[\"extended\",{\"1\":{\"20\":1}}],[\"extensions\",{\"1\":{\"19\":1,\"20\":2}}],[\"extension\",{\"0\":{\"19\":1}}],[\"echarts\",{\"0\":{\"34\":1}}],[\"each\",{\"1\":{\"18\":1}}],[\"easily\",{\"1\":{\"16\":1}}],[\"edit\",{\"1\":{\"9\":1}}],[\"end\",{\"1\":{\"49\":1}}],[\"enrich\",{\"1\":{\"20\":1}}],[\"encryption\",{\"0\":{\"13\":1},\"1\":{\"6\":1,\"38\":1},\"2\":{\"15\":1}}],[\"enhancement\",{\"0\":{\"20\":1,\"21\":1,\"31\":1}}],[\"enhance\",{\"0\":{\"16\":1},\"1\":{\"6\":1,\"21\":1,\"38\":1},\"2\":{\"46\":1}}],[\"options\",{\"1\":{\"50\":1}}],[\"or\",{\"1\":{\"16\":1}}],[\"of\",{\"1\":{\"9\":1,\"40\":1,\"49\":1,\"50\":1}}],[\"on\",{\"1\":{\"9\":1}}],[\"cherry\",{\"0\":{\"54\":1},\"2\":{\"57\":1}}],[\"chart\",{\"0\":{\"33\":1}}],[\"curly\",{\"2\":{\"98\":1,\"103\":1,\"108\":1,\"113\":1}}],[\"cup\",{\"1\":{\"40\":1}}],[\"customize\",{\"1\":{\"50\":1}}],[\"custom\",{\"0\":{\"22\":1},\"1\":{\"22\":10,\"49\":1}}],[\"center\",{\"1\":{\"26\":1}}],[\"create\",{\"1\":{\"16\":1}}],[\"coffee\",{\"1\":{\"40\":1}}],[\"color\",{\"1\":{\"31\":1,\"32\":1}}],[\"code\",{\"0\":{\"24\":1,\"39\":1},\"1\":{\"22\":1}}],[\"comments\",{\"1\":{\"50\":1}}],[\"comment\",{\"1\":{\"9\":1,\"47\":1}}],[\"const\",{\"1\":{\"22\":1}}],[\"concept\",{\"1\":{\"18\":1}}],[\"convert\",{\"1\":{\"16\":1}}],[\"contain\",{\"1\":{\"50\":1}}],[\"contains\",{\"1\":{\"49\":1}}],[\"container\",{\"0\":{\"22\":1},\"1\":{\"22\":5}}],[\"contents\",{\"1\":{\"50\":1}}],[\"content\",{\"0\":{\"49\":1},\"1\":{\"13\":1,\"19\":1,\"43\":1,\"47\":1,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"contributors\",{\"1\":{\"9\":1,\"50\":1}}],[\"config\",{\"0\":{\"18\":1,\"47\":1},\"1\":{\"6\":1,\"38\":1,\"48\":1},\"2\":{\"52\":1,\"53\":1}}],[\"configuration\",{\"1\":{\"0\":1,\"18\":1}}],[\"category\",{\"1\":{\"48\":1}}],[\"catalog\",{\"0\":{\"6\":1}}],[\"card\",{\"0\":{\"32\":1}}],[\"can\",{\"1\":{\"9\":1,\"16\":2,\"29\":1,\"48\":1,\"49\":1,\"50\":1}}],[\"heading\",{\"0\":{\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"here\",{\"1\":{\"2\":1,\"49\":1,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"https\",{\"1\":{\"32\":2}}],[\"having\",{\"1\":{\"27\":1}}],[\"has\",{\"1\":{\"20\":1,\"28\":1}}],[\"h2o\",{\"1\":{\"25\":1}}],[\"hope\",{\"1\":{\"32\":2,\"40\":1,\"48\":1}}],[\"how\",{\"1\":{\"17\":1}}],[\"homepage\",{\"1\":{\"0\":1}}],[\"home\",{\"0\":{\"0\":1},\"1\":{\"0\":2},\"2\":{\"1\":1}}],[\"public\",{\"1\":{\"49\":1}}],[\"parsed\",{\"1\":{\"19\":1}}],[\"paragraph\",{\"1\":{\"13\":26}}],[\"pages\",{\"1\":{\"16\":2}}],[\"page\",{\"0\":{\"2\":1,\"3\":1,\"47\":1,\"48\":1,\"49\":1,\"50\":1},\"1\":{\"0\":2,\"6\":1,\"9\":4,\"18\":1,\"38\":1,\"47\":1,\"48\":2,\"50\":2},\"2\":{\"4\":1,\"52\":1,\"53\":1}}],[\"presentation\",{\"0\":{\"43\":1}}],[\"prev\",{\"1\":{\"9\":1}}],[\"provides\",{\"1\":{\"21\":1}}],[\"profile\",{\"1\":{\"2\":1}}],[\"playground\",{\"0\":{\"41\":1,\"42\":1}}],[\"plan\",{\"1\":{\"30\":2}}],[\"place\",{\"1\":{\"2\":1,\"49\":1}}],[\"plugin\",{\"1\":{\"21\":1}}],[\"plugins\",{\"1\":{\"19\":1}}],[\"please\",{\"1\":{\"0\":1,\"17\":1,\"20\":1,\"49\":1}}],[\"dragon\",{\"0\":{\"59\":1},\"2\":{\"62\":1}}],[\"dark\",{\"1\":{\"49\":1}}],[\"date\",{\"1\":{\"48\":1}}],[\"danger\",{\"1\":{\"22\":1}}],[\"desc\",{\"1\":{\"32\":1}}],[\"detail\",{\"1\":{\"22\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"27\":1,\"28\":1,\"29\":1,\"30\":1,\"31\":1,\"32\":1,\"33\":1,\"34\":1,\"35\":1,\"36\":1,\"37\":1,\"38\":1,\"39\":1,\"40\":1,\"41\":1,\"42\":1,\"43\":1}}],[\"details\",{\"1\":{\"22\":1}}],[\"demo\",{\"0\":{\"5\":1,\"39\":1},\"1\":{\"0\":1,\"6\":1,\"9\":1,\"17\":1,\"38\":1},\"2\":{\"8\":1}}],[\"donate\",{\"1\":{\"40\":1}}],[\"don\",{\"1\":{\"17\":1,\"18\":1}}],[\"document\",{\"1\":{\"20\":1}}],[\"documentation\",{\"1\":{\"16\":1}}],[\"docs\",{\"1\":{\"0\":1}}],[\"directory\",{\"1\":{\"49\":1}}],[\"different\",{\"1\":{\"16\":1}}],[\"disabling\",{\"0\":{\"9\":1},\"2\":{\"12\":1}}],[\"disables\",{\"1\":{\"9\":1}}],[\"disable\",{\"1\":{\"6\":1,\"9\":1,\"38\":1},\"2\":{\"11\":1}}],[\"fruit\",{\"0\":{\"59\":1},\"2\":{\"62\":2,\"67\":1,\"87\":1,\"92\":1,\"97\":1,\"102\":1}}],[\"free\",{\"1\":{\"49\":1}}],[\"from\",{\"1\":{\"16\":1}}],[\"frontmatter\",{\"1\":{\"9\":1,\"18\":3,\"48\":1,\"50\":1}}],[\"front\",{\"1\":{\"0\":1}}],[\"flowchart\",{\"0\":{\"35\":1}}],[\"file\",{\"1\":{\"16\":1}}],[\"files\",{\"0\":{\"38\":1},\"1\":{\"16\":2,\"49\":1}}],[\"footnote\",{\"0\":{\"28\":1},\"1\":{\"28\":1,\"43\":1}}],[\"footer\",{\"1\":{\"9\":1,\"50\":1}}],[\"following\",{\"1\":{\"9\":1}}],[\"for\",{\"1\":{\"0\":1,\"18\":1,\"20\":1,\"49\":3}}],[\"functions\",{\"1\":{\"21\":1}}],[\"function\",{\"1\":{\"6\":1,\"9\":1,\"38\":1}}],[\"features\",{\"0\":{\"5\":1,\"9\":1},\"1\":{\"9\":1},\"2\":{\"8\":1,\"12\":1}}],[\"ir⋯\",{\"1\":{\"37\":1}}],[\"images\",{\"1\":{\"49\":2}}],[\"image\",{\"0\":{\"31\":1}}],[\"important\",{\"1\":{\"18\":1,\"29\":1}}],[\"id\",{\"1\":{\"27\":1}}],[\"i\",{\"1\":{\"26\":2,\"49\":2}}],[\"if\",{\"1\":{\"17\":1,\"18\":1}}],[\"it\",{\"1\":{\"16\":1,\"18\":1,\"19\":2}}],[\"including\",{\"1\":{\"50\":1}}],[\"include\",{\"0\":{\"38\":1}}],[\"info\",{\"1\":{\"18\":1}}],[\"information\",{\"0\":{\"48\":1},\"1\":{\"9\":1,\"22\":1,\"48\":1,\"50\":2}}],[\"introduce\",{\"1\":{\"18\":1}}],[\"introduction\",{\"0\":{\"17\":1},\"1\":{\"2\":1,\"18\":1}}],[\"intro\",{\"0\":{\"2\":1},\"1\":{\"17\":1}}],[\"in\",{\"1\":{\"0\":1,\"18\":1,\"19\":1,\"20\":1,\"22\":1,\"48\":1,\"49\":1,\"50\":1}}],[\"is\",{\"1\":{\"0\":1,\"9\":1,\"18\":1,\"32\":2,\"43\":1,\"47\":1,\"48\":3,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"big\",{\"2\":{\"63\":1,\"78\":1,\"83\":1,\"88\":1,\"93\":1}}],[\"blue\",{\"1\":{\"49\":1}}],[\"bloghome\",{\"1\":{\"0\":1}}],[\"blog\",{\"0\":{\"0\":1},\"1\":{\"0\":2,\"16\":1},\"2\":{\"1\":1}}],[\"but\",{\"1\":{\"49\":1}}],[\"button\",{\"1\":{\"9\":1,\"50\":1}}],[\"b\",{\"1\":{\"30\":1}}],[\"being\",{\"1\":{\"79\":1,\"99\":1}}],[\"besides\",{\"1\":{\"49\":1}}],[\"before\",{\"1\":{\"47\":1}}],[\"be\",{\"1\":{\"19\":1}}],[\"banana\",{\"0\":{\"94\":1,\"99\":1,\"104\":1,\"109\":1},\"1\":{\"99\":1},\"2\":{\"97\":1,\"102\":1,\"107\":1,\"112\":1}}],[\"badge\",{\"1\":{\"49\":3}}],[\"basically\",{\"1\":{\"16\":1}}],[\"back\",{\"1\":{\"9\":1,\"50\":1}}],[\"breadcrumb\",{\"1\":{\"9\":1,\"50\":1}}],[\"by\",{\"1\":{\"9\":1,\"19\":1,\"21\":1}}],[\"both\",{\"1\":{\"0\":1}}],[\"small\",{\"2\":{\"58\":1,\"68\":1}}],[\"s\",{\"1\":{\"48\":1}}],[\"stared\",{\"1\":{\"79\":1,\"99\":1}}],[\"starting\",{\"1\":{\"49\":2}}],[\"strawberry\",{\"0\":{\"64\":1},\"2\":{\"67\":1}}],[\"structure\",{\"0\":{\"50\":1},\"1\":{\"16\":1}}],[\"stylize\",{\"0\":{\"40\":1}}],[\"svg\",{\"1\":{\"32\":1}}],[\"scheme\",{\"1\":{\"31\":1}}],[\"subscript\",{\"0\":{\"25\":1}}],[\"support\",{\"1\":{\"31\":1}}],[\"supports\",{\"1\":{\"19\":1}}],[\"superscript\",{\"0\":{\"25\":1}}],[\"safely\",{\"1\":{\"22\":1}}],[\"syntax\",{\"1\":{\"19\":1,\"20\":1,\"21\":1}}],[\"site\",{\"1\":{\"32\":2}}],[\"sites\",{\"1\":{\"16\":1}}],[\"size\",{\"1\":{\"31\":1}}],[\"sidebar\",{\"1\":{\"9\":1,\"50\":1}}],[\"so\",{\"1\":{\"16\":2}}],[\"some\",{\"1\":{\"9\":1}}],[\"slide\",{\"0\":{\"3\":1},\"2\":{\"4\":1}}],[\"see\",{\"1\":{\"0\":1}}],[\"setting\",{\"1\":{\"9\":1,\"31\":1}}],[\"set\",{\"1\":{\"0\":1,\"48\":1}}],[\"should\",{\"1\":{\"0\":1,\"16\":1,\"49\":1,\"50\":1}}],[\"tex\",{\"0\":{\"37\":1}}],[\"text\",{\"1\":{\"13\":26,\"28\":1,\"49\":1}}],[\"table\",{\"1\":{\"50\":1}}],[\"tabs\",{\"0\":{\"23\":1,\"24\":1}}],[\"tags\",{\"1\":{\"48\":1}}],[\"tasklist\",{\"0\":{\"30\":1}}],[\"tip\",{\"1\":{\"22\":1}}],[\"title\",{\"1\":{\"22\":5,\"32\":1,\"50\":1}}],[\"time\",{\"1\":{\"9\":1,\"50\":1}}],[\"t\",{\"1\":{\"17\":1,\"18\":1}}],[\"that\",{\"1\":{\"9\":1,\"16\":1}}],[\"there\",{\"1\":{\"32\":2}}],[\"these\",{\"1\":{\"20\":1}}],[\"theme\",{\"0\":{\"21\":1},\"1\":{\"21\":1,\"49\":1,\"50\":1}}],[\"them\",{\"1\":{\"16\":1,\"49\":2,\"50\":1}}],[\"the\",{\"1\":{\"0\":1,\"9\":4,\"13\":1,\"19\":1,\"21\":1,\"48\":2,\"49\":2,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"this\",{\"1\":{\"0\":2,\"9\":1,\"28\":1,\"43\":1,\"50\":1}}],[\"true\",{\"1\":{\"0\":1}}],[\"tomato\",{\"0\":{\"69\":1}}],[\"toc\",{\"1\":{\"50\":1}}],[\"top\",{\"1\":{\"9\":1,\"50\":1}}],[\"to\",{\"1\":{\"0\":1,\"9\":1,\"16\":3,\"17\":1,\"18\":1,\"20\":1,\"49\":1,\"50\":1}}],[\"apple\",{\"0\":{\"74\":1,\"79\":1,\"84\":1,\"89\":1},\"1\":{\"79\":1},\"2\":{\"77\":1,\"82\":1,\"87\":1,\"92\":1}}],[\"at\",{\"1\":{\"49\":1}}],[\"attrs\",{\"0\":{\"27\":1}}],[\"absolute\",{\"1\":{\"49\":1}}],[\"author\",{\"1\":{\"48\":1}}],[\"assets\",{\"1\":{\"49\":1}}],[\"as\",{\"1\":{\"47\":1}}],[\"am\",{\"1\":{\"26\":2}}],[\"align\",{\"0\":{\"26\":1},\"1\":{\"26\":1}}],[\"are\",{\"1\":{\"17\":1,\"48\":1,\"49\":1}}],[\"article\",{\"0\":{\"13\":1},\"1\":{\"13\":1,\"79\":1,\"99\":1}}],[\"according\",{\"1\":{\"16\":1}}],[\"actual\",{\"1\":{\"13\":1}}],[\"an\",{\"1\":{\"9\":1}}],[\"and\",{\"0\":{\"9\":1,\"25\":1},\"1\":{\"0\":1,\"2\":1,\"9\":1,\"16\":1,\"17\":2,\"21\":1,\"31\":1,\"48\":1,\"50\":3},\"2\":{\"12\":1}}],[\"a\",{\"1\":{\"0\":1,\"17\":1,\"18\":1,\"22\":6,\"27\":1,\"30\":1,\"40\":1,\"49\":2,\"79\":1,\"99\":1}}]],\"serializationVersion\":2},\"/\":{\"documentCount\":146,\"nextId\":146,\"documentIds\":{\"0\":\"v-c8296fee\",\"1\":\"v-c8296fee@2\",\"2\":\"v-0852455e\",\"3\":\"v-0852455e@2\",\"4\":\"v-1d22e941\",\"5\":\"v-1d22e941@2\",\"6\":\"v-5decfa84\",\"7\":\"v-5decfa84@2\",\"8\":\"v-075c6c62\",\"9\":\"v-075c6c62@2\",\"10\":\"v-506407f4\",\"11\":\"v-506407f4@2\",\"12\":\"v-37a8c5a0\",\"13\":\"v-37a8c5a0@2\",\"14\":\"v-0379cba1\",\"15\":\"v-0379cba1@2\",\"16\":\"v-0fe52c37\",\"17\":\"v-0fe52c37@2\",\"18\":\"v-c6edb6ae\",\"19\":\"v-c6edb6ae@2\",\"20\":\"v-54d7ff21\",\"21\":\"v-54d7ff21@2\",\"22\":\"v-2c3ee7f5\",\"23\":\"v-2c3ee7f5@2\",\"24\":\"v-27b02be6\",\"25\":\"v-27b02be6@2\",\"26\":\"v-02c6a6b2\",\"27\":\"v-02c6a6b2@2\",\"28\":\"v-0017792c\",\"29\":\"v-0017792c@2\",\"30\":\"v-2e75e8de\",\"31\":\"v-2e75e8de@2\",\"32\":\"v-6f7bfa04\",\"33\":\"v-6f7bfa04@2\",\"34\":\"v-0e0b961f\",\"35\":\"v-0e0b961f@2\",\"36\":\"v-7e751551\",\"37\":\"v-7e751551@2\",\"38\":\"v-b6ff5888\",\"39\":\"v-b6ff5888@2\",\"40\":\"v-29e33f95\",\"41\":\"v-29e33f95@2\",\"42\":\"v-dbaf7c9c\",\"43\":\"v-dbaf7c9c@2\",\"44\":\"v-1e3e75c0\",\"45\":\"v-1e3e75c0@2\",\"46\":\"v-0564ef99\",\"47\":\"v-0564ef99@2\",\"48\":\"v-3de926ea\",\"49\":\"v-3de926ea@2\",\"50\":\"v-7b34f334\",\"51\":\"v-7b34f334@2\",\"52\":\"v-3c599b43\",\"53\":\"v-3c599b43@2\",\"54\":\"v-fbb94a6e\",\"55\":\"v-fbb94a6e@2\",\"56\":\"v-1e4ce2de\",\"57\":\"v-1e4ce2de@2\",\"58\":\"v-d39aaa20\",\"59\":\"v-d39aaa20@2\",\"60\":\"v-a0d528ce\",\"61\":\"v-a0d528ce@2\",\"62\":\"v-0c83ddba\",\"63\":\"v-0c83ddba@2\",\"64\":\"v-231414e4\",\"65\":\"v-231414e4@2\",\"66\":\"v-0115d78b\",\"67\":\"v-0115d78b@2\",\"68\":\"v-2ae80a11\",\"69\":\"v-2ae80a11@2\",\"70\":\"v-5f9776df\",\"71\":\"v-5f9776df@2\",\"72\":\"v-540234fd\",\"73\":\"v-540234fd@2\",\"74\":\"v-1f059254\",\"75\":\"v-1f059254@2\",\"76\":\"v-1def6584\",\"77\":\"v-1def6584@2\",\"78\":\"v-61bce55f\",\"79\":\"v-61bce55f@2\",\"80\":\"v-62a926ee\",\"81\":\"v-62a926ee@2\",\"82\":\"v-1ea0ad2b\",\"83\":\"v-1ea0ad2b@2\",\"84\":\"v-097a26e0\",\"85\":\"v-097a26e0@2\",\"86\":\"v-4f52202f\",\"87\":\"v-4f52202f@2\",\"88\":\"v-a5303446\",\"89\":\"v-a5303446@2\",\"90\":\"v-4f1e78a0\",\"91\":\"v-4f1e78a0@2\",\"92\":\"v-521d399c\",\"93\":\"v-521d399c@2\",\"94\":\"v-b2f11bc8\",\"95\":\"v-b2f11bc8@2\",\"96\":\"v-4c8be360\",\"97\":\"v-4c8be360@2\",\"98\":\"v-2d29c23d\",\"99\":\"v-2d29c23d@2\",\"100\":\"v-67ef9756\",\"101\":\"v-67ef9756@2\",\"102\":\"v-366a930c\",\"103\":\"v-366a930c@2\",\"104\":\"v-4729f7b3\",\"105\":\"v-4729f7b3@2\",\"106\":\"v-af0ebf8e\",\"107\":\"v-af0ebf8e@2\",\"108\":\"v-259091a4\",\"109\":\"v-259091a4@2\",\"110\":\"v-0a160bb2\",\"111\":\"v-0a160bb2@2\",\"112\":\"v-6de8295f\",\"113\":\"v-6de8295f@2\",\"114\":\"v-1d9f85f4\",\"115\":\"v-1d9f85f4@2\",\"116\":\"v-d02de8d0\",\"117\":\"v-d02de8d0@2\",\"118\":\"v-bdcc4a40\",\"119\":\"v-bdcc4a40@2\",\"120\":\"v-1f7c19fa\",\"121\":\"v-1f7c19fa@2\",\"122\":\"v-73b4cc35\",\"123\":\"v-73b4cc35@2\",\"124\":\"v-0a768313\",\"125\":\"v-0a768313@2\",\"126\":\"v-6de5e384\",\"127\":\"v-6de5e384@2\",\"128\":\"v-0e85e50e\",\"129\":\"v-0e85e50e@2\",\"130\":\"v-21387c08\",\"131\":\"v-21387c08@2\",\"132\":\"v-1434d78e\",\"133\":\"v-1434d78e@2\",\"134\":\"v-6de5f361\",\"135\":\"v-6de5f361@2\",\"136\":\"v-1beaf78e\",\"137\":\"v-1beaf78e@2\",\"138\":\"v-378c8b4f\",\"139\":\"v-378c8b4f@2\",\"140\":\"v-6de41e24\",\"141\":\"v-6de41e24@2\",\"142\":\"v-24f987b1\",\"143\":\"v-24f987b1@2\",\"144\":\"v-600b6b8c\",\"145\":\"v-600b6b8c@2\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[null,null,1],\"2\":[1],\"3\":[null,null,1],\"4\":[1],\"5\":[null,null,1],\"6\":[1],\"7\":[null,null,1],\"8\":[1],\"9\":[null,null,1],\"10\":[1],\"11\":[null,null,1],\"12\":[1],\"13\":[null,null,1],\"14\":[1],\"15\":[null,null,1],\"16\":[2],\"17\":[null,null,2],\"18\":[2],\"19\":[null,null,2],\"20\":[1],\"21\":[null,null,1],\"22\":[1],\"23\":[null,null,1],\"24\":[1],\"25\":[null,null,1],\"26\":[2],\"27\":[null,null,2],\"28\":[2],\"29\":[null,null,2],\"30\":[3],\"31\":[null,null,3],\"32\":[2],\"33\":[null,null,2],\"34\":[2],\"35\":[null,null,2],\"36\":[3],\"37\":[null,null,3],\"38\":[2],\"39\":[null,null,2],\"40\":[2],\"41\":[null,null,2],\"42\":[2],\"43\":[null,null,2],\"44\":[2],\"45\":[null,null,2],\"46\":[2],\"47\":[null,null,2],\"48\":[2],\"49\":[null,null,2],\"50\":[2],\"51\":[null,null,2],\"52\":[2],\"53\":[null,null,2],\"54\":[1],\"55\":[null,null,1],\"56\":[2],\"57\":[null,null,2],\"58\":[2],\"59\":[null,null,2],\"60\":[2],\"61\":[null,null,2],\"62\":[2],\"63\":[null,null,2],\"64\":[2],\"65\":[null,null,2],\"66\":[2],\"67\":[null,null,2],\"68\":[2],\"69\":[null,null,2],\"70\":[2],\"71\":[null,null,2],\"72\":[1],\"73\":[null,null,1],\"74\":[2],\"75\":[null,null,2],\"76\":[3],\"77\":[null,null,3],\"78\":[2],\"79\":[null,null,2],\"80\":[3],\"81\":[null,null,3],\"82\":[2],\"83\":[null,null,2],\"84\":[2],\"85\":[null,null,2],\"86\":[2],\"87\":[null,null,2],\"88\":[3],\"89\":[null,null,3],\"90\":[2],\"91\":[null,null,2],\"92\":[2],\"93\":[null,null,2],\"94\":[3],\"95\":[null,null,3],\"96\":[3],\"97\":[null,null,3],\"98\":[2],\"99\":[null,null,2],\"100\":[2],\"101\":[null,null,2],\"102\":[2],\"103\":[null,null,2],\"104\":[2],\"105\":[null,null,2],\"106\":[2],\"107\":[null,null,2],\"108\":[2],\"109\":[null,null,2],\"110\":[2],\"111\":[null,null,2],\"112\":[2],\"113\":[null,null,2],\"114\":[2],\"115\":[null,null,2],\"116\":[3],\"117\":[null,null,3],\"118\":[2],\"119\":[null,null,2],\"120\":[3],\"121\":[null,null,3],\"122\":[3],\"123\":[null,null,3],\"124\":[2],\"125\":[null,null,2],\"126\":[2],\"127\":[null,null,2],\"128\":[2],\"129\":[null,null,2],\"130\":[4],\"131\":[null,null,4],\"132\":[2],\"133\":[null,null,2],\"134\":[2],\"135\":[null,null,2],\"136\":[2],\"137\":[null,null,2],\"138\":[2],\"139\":[null,null,2],\"140\":[2],\"141\":[null,null,2],\"142\":[2],\"143\":[null,null,2],\"144\":[2],\"145\":[null,null,2]},\"averageFieldLength\":[1.710499467039581,null,1.6357223395511853],\"storedFields\":{\"0\":{\"h\":\"Posts\"},\"1\":{\"c\":[\"Posts\"]},\"2\":{\"h\":\"Apple\"},\"3\":{\"c\":[\"Apple\"]},\"4\":{\"h\":\"Banana\"},\"5\":{\"c\":[\"Banana\"]},\"6\":{\"h\":\"Category\"},\"7\":{\"c\":[\"Category\"]},\"8\":{\"h\":\"Tag\"},\"9\":{\"c\":[\"Tag\"]},\"10\":{\"h\":\"Articles\"},\"11\":{\"c\":[\"Articles\"]},\"12\":{\"h\":\"Star\"},\"13\":{\"c\":[\"Star\"]},\"14\":{\"h\":\"Timeline\"},\"15\":{\"c\":[\"Timeline\"]},\"16\":{\"h\":\"Guide Category\"},\"17\":{\"c\":[\"Guide Category\"]},\"18\":{\"h\":\"disable Tag\"},\"19\":{\"c\":[\"disable Tag\"]},\"20\":{\"h\":\"文章\"},\"21\":{\"c\":[\"文章\"]},\"22\":{\"h\":\"收藏\"},\"23\":{\"c\":[\"收藏\"]},\"24\":{\"h\":\"时间轴\"},\"25\":{\"c\":[\"时间轴\"]},\"26\":{\"h\":\"Cherry Category\"},\"27\":{\"c\":[\"Cherry Category\"]},\"28\":{\"h\":\"encryption Tag\"},\"29\":{\"c\":[\"encryption Tag\"]},\"30\":{\"h\":\"Dragon Fruit Category\"},\"31\":{\"c\":[\"Dragon Fruit Category\"]},\"32\":{\"h\":\"Markdown Tag\"},\"33\":{\"c\":[\"Markdown Tag\"]},\"34\":{\"h\":\"Fruit Category\"},\"35\":{\"c\":[\"Fruit Category\"]},\"36\":{\"h\":\"Page config Tag\"},\"37\":{\"c\":[\"Page config Tag\"]},\"38\":{\"h\":\"Strawberry Category\"},\"39\":{\"c\":[\"Strawberry Category\"]},\"40\":{\"h\":\"Guide Tag\"},\"41\":{\"c\":[\"Guide Tag\"]},\"42\":{\"h\":\"Vegetable Category\"},\"43\":{\"c\":[\"Vegetable Category\"]},\"44\":{\"h\":\"red Tag\"},\"45\":{\"c\":[\"red Tag\"]},\"46\":{\"h\":\"Apple Category\"},\"47\":{\"c\":[\"Apple Category\"]},\"48\":{\"h\":\"small Tag\"},\"49\":{\"c\":[\"small Tag\"]},\"50\":{\"h\":\"Banana Category\"},\"51\":{\"c\":[\"Banana Category\"]},\"52\":{\"h\":\"round Tag\"},\"53\":{\"c\":[\"round Tag\"]},\"54\":{\"h\":\"分类\"},\"55\":{\"c\":[\"分类\"]},\"56\":{\"h\":\"big Tag\"},\"57\":{\"c\":[\"big Tag\"]},\"58\":{\"h\":\"数据集 分类\"},\"59\":{\"c\":[\"数据集 分类\"]},\"60\":{\"h\":\"yellow Tag\"},\"61\":{\"c\":[\"yellow Tag\"]},\"62\":{\"h\":\"评估方法 分类\"},\"63\":{\"c\":[\"评估方法 分类\"]},\"64\":{\"h\":\"curly Tag\"},\"65\":{\"c\":[\"curly Tag\"]},\"66\":{\"h\":\"微调技术 分类\"},\"67\":{\"c\":[\"微调技术 分类\"]},\"68\":{\"h\":\"long Tag\"},\"69\":{\"c\":[\"long Tag\"]},\"70\":{\"h\":\"语言模型 分类\"},\"71\":{\"c\":[\"语言模型 分类\"]},\"72\":{\"h\":\"标签\"},\"73\":{\"c\":[\"标签\"]},\"74\":{\"h\":\"提示技术 分类\"},\"75\":{\"c\":[\"提示技术 分类\"]},\"76\":{\"h\":\"Instruct Tuning 标签\"},\"77\":{\"c\":[\"Instruct Tuning 标签\"]},\"78\":{\"h\":\"Token 分类\"},\"79\":{\"c\":[\"Token 分类\"]},\"80\":{\"h\":\"Prompt Tuning 标签\"},\"81\":{\"c\":[\"Prompt Tuning 标签\"]},\"82\":{\"h\":\"语言模型 标签\"},\"83\":{\"c\":[\"语言模型 标签\"]},\"84\":{\"h\":\"评估 标签\"},\"85\":{\"c\":[\"评估 标签\"]},\"86\":{\"h\":\"PEFT 标签\"},\"87\":{\"c\":[\"PEFT 标签\"]},\"88\":{\"h\":\"Hugging Face 标签\"},\"89\":{\"c\":[\"Hugging Face 标签\"]},\"90\":{\"h\":\"LoRA 标签\"},\"91\":{\"c\":[\"LoRA 标签\"]},\"92\":{\"h\":\"AdaLoRA 标签\"},\"93\":{\"c\":[\"AdaLoRA 标签\"]},\"94\":{\"h\":\"Prefix Tuning 标签\"},\"95\":{\"c\":[\"Prefix Tuning 标签\"]},\"96\":{\"h\":\"P-Tuning 标签\"},\"97\":{\"c\":[\"P-Tuning 标签\"]},\"98\":{\"h\":\"优化 标签\"},\"99\":{\"c\":[\"优化 标签\"]},\"100\":{\"h\":\"内存 标签\"},\"101\":{\"c\":[\"内存 标签\"]},\"102\":{\"h\":\"机器学习 标签\"},\"103\":{\"c\":[\"机器学习 标签\"]},\"104\":{\"h\":\"Transformer 标签\"},\"105\":{\"c\":[\"Transformer 标签\"]},\"106\":{\"h\":\"字节 标签\"},\"107\":{\"c\":[\"字节 标签\"]},\"108\":{\"h\":\"模型 标签\"},\"109\":{\"c\":[\"模型 标签\"]},\"110\":{\"h\":\"深度学习 标签\"},\"111\":{\"c\":[\"深度学习 标签\"]},\"112\":{\"h\":\"LLM 标签\"},\"113\":{\"c\":[\"LLM 标签\"]},\"114\":{\"h\":\"推理 标签\"},\"115\":{\"c\":[\"推理 标签\"]},\"116\":{\"h\":\"Reinforcement Learning 标签\"},\"117\":{\"c\":[\"Reinforcement Learning 标签\"]},\"118\":{\"h\":\"OpenAI 标签\"},\"119\":{\"c\":[\"OpenAI 标签\"]},\"120\":{\"h\":\"Policy-based 标签\"},\"121\":{\"c\":[\"Policy-based 标签\"]},\"122\":{\"h\":\"Value-based 标签\"},\"123\":{\"c\":[\"Value-based 标签\"]},\"124\":{\"h\":\"摘要 标签\"},\"125\":{\"c\":[\"摘要 标签\"]},\"126\":{\"h\":\"GLM 标签\"},\"127\":{\"c\":[\"GLM 标签\"]},\"128\":{\"h\":\"Google 标签\"},\"129\":{\"c\":[\"Google 标签\"]},\"130\":{\"h\":\"In-context Learning 标签\"},\"131\":{\"c\":[\"In-context Learning 标签\"]},\"132\":{\"h\":\"ChatGPT 标签\"},\"133\":{\"c\":[\"ChatGPT 标签\"]},\"134\":{\"h\":\"GPT 标签\"},\"135\":{\"c\":[\"GPT 标签\"]},\"136\":{\"h\":\"强化学习 标签\"},\"137\":{\"c\":[\"强化学习 标签\"]},\"138\":{\"h\":\"知识回路 标签\"},\"139\":{\"c\":[\"知识回路 标签\"]},\"140\":{\"h\":\"CoT 标签\"},\"141\":{\"c\":[\"CoT 标签\"]},\"142\":{\"h\":\"Memory 标签\"},\"143\":{\"c\":[\"Memory 标签\"]},\"144\":{\"h\":\"分词器 标签\"},\"145\":{\"c\":[\"分词器 标签\"]}},\"dirtCount\":0,\"index\":[[\"分词器\",{\"0\":{\"144\":1},\"2\":{\"145\":1}}],[\"分类\",{\"0\":{\"54\":1,\"58\":1,\"62\":1,\"66\":1,\"70\":1,\"74\":1,\"78\":1},\"2\":{\"55\":1,\"59\":1,\"63\":1,\"67\":1,\"71\":1,\"75\":1,\"79\":1}}],[\"memory\",{\"0\":{\"142\":1},\"2\":{\"143\":1}}],[\"markdown\",{\"0\":{\"32\":1},\"2\":{\"33\":1}}],[\"知识回路\",{\"0\":{\"138\":1},\"2\":{\"139\":1}}],[\"强化学习\",{\"0\":{\"136\":1},\"2\":{\"137\":1}}],[\"in\",{\"0\":{\"130\":1},\"2\":{\"131\":1}}],[\"instruct\",{\"0\":{\"76\":1},\"2\":{\"77\":1}}],[\"gpt\",{\"0\":{\"134\":1},\"2\":{\"135\":1}}],[\"google\",{\"0\":{\"128\":1},\"2\":{\"129\":1}}],[\"glm\",{\"0\":{\"126\":1},\"2\":{\"127\":1}}],[\"guide\",{\"0\":{\"16\":1,\"40\":1},\"2\":{\"17\":1,\"41\":1}}],[\"摘要\",{\"0\":{\"124\":1},\"2\":{\"125\":1}}],[\"value\",{\"0\":{\"122\":1},\"2\":{\"123\":1}}],[\"vegetable\",{\"0\":{\"42\":1},\"2\":{\"43\":1}}],[\"openai\",{\"0\":{\"118\":1},\"2\":{\"119\":1}}],[\"推理\",{\"0\":{\"114\":1},\"2\":{\"115\":1}}],[\"learning\",{\"0\":{\"116\":1,\"130\":1},\"2\":{\"117\":1,\"131\":1}}],[\"llm\",{\"0\":{\"112\":1},\"2\":{\"113\":1}}],[\"lora\",{\"0\":{\"90\":1},\"2\":{\"91\":1}}],[\"long\",{\"0\":{\"68\":1},\"2\":{\"69\":1}}],[\"深度学习\",{\"0\":{\"110\":1},\"2\":{\"111\":1}}],[\"模型\",{\"0\":{\"108\":1},\"2\":{\"109\":1}}],[\"字节\",{\"0\":{\"106\":1},\"2\":{\"107\":1}}],[\"机器学习\",{\"0\":{\"102\":1},\"2\":{\"103\":1}}],[\"内存\",{\"0\":{\"100\":1},\"2\":{\"101\":1}}],[\"优化\",{\"0\":{\"98\":1},\"2\":{\"99\":1}}],[\"face\",{\"0\":{\"88\":1},\"2\":{\"89\":1}}],[\"fruit\",{\"0\":{\"30\":1,\"34\":1},\"2\":{\"31\":1,\"35\":1}}],[\"hugging\",{\"0\":{\"88\":1},\"2\":{\"89\":1}}],[\"评估\",{\"0\":{\"84\":1},\"2\":{\"85\":1}}],[\"评估方法\",{\"0\":{\"62\":1},\"2\":{\"63\":1}}],[\"提示技术\",{\"0\":{\"74\":1},\"2\":{\"75\":1}}],[\"标签\",{\"0\":{\"72\":1,\"76\":1,\"80\":1,\"82\":1,\"84\":1,\"86\":1,\"88\":1,\"90\":1,\"92\":1,\"94\":1,\"96\":1,\"98\":1,\"100\":1,\"102\":1,\"104\":1,\"106\":1,\"108\":1,\"110\":1,\"112\":1,\"114\":1,\"116\":1,\"118\":1,\"120\":1,\"122\":1,\"124\":1,\"126\":1,\"128\":1,\"130\":1,\"132\":1,\"134\":1,\"136\":1,\"138\":1,\"140\":1,\"142\":1,\"144\":1},\"2\":{\"73\":1,\"77\":1,\"81\":1,\"83\":1,\"85\":1,\"87\":1,\"89\":1,\"91\":1,\"93\":1,\"95\":1,\"97\":1,\"99\":1,\"101\":1,\"103\":1,\"105\":1,\"107\":1,\"109\":1,\"111\":1,\"113\":1,\"115\":1,\"117\":1,\"119\":1,\"121\":1,\"123\":1,\"125\":1,\"127\":1,\"129\":1,\"131\":1,\"133\":1,\"135\":1,\"137\":1,\"139\":1,\"141\":1,\"143\":1,\"145\":1}}],[\"语言模型\",{\"0\":{\"70\":1,\"82\":1},\"2\":{\"71\":1,\"83\":1}}],[\"微调技术\",{\"0\":{\"66\":1},\"2\":{\"67\":1}}],[\"yellow\",{\"0\":{\"60\":1},\"2\":{\"61\":1}}],[\"数据集\",{\"0\":{\"58\":1},\"2\":{\"59\":1}}],[\"based\",{\"0\":{\"120\":1,\"122\":1},\"2\":{\"121\":1,\"123\":1}}],[\"banana\",{\"0\":{\"4\":1,\"50\":1},\"2\":{\"5\":1,\"51\":1}}],[\"big\",{\"0\":{\"56\":1},\"2\":{\"57\":1}}],[\"reinforcement\",{\"0\":{\"116\":1},\"2\":{\"117\":1}}],[\"red\",{\"0\":{\"44\":1},\"2\":{\"45\":1}}],[\"round\",{\"0\":{\"52\":1},\"2\":{\"53\":1}}],[\"small\",{\"0\":{\"48\":1},\"2\":{\"49\":1}}],[\"strawberry\",{\"0\":{\"38\":1},\"2\":{\"39\":1}}],[\"star\",{\"0\":{\"12\":1},\"2\":{\"13\":1}}],[\"policy\",{\"0\":{\"120\":1},\"2\":{\"121\":1}}],[\"posts\",{\"0\":{\"0\":1},\"2\":{\"1\":1}}],[\"p\",{\"0\":{\"96\":1},\"2\":{\"97\":1}}],[\"prefix\",{\"0\":{\"94\":1},\"2\":{\"95\":1}}],[\"prompt\",{\"0\":{\"80\":1},\"2\":{\"81\":1}}],[\"peft\",{\"0\":{\"86\":1},\"2\":{\"87\":1}}],[\"page\",{\"0\":{\"36\":1},\"2\":{\"37\":1}}],[\"dragon\",{\"0\":{\"30\":1},\"2\":{\"31\":1}}],[\"disable\",{\"0\":{\"18\":1},\"2\":{\"19\":1}}],[\"encryption\",{\"0\":{\"28\":1},\"2\":{\"29\":1}}],[\"cot\",{\"0\":{\"140\":1},\"2\":{\"141\":1}}],[\"context\",{\"0\":{\"130\":1},\"2\":{\"131\":1}}],[\"config\",{\"0\":{\"36\":1},\"2\":{\"37\":1}}],[\"chatgpt\",{\"0\":{\"132\":1},\"2\":{\"133\":1}}],[\"cherry\",{\"0\":{\"26\":1},\"2\":{\"27\":1}}],[\"curly\",{\"0\":{\"64\":1},\"2\":{\"65\":1}}],[\"category\",{\"0\":{\"6\":1,\"16\":1,\"26\":1,\"30\":1,\"34\":1,\"38\":1,\"42\":1,\"46\":1,\"50\":1},\"2\":{\"7\":1,\"17\":1,\"27\":1,\"31\":1,\"35\":1,\"39\":1,\"43\":1,\"47\":1,\"51\":1}}],[\"时间轴\",{\"0\":{\"24\":1},\"2\":{\"25\":1}}],[\"收藏\",{\"0\":{\"22\":1},\"2\":{\"23\":1}}],[\"文章\",{\"0\":{\"20\":1},\"2\":{\"21\":1}}],[\"transformer\",{\"0\":{\"104\":1},\"2\":{\"105\":1}}],[\"token\",{\"0\":{\"78\":1},\"2\":{\"79\":1}}],[\"tuning\",{\"0\":{\"76\":1,\"80\":1,\"94\":1,\"96\":1},\"2\":{\"77\":1,\"81\":1,\"95\":1,\"97\":1}}],[\"timeline\",{\"0\":{\"14\":1},\"2\":{\"15\":1}}],[\"tag\",{\"0\":{\"8\":1,\"18\":1,\"28\":1,\"32\":1,\"36\":1,\"40\":1,\"44\":1,\"48\":1,\"52\":1,\"56\":1,\"60\":1,\"64\":1,\"68\":1},\"2\":{\"9\":1,\"19\":1,\"29\":1,\"33\":1,\"37\":1,\"41\":1,\"45\":1,\"49\":1,\"53\":1,\"57\":1,\"61\":1,\"65\":1,\"69\":1}}],[\"adalora\",{\"0\":{\"92\":1},\"2\":{\"93\":1}}],[\"articles\",{\"0\":{\"10\":1},\"2\":{\"11\":1}}],[\"apple\",{\"0\":{\"2\":1,\"46\":1},\"2\":{\"3\":1,\"47\":1}}]],\"serializationVersion\":2},\"/zh/\":{\"documentCount\":236,\"nextId\":236,\"documentIds\":{\"0\":\"v-2d0ad528\",\"1\":\"v-2d0ad528@2\",\"2\":\"v-858cfdd6\",\"3\":\"v-564155e4\",\"4\":\"v-564155e4#目录\",\"5\":\"v-564155e4@2\",\"6\":\"v-230f5516\",\"7\":\"v-230f5516#_1-instruct-tuninig数据集分享\",\"8\":\"v-230f5516#_2-prompt-tuning数据集分享\",\"9\":\"v-230f5516@0\",\"10\":\"v-230f5516@1\",\"11\":\"v-947fe6ca\",\"12\":\"v-947fe6ca@0\",\"13\":\"v-947fe6ca@1\",\"14\":\"v-947fe6ca@2\",\"15\":\"v-d48826ac\",\"16\":\"v-d48826ac#_1-数据集数据\",\"17\":\"v-d48826ac#_2-数据集优势\",\"18\":\"v-d48826ac#_3-评估结果\",\"19\":\"v-d48826ac#_4-评估结果分析\",\"20\":\"v-d48826ac@0\",\"21\":\"v-d48826ac@1\",\"22\":\"v-01231baf\",\"23\":\"v-01231baf@0\",\"24\":\"v-01231baf@1\",\"25\":\"v-01231baf@2\",\"26\":\"v-8ff444ae\",\"27\":\"v-8ff444ae#_1-测试数据\",\"28\":\"v-8ff444ae#_2-两种设置\",\"29\":\"v-8ff444ae#_2-1-ao-answer-only\",\"30\":\"v-8ff444ae#_2-2-cot\",\"31\":\"v-8ff444ae#_3-结果展示\",\"32\":\"v-8ff444ae#_3-1-ao\",\"33\":\"v-8ff444ae#_3-2-cot\",\"34\":\"v-8ff444ae#_3-3-c-eval-hard\",\"35\":\"v-8ff444ae@0\",\"36\":\"v-8ff444ae@1\",\"37\":\"v-6676e606\",\"38\":\"v-6676e606#_1-peft定义\",\"39\":\"v-6676e606#_2-peft分类\",\"40\":\"v-6676e606#_2-1-lora\",\"41\":\"v-6676e606#_2-2-adalora\",\"42\":\"v-6676e606#_2-3-prompt分类\",\"43\":\"v-6676e606#_2-4-prefix-tuning\",\"44\":\"v-6676e606#_2-5-prompt-tuning\",\"45\":\"v-6676e606#_2-6-p-tuning\",\"46\":\"v-6676e606#_2-7-各类提示微调对比\",\"47\":\"v-6676e606#_3-实验结果\",\"48\":\"v-6676e606#_4-参考文章\",\"49\":\"v-6676e606@0\",\"50\":\"v-6676e606@1\",\"51\":\"v-dfe0bb22\",\"52\":\"v-dfe0bb22#_1-公式解析\",\"53\":\"v-dfe0bb22#_2-非对称量化\",\"54\":\"v-dfe0bb22@0\",\"55\":\"v-dfe0bb22@1\",\"56\":\"v-33571859\",\"57\":\"v-33571859@0\",\"58\":\"v-33571859@1\",\"59\":\"v-33571859@2\",\"60\":\"v-60ef646e\",\"61\":\"v-60ef646e#_1-介绍\",\"62\":\"v-60ef646e#_2-优化算法\",\"63\":\"v-60ef646e#_2-1-remove-padding-算法\",\"64\":\"v-60ef646e#_2-2-融合的多头注意力\",\"65\":\"v-60ef646e#_2-3-cutlass-grouped-gemm\",\"66\":\"v-60ef646e#_3-变种-transformer-支持\",\"67\":\"v-60ef646e@0\",\"68\":\"v-60ef646e@1\",\"69\":\"v-60ef646e@2\",\"70\":\"v-1f54a3f4\",\"71\":\"v-1f54a3f4#_1-模型架构\",\"72\":\"v-1f54a3f4#_2-训练框架\",\"73\":\"v-1f54a3f4#_2-1-无监督预训练\",\"74\":\"v-1f54a3f4#_2-2-监督微调\",\"75\":\"v-1f54a3f4@0\",\"76\":\"v-1f54a3f4@1\",\"77\":\"v-6e6e5be0\",\"78\":\"v-6e6e5be0#_1-预备知识\",\"79\":\"v-6e6e5be0#_1-1-什么是ntp任务\",\"80\":\"v-6e6e5be0#_1-2-利用-llm-进行数据压缩\",\"81\":\"v-6e6e5be0#_1-3-压缩即智能\",\"82\":\"v-6e6e5be0#_2-gpt-模型对知识的提取过程\",\"83\":\"v-6e6e5be0#_3-知识点在-transformer-中的分布\",\"84\":\"v-6e6e5be0@0\",\"85\":\"v-6e6e5be0@1\",\"86\":\"v-5b6573b9\",\"87\":\"v-5b6573b9@0\",\"88\":\"v-5b6573b9@1\",\"89\":\"v-084e7ec6\",\"90\":\"v-084e7ec6@0\",\"91\":\"v-084e7ec6@1\",\"92\":\"v-084e7ec6@2\",\"93\":\"v-7183d100\",\"94\":\"v-7183d100#_1-基本概念\",\"95\":\"v-7183d100#_2-马尔科夫决策过程\",\"96\":\"v-7183d100#_3-强化学习分类\",\"97\":\"v-7183d100@0\",\"98\":\"v-7183d100@1\",\"99\":\"v-7183d100@2\",\"100\":\"v-6e4a6b67\",\"101\":\"v-6e4a6b67#_1-策略梯度算法\",\"102\":\"v-6e4a6b67#_1-1-算法核心思想\",\"103\":\"v-6e4a6b67#_1-2-评价标准\",\"104\":\"v-6e4a6b67#_2-优势演员-评论家算法\",\"105\":\"v-6e4a6b67#_3-trpo\",\"106\":\"v-6e4a6b67#_4-ppo\",\"107\":\"v-6e4a6b67#参考\",\"108\":\"v-6e4a6b67@0\",\"109\":\"v-6e4a6b67@1\",\"110\":\"v-6e4a6b67@2\",\"111\":\"v-1bb77d88\",\"112\":\"v-1bb77d88#_1-sarsa\",\"113\":\"v-1bb77d88#_2-q-learning\",\"114\":\"v-1bb77d88#_3-on-policy和off-policy\",\"115\":\"v-1bb77d88@0\",\"116\":\"v-1bb77d88@1\",\"117\":\"v-1bb77d88@2\",\"118\":\"v-2f77b9dc\",\"119\":\"v-2f77b9dc#_1-问题提出\",\"120\":\"v-2f77b9dc#_2-背景\",\"121\":\"v-2f77b9dc#_3-实验结论\",\"122\":\"v-2f77b9dc#_3-1-模型参数规模与token数量需要匹配\",\"123\":\"v-2f77b9dc#_3-2-多轮epoch的训练会降低模型性能\",\"124\":\"v-2f77b9dc#_3-3-更大规模的数据集会缓解重复epoch对模型性能下降的影响\",\"125\":\"v-2f77b9dc#_3-4-提高数据集的质量也无法挽救重复训练带来的过拟合\",\"126\":\"v-2f77b9dc#_3-5参数数量和flops在重复训练上的影响\",\"127\":\"v-2f77b9dc#_3-6-小计算量模型的过拟合趋势与大计算量的差不多\",\"128\":\"v-2f77b9dc#_3-7-多样的训练目标可以减轻多epoch下降吗\",\"129\":\"v-2f77b9dc#_3-8-dropout是一个被大语言模型忽视的正则技术-虽然慢-但是可以降低多epoch的影响\",\"130\":\"v-2f77b9dc#_3-9-在训练过程中逐渐使用dropout是有效的策略\",\"131\":\"v-2f77b9dc#_3-10-dropout对不同规模模型的影响不同\",\"132\":\"v-2f77b9dc#_3-11-通过moe扫描确定稠密模型的最佳超参数\",\"133\":\"v-2f77b9dc#_4-总结\",\"134\":\"v-2f77b9dc@0\",\"135\":\"v-2f77b9dc@1\",\"136\":\"v-618590a0\",\"137\":\"v-618590a0#_1-问题提出\",\"138\":\"v-618590a0#_2-unlimiformer技术原理\",\"139\":\"v-618590a0#_2-1-unlimiformer编码\",\"140\":\"v-618590a0#_2-2-检索增强的交叉注意力机制\",\"141\":\"v-618590a0#_3-实验结果\",\"142\":\"v-618590a0#_3-1-长文档摘要\",\"143\":\"v-618590a0#_3-2-书籍摘要\",\"144\":\"v-618590a0@0\",\"145\":\"v-618590a0@1\",\"146\":\"v-129844b1\",\"147\":\"v-129844b1#_1-基座模型的升级\",\"148\":\"v-129844b1#_1-1-transformer架构\",\"149\":\"v-129844b1#_1-2-词汇表大小\",\"150\":\"v-129844b1#_1-3-模型结构\",\"151\":\"v-129844b1#_1-3-1-总体架构\",\"152\":\"v-129844b1#_1-3-2-参数量\",\"153\":\"v-129844b1#_1-3-3-归一化层\",\"154\":\"v-129844b1#_1-3-4-激活函数\",\"155\":\"v-129844b1#_2-flashattention\",\"156\":\"v-129844b1#_3-multi-query-attention\",\"157\":\"v-129844b1#_4-测试结果\",\"158\":\"v-129844b1@0\",\"159\":\"v-129844b1@1\",\"160\":\"v-2f6eb7a8\",\"161\":\"v-2f6eb7a8#_1-gpt系列模型发展历程\",\"162\":\"v-2f6eb7a8#_2-指令微调\",\"163\":\"v-2f6eb7a8#_3-模型的训练方法和数据集\",\"164\":\"v-2f6eb7a8#_4-上下文学习\",\"165\":\"v-2f6eb7a8#_5-参考\",\"166\":\"v-2f6eb7a8@0\",\"167\":\"v-2f6eb7a8@1\",\"168\":\"v-2f6eb7a8@2\",\"169\":\"v-1f5cb91e\",\"170\":\"v-1f5cb91e#_1-encoder-decoder\",\"171\":\"v-1f5cb91e#_1-1-t5\",\"172\":\"v-1f5cb91e#_1-2-chatglm\",\"173\":\"v-1f5cb91e#_2-encoder-only\",\"174\":\"v-1f5cb91e#_3-decoder-only\",\"175\":\"v-1f5cb91e#_3-1-gpt2\",\"176\":\"v-1f5cb91e#_3-2-bloom\",\"177\":\"v-1f5cb91e#_3-3-llama\",\"178\":\"v-1f5cb91e#_4-总结\",\"179\":\"v-1f5cb91e@0\",\"180\":\"v-1f5cb91e@1\",\"181\":\"v-1f511768\",\"182\":\"v-1f511768#_1-语言建模\",\"183\":\"v-1f511768#_2-模型架构\",\"184\":\"v-1f511768#_3-模型架构解析\",\"185\":\"v-1f511768#_3-1-ln\",\"186\":\"v-1f511768#_3-2-multi-head-self-attention\",\"187\":\"v-1f511768#_3-3-gpt2attention\",\"188\":\"v-1f511768#_3-4-参数量计算\",\"189\":\"v-1f511768@0\",\"190\":\"v-1f511768@1\",\"191\":\"v-e581d6e0\",\"192\":\"v-e581d6e0#_1-策略梯度算法\",\"193\":\"v-e581d6e0#_2-重要性采样\",\"194\":\"v-e581d6e0#_3-优势函数\",\"195\":\"v-e581d6e0#_4-kl散度的外在约束\",\"196\":\"v-e581d6e0#_5-kl惩罚\",\"197\":\"v-e581d6e0#_6-ppo裁剪-clip\",\"198\":\"v-e581d6e0@0\",\"199\":\"v-e581d6e0@1\",\"200\":\"v-451aeaaf\",\"201\":\"v-451aeaaf#_1-gpt中知识回路存在的证据\",\"202\":\"v-451aeaaf#_1-1-数学能力的知识回路\",\"203\":\"v-451aeaaf#_1-2-induction-head回路\",\"204\":\"v-451aeaaf#_1-3-attention-回路\",\"205\":\"v-451aeaaf#_2-回路竞争猜想\",\"206\":\"v-451aeaaf#_3-参考\",\"207\":\"v-451aeaaf@1\",\"208\":\"v-20bcd658\",\"209\":\"v-20bcd658@0\",\"210\":\"v-20bcd658@1\",\"211\":\"v-f6ba5632\",\"212\":\"v-f6ba5632@0\",\"213\":\"v-f6ba5632@1\",\"214\":\"v-f6ba5632@2\",\"215\":\"v-f9344a26\",\"216\":\"v-f9344a26#_1-问题提出\",\"217\":\"v-f9344a26#_2-recurrentgpt原理\",\"218\":\"v-f9344a26#_3-在线演示\",\"219\":\"v-f9344a26#_4-相关研究\",\"220\":\"v-f9344a26@0\",\"221\":\"v-f9344a26@1\",\"222\":\"v-3c7ae03a\",\"223\":\"v-3c7ae03a@0\",\"224\":\"v-3c7ae03a@1\",\"225\":\"v-3c7ae03a@2\",\"226\":\"v-331bd79c\",\"227\":\"v-331bd79c#_1-分词算法\",\"228\":\"v-331bd79c#_2-一个示例\",\"229\":\"v-331bd79c#_3-gpt2tokenizer\",\"230\":\"v-331bd79c#_3-1-训练\",\"231\":\"v-331bd79c#_3-2-编码\",\"232\":\"v-331bd79c#_3-3-解码\",\"233\":\"v-331bd79c#_3-4-总结\",\"234\":\"v-331bd79c@0\",\"235\":\"v-331bd79c@1\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[null,null,1],\"2\":[1,6],\"3\":[1],\"4\":[1,7],\"5\":[null,null,1],\"6\":[3,11],\"7\":[3,128],\"8\":[3,157],\"9\":[null,null,1],\"10\":[null,null,3],\"11\":[1],\"12\":[null,null,1],\"13\":[null,null,1],\"14\":[null,null,1],\"15\":[1,14],\"16\":[2,28],\"17\":[2,45],\"18\":[2,20],\"19\":[2,44],\"20\":[null,null,1],\"21\":[null,null,2],\"22\":[1],\"23\":[null,null,1],\"24\":[null,null,1],\"25\":[null,null,1],\"26\":[2,24],\"27\":[2,44],\"28\":[2],\"29\":[6,3],\"30\":[2,3],\"31\":[2],\"32\":[3,3],\"33\":[3,3],\"34\":[4,5],\"35\":[null,null,1],\"36\":[null,null,2],\"37\":[2,19],\"38\":[2,21],\"39\":[2,70],\"40\":[3,109],\"41\":[2,90],\"42\":[3,22],\"43\":[4,83],\"44\":[4,35],\"45\":[4,66],\"46\":[3,45],\"47\":[2,6],\"48\":[2,31],\"49\":[null,null,1],\"50\":[null,null,9],\"51\":[1,6],\"52\":[2,67],\"53\":[2,8],\"54\":[null,null,1],\"55\":[null,null,3],\"56\":[1],\"57\":[null,null,1],\"58\":[null,null,1],\"59\":[null,null,1],\"60\":[2,9],\"61\":[2,53],\"62\":[2],\"63\":[5,38],\"64\":[2,62],\"65\":[5,78],\"66\":[4,23],\"67\":[null,null,1],\"68\":[null,null,3],\"69\":[null,null,2],\"70\":[8,7],\"71\":[2,3],\"72\":[2],\"73\":[3,27],\"74\":[2,22],\"75\":[null,null,1],\"76\":[null,null,2],\"77\":[3,30],\"78\":[2],\"79\":[2,15],\"80\":[5,10],\"81\":[3,61],\"82\":[3,111],\"83\":[4,79],\"84\":[null,null,1],\"85\":[null,null,1],\"86\":[2,18],\"87\":[null,null,1],\"88\":[null,null,2],\"89\":[1],\"90\":[null,null,1],\"91\":[null,null,1],\"92\":[null,null,1],\"93\":[1,15],\"94\":[2,17],\"95\":[2,34],\"96\":[2,41],\"97\":[null,null,1],\"98\":[null,null,3],\"99\":[null,null,1],\"100\":[1,11],\"101\":[2],\"102\":[2,21],\"103\":[3,63],\"104\":[3,28],\"105\":[2,19],\"106\":[2,1],\"107\":[1,36],\"108\":[null,null,1],\"109\":[null,null,5],\"110\":[null,null,1],\"111\":[1,9],\"112\":[2,19],\"113\":[3,8],\"114\":[4,22],\"115\":[null,null,1],\"116\":[null,null,5],\"117\":[null,null,1],\"118\":[11,23],\"119\":[2,15],\"120\":[2,34],\"121\":[2],\"122\":[3,15],\"123\":[3,63],\"124\":[2,14],\"125\":[3,17],\"126\":[2,21],\"127\":[3,10],\"128\":[4,9],\"129\":[5,26],\"130\":[3,10],\"131\":[3,5],\"132\":[3,5],\"133\":[2,17],\"134\":[null,null,1],\"135\":[null,null,3],\"136\":[2,11],\"137\":[2,141],\"138\":[2,30],\"139\":[3,18],\"140\":[2,41],\"141\":[2],\"142\":[3,18],\"143\":[3,14],\"144\":[null,null,1],\"145\":[null,null,3],\"146\":[1,57],\"147\":[2],\"148\":[2,4],\"149\":[3,6],\"150\":[3],\"151\":[3,80],\"152\":[4,104],\"153\":[3,10],\"154\":[4,2],\"155\":[2,72],\"156\":[4,57],\"157\":[2,3],\"158\":[null,null,1],\"159\":[null,null,1],\"160\":[1,4],\"161\":[2,32],\"162\":[2,54],\"163\":[2,51],\"164\":[2,42],\"165\":[2,113],\"166\":[null,null,1],\"167\":[null,null,8],\"168\":[null,null,1],\"169\":[1,13],\"170\":[3,15],\"171\":[2,19],\"172\":[3,24],\"173\":[3,3],\"174\":[3,21],\"175\":[3,13],\"176\":[3,3],\"177\":[2,3],\"178\":[2,14],\"179\":[null,null,1],\"180\":[null,null,1],\"181\":[1,17],\"182\":[2,39],\"183\":[2,80],\"184\":[2,8],\"185\":[3,50],\"186\":[6,28],\"187\":[2,111],\"188\":[3,59],\"189\":[null,null,1],\"190\":[null,null,1],\"191\":[2,12],\"192\":[2,28],\"193\":[2,25],\"194\":[2,31],\"195\":[2,36],\"196\":[2,63],\"197\":[4,25],\"198\":[null,null,1],\"199\":[null,null,2],\"200\":[3,13],\"201\":[2,22],\"202\":[2,117],\"203\":[4,83],\"204\":[4,79],\"205\":[2,50],\"206\":[2,47],\"207\":[null,null,2],\"208\":[4,46],\"209\":[null,null,1],\"210\":[null,null,3],\"211\":[1],\"212\":[null,null,1],\"213\":[null,null,1],\"214\":[null,null,1],\"215\":[7,16],\"216\":[2,27],\"217\":[2,113],\"218\":[2,18],\"219\":[2,32],\"220\":[null,null,1],\"221\":[null,null,3],\"222\":[1],\"223\":[null,null,1],\"224\":[null,null,1],\"225\":[null,null,1],\"226\":[1,17],\"227\":[2,29],\"228\":[2,22],\"229\":[2,13],\"230\":[3,10],\"231\":[3,22],\"232\":[2,61],\"233\":[3,20],\"234\":[null,null,1],\"235\":[null,null,2]},\"averageFieldLength\":[2.4395696664418063,33.3906083386821,1.320592615243061],\"storedFields\":{\"0\":{\"h\":\"主页\"},\"1\":{\"c\":[\"主页\"]},\"2\":{\"h\":\"介绍页\",\"t\":[\"HUST Artificial Intelligence and Embedded Lab\"]},\"3\":{\"h\":\"论文分享\"},\"4\":{\"h\":\"目录\",\"t\":[\"本页面包含一些论文分享的分类：\",\"语言模型\",\"提示技术\",\"微调技术\",\"评估方法\",\"数据集\",\"Token\"]},\"5\":{\"c\":[\"论文分享\"]},\"6\":{\"h\":\"Instruct Tuning和Prompt Tuning数据集分享\",\"t\":[\"Instruct Tuning（指令微调）数据集和Prompt Tuning(提示微调)数据集在模型微调方面，尤其是在模型与人类认识对齐方面，作用巨大。本文针对一些质量较高的指令微调数据集和提示微调数据集，进行了简要介绍。\"]},\"7\":{\"h\":\"1 Instruct Tuninig数据集分享\",\"t\":[\"（1） Super-Natural Instruction 【Allen AI】\",\"这些自然语言指令清楚而完整地描述了一项任务（传统上定义为将输入字符串映射到输出字符串）。配备“理解”语言说明的模型，如果提供了任务说明，应该可以成功解决任何看不见的任务。\",\"（2）HH-RLHF【Anthropic】\",\"项目链接：https://github.com/anthropics/hh-rlhf 数量： 训练集：161k 测试集：8.55k Anthropic 公司旗下的 Claud 是 ChatGPT 的主要竞品之一。 Anthropic 开源了其在自己产品线中使用的 RLHF 数据集： 链接：https://huggingface.co/datasets/Anthropic/hh-rlhf\",\"（3）Unnatural Instruction【orhonovich】\",\"使用 LLMs 自主生成 instruction 数据是 instruct-tuning 领域较为活跃的一个方向。 Unnatural Instruction 使用 GPT3（text-davinci-002）生成了 64k 的 instruction prompt 数据。并使用同样的模型将 64k 的 prompt 进行改写，最终得到了 240k 条 instruction 数据。 论文中显示，在 Instruct-Tuning 中 LLMs 自主生成的 prompt 表现出了良好的效果，甚至超过了在 P3 等数据上进行微调的 T0 等模型。\",\"（4）Self-Instruct【yizhongw】\",\"项目链接：https://github.com/yizhongw/self-instruct Self-Instruct 同样是使用 LLMs 生成 prompt 进行 instruct-tuning 的思路。不过使用了更 fine-grained 的生成流程。 Task pool 和 Quality filtering 等概念被引入，部分缓解了 self-intrauct 类型数据的 noise 问题\",\"（5）Flan Collection【Google】\",\"项目链接：https://github.com/google-research/FLAN/tree/main/flan/v2 Google 在这个项目中将自己的 Flan 2021 数据与一些开源的 instruction 数据（P3，super-natural instruction 等）进行了合并\",\"（6）InstructDial【prakharguptaz】\",\"项目链接：https://github.com/prakharguptaz/Instructdial/tree/main/datasets InstructDial 是在特定的一种任务类型上进行指令微调的尝试。实验结果表明，在对话指令数据上微调后，模型在对话任务上的表现强于在超大规模任务集上的结果\"]},\"8\":{\"h\":\"2 Prompt Tuning数据集分享\",\"t\":[\"（1）PromptSource【BigScience】\",\"项目链接：https://github.com/bigscience-workshop/promptsource BigScience 由 Hugging Face 和法国 CNRS，IDRIS，GENCI 等联合组织，是当下最大的开源 LLMs 组织之一。 BigScience 在 2021 年末开发了PromptSource项目，开源了一系列工具 toolkits，帮助研究者基于现有NLP 任务构建 prompt。截止目前，PromptSource 项目包含了 270 个 NLP 任务的超过 2000 个 prompt 模版。\",\"（2）P3【BigScience】\",\"项目链接：https://huggingface.co/datasets/bigscience/P3 语言：英文 在promptsource基础上，BigScience 构建了 P3 数据集。在 Hugging Face Hub 上你可以找到 P3 数据，P3 的数据规模在 100M-1B 之间。\",\"（3）xMTF 【BigScience，包含中文】\",\"项目链接：https://huggingface.co/datasets/bigscience/P3\",\"BigScience 在英语 prompt 的基础上，扩展其 prompt 到多种非英语语言。 该项目包含了 13 个 NLP 任务，并采用了 46 个不同的语言的版本。对应的 prompt 包含的语种个数不定。\",\"（4）UnifiedSKG 【HKU】\",\"项目主页 ：https://unifiedskg.com/\",\"UnifiedSKG 在 Text-to-Text 的框架中加入了 knowledge grounding，也就是在 prompt-output 的框架中，加入了结构化数据做辅助，共21个任务数据集，\",\"解决问题：做打破彼此任务之间的边界的第一次简单尝试，使得这些可以在同一个UnifiedSKG framework下进行学习并在这些任务上取得不错的结果\",\"为方便读者阅读，上述数据集可以总结概括为以下表格\",\"数据集/项目名称\",\"组织/作者\",\"类别\",\"简介\",\"Natural Instruction / Super-Natural Instruction\",\"Allen AI\",\"指令微调\",\"包含61个NLP任务（Natural Instruction）和1600个NLP任务（Super-Natural Instruction）的指令数据\",\"HH-RLHF\",\"Anthropic\",\"指令微调\",\"旨在训练Helpful and Harmless（HH）的LLMs的RLHF数据集\",\"Unnatural Instruction\",\"orhonovich\",\"指令微调\",\"使用GPT3将 64k 的 prompt 进行改写，最终得到了 240k 条 instruction 数据。\",\"Self-Instruct\",\"yizhongw\",\"指令微调\",\"使用LLMs生成prompt进行instruct-tuning的方法，引入Task pool和Quality filtering等概念\",\"Flan Collection\",\"Google\",\"指令微调\",\"将Flan 2021数据与一些开源的instruction数据（P3，super-natural instruction等）进行合并\",\"InstructDial\",\"prakharguptaz\",\"指令微调\",\"在特定的一种任务类型（对话指令）上进行指令微调的尝试\",\"PromptSource / P3\",\"BigScience\",\"提示微调\",\"包含270个NLP任务的2000多个prompt模版（PromptSource）和规模在100M-1B之间的P3数据集\",\"xMTF\",\"BigScience\",\"提示微调\",\"包含13个NLP任务、46种语言的多语言prompt数据\",\"Unnatural Instruction\",\"orhonovich\",\"提示微调\",\"使用GPT3生成64k的instruction prompt数据，经改写后得到240k条instruction数据\",\"UnifiedSKG\",\"HKU\",\"提示微调\",\"在Text-to-Text框架中加入knowledge grounding，将结构化数据序列化并嵌入到prompt中\",\"阅读原文\"]},\"9\":{\"c\":[\"数据集\"]},\"10\":{\"c\":[\"Instruct Tuning\",\"Prompt Tuning\"]},\"11\":{\"h\":\"数据集\"},\"12\":{\"c\":[\"数据集\"]},\"13\":{\"c\":[\"Dataset\"]},\"14\":{\"c\":[\"数据集\"]},\"15\":{\"h\":\"M3KE评估数据集分享\",\"t\":[\"M3KE数据集是一种针对大语言模型的多层次、多主题的知识评估数据集，旨在衡量中文大型语言模型在零样本和少样本设置中获取知识的能力。\",\"提示\",\"项目地址：https://github.com/tjunlp-lab/M3KE\",\"项目贡献者/机构：天津大学与华为诺亚方实验室\"]},\"16\":{\"h\":\"1 数据集数据\",\"t\":[\"M3KE 收集了 20,477 个真人标准化考试题目（包含 4 个候选答案），覆盖 71 个任务，包括小学、初中、高中、大学、研究生入学考试题目，涉及人文、历史、政治、法律、教育、心理学、科学、工程技术、艺术等学科。\",\"图1.1 M3KE数据集中任务分布\"]},\"17\":{\"h\":\"2 数据集优势\",\"t\":[\"（1） 契合中国教育体系，覆盖多教育阶段 研究人员模仿中国学生的教育经历，即小学、初中、高中、大学等主要教育阶段，旨在评估中文大模型在不同教育阶段下的表现。由于每个教育阶段需要掌握的知识点不同（例如，在语文学科中，小学和初中的知识或考点存在明显的差异），因此，M3KE 在不同教育阶段会包含相同的学科。为了提高数据集中学科知识点的覆盖范围，研究人员选择了中国升学考试中的统考试题，包括小升初、中考、高考，研究生入学考试和中国公务员考试等真题题目。 （2） 覆盖多学科领域 为提高数据集的学科覆盖率，研究人员基于人文艺术、社会科学和自然科学三大类进行构建，包括：文学、理学，历史、政治、法学、教育学、心理学、科学、工程技术、艺术等学科。为进一步拓展数据集的丰富度，研究人员补充了中医、宗教以及计算机等级考试等任务。\",\"图2.1 M3KE数据集中任务领域和难度的分布\",\"图2.2 M3KE数据与其他评估数据集对比\"]},\"18\":{\"h\":\"3 评估结果\",\"t\":[\"在零样本设置条件下，模型要求直接回答问题；在少样本设置条件下，会预先给定模型同任务的若干示例，引导模型进行情景学习（In-Context Learning）。在 M3KE 中，所有题目均使用准确率计算得分。 （1） 不同学科类别下的模型零样本/少样本评估结果\",\"图3.1 四个学科分类下各模型的零样本和少样本平均准确率\",\"（2） 不同教育阶段下的模型零样本/少样本评估结果\",\"图3.2 五个教育水平下各模型的零样本和少样本平均准确率\"]},\"19\":{\"h\":\"4 评估结果分析\",\"t\":[\"（1）在零样本评估中（Table 4&6），所有参数小于 10B 的预训练语言模型（未经过微调）准确率都低于随机结果（25%），少样本的设置（Table 5&7）有助于模型性能的提升。但是，GLM130B 在零样本评估的结果好于少样本评估结果，原因可能是 GLM130B 在预训练阶段已经使用了部分指令数据，使其已经具备较好的零样本学习能力。\",\"（2）大部分经过微调后的中文大模型仅达到随机结果（25%）水平，即使在小学阶段的测试中（Table 6&7）。这说明较低教育阶段中的知识仍然是当前中文大模型的短板之一。\",\"（3）在零样本评估中，BELLE-7B-2M 取得了中文大模型中最好的成绩，但仍然与 GPT-3.5-turbo 有 14.8% 的差距。此外，有监督微调指令的数量也是一个重要的因素，经过两百万指令微调的 BELLE-7B-2M 好于经过二十万指令微调的 BELLE-7B-0.2M（Table 4）。\"]},\"20\":{\"c\":[\"评估方法\"]},\"21\":{\"c\":[\"语言模型\",\"评估\"]},\"22\":{\"h\":\"评估方法\"},\"23\":{\"c\":[\"评估方法\"]},\"24\":{\"c\":[\"Eval\"]},\"25\":{\"c\":[\"评估方法\"]},\"26\":{\"h\":\"C-EVAL\",\"t\":[\"C-Eval是一个针对基础模型的综合中文评估套件。它由 13948 道多项选择题组成，涵盖 52 个不同学科和四个难度级别，如下所示。请访问我们的网站或查看我们的论文以了解更多详细信息。\",\"论文：C-EVAL：A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models\",\"评估模型：\"]},\"27\":{\"h\":\"1 测试数据\",\"t\":[\"论文作者团队从中国真实的、具有挑战性的人类的考试题中构建了 C-EVAL，这些考试可以被分为四大类共 52 种不同的学科，每个学科内两百到五百道不等的四个选项的单项选择题，其中四大类分别是 STEM（Science、Technology、Engineering、Mathematics），人文科学，社会科学与其他（包含医学、公务员考试、注册会计师考试、消防工程师考试等）。\",\"C-EVAL 涵盖四个难度级别，分别是初中、高中、大学与专业，数据主要来源于互联网中爬虫得到的试题与一部分作者收集的试题分享，由于爬虫得到的试题格式不统一，作者人工将试题数据做了统一，并将题目中涉及的公式都转化为了标准的 Latex 版本并纠正或删除了一部分错误试题。作者也设计了few-shot测试数据进行测试。此外，作者团队从 C-EVAL 中选择了具有挑战性的数学、物理和化学等 8 个学科的问题，组成了一个独立的 C-EVAL HARD 评测集，这些问题基本需要大学及以上的水平才能进行解决，并且思维与推理过程颇有难度。\"]},\"28\":{\"h\":\"2 两种设置\"},\"29\":{\"h\":\"2.1 AO（Answer Only）\",\"t\":[\"图2.1 AO的prompt设置\"]},\"30\":{\"h\":\"2.2 COT\",\"t\":[\"图2.2 COT的prompt设置\"]},\"31\":{\"h\":\"3 结果展示\"},\"32\":{\"h\":\"3.1 AO\",\"t\":[\"图2.3 AO的结果表格\"]},\"33\":{\"h\":\"3.2 COT\",\"t\":[\"图2.4 COT的结果表格\"]},\"34\":{\"h\":\"3.3 C-Eval Hard\",\"t\":[\"图2.5 C-Eval Hard的结果表格\"]},\"35\":{\"c\":[\"评估方法\"]},\"36\":{\"c\":[\"语言模型\",\"评估\"]},\"37\":{\"h\":\"PEFT：最先进的参数高效微调方法\",\"t\":[\"参数高效微调 （PEFT） 方法能够将预训练的语言模型 （PLM） 有效地适应各种下游应用程序，而无需微调模型的所有参数。微调大型 PLM 的成本通常高得令人望而却步。在这方面，PEFT方法仅微调少量（额外）模型参数，从而大大降低了计算和存储成本。\",\"代码地址：https://github.com/huggingface/peft\"]},\"38\":{\"h\":\"1 PEFT定义\",\"t\":[\"PEFT，即参数高效微调 （Parameter-Efficient Fine-Tuning）技术，同时是Hugging Face开源的一个高效微调大模型的库。\",\"PEFT能够将预训练的语言模型 （PLM） 有效地适应各种下游应用程序，而无需微调模型的所有参数。在微调大型 PLM时，PEFT方法仅微调少量（额外）模型参数，从而大大降低了计算和存储成本。最近的PEFT技术实现了与完全微调相当的性能。\"]},\"39\":{\"h\":\"2 PEFT分类\",\"t\":[\"Hugging Face开源的PEFT库目前支持5种方法，分别是：\",\"（1）LoRA: LoRA: Low-Rank Adaptation of Large Language Models(微软，2021年10月)\",\"（2）AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning(微软，2023年3月)\",\"（3）Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation(斯坦福，2021年8月)；P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks(清华KEG，2022年3月20)；Prefix Tuning在input前面加入prefix部分，并针对拥有自由参数的prefix部分进行微调训练\",\"（4）P-Tuning: GPT Understands, Too(清华，北京智源，2021年3月18)；P-Tuning将prompt对应的token替换为可训练的嵌入，并进行微调训练\",\"（5）Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning(谷歌，2021年9月)；Prompt Tuning针对每一类任务，训练出任务对应prompt的embedding向量\",\"其中，Prefix Tuning、P-Tuning、Prompt Tuning可理解为针对prompt部分的微调。\"]},\"40\":{\"h\":\"2.1 LoRA\",\"t\":[\"LoRA，英文全称Low-Rank Adaptation of Large Language Models，直译为大语言模型的低阶适应，是微软的研究人员为了解决大语言模型微调而开发的一项技术。\",\"LoRA的做法是，冻结预训练好的模型权重参数，然后在每个Transformer块里注入可训练的层，由于不需要对模型的权重参数重新计算梯度，所以，大大减少了需要训练的计算量。\",\"结合上图，可以直观地理解LoRA的实现原理。LoRA冻结预训练模型权重，并将可训练的秩分解矩阵注入到Transformer层的每个权重中，大大减少了下游任务的可训练参数数量。直白的来说，实际上是增加了右侧的“旁支”，也就是先用一个Linear层A，将数据从 d维降到r，再用第二个Linear层B，将数据从r变回d维。最后再将左右两部分的结果相加融合，得到输出的hidden_state。\",\"对于左右两个部分，右侧看起来像是左侧原有矩阵W的分解，从而将参数量从 n ∗ n 变成了n * r + n * r ，在 r < < n 的情况下，参数量就大大地降低了。\",\"事实上，该思想与Albert的思想有异曲同工之处，在Albert中，作者通过两个策略降低了训练的参数量，其一是Embedding矩阵分解，其二是跨层参数共享。\",\"在Albert中，作者考虑到词表的维度很大，所以将Embedding矩阵分解成两个相对较小的矩阵，用来模拟Embedding矩阵的效果，这样一来需要训练的参数量就减少了很多。\",\"LORA也是类似的思想，并且它不再局限于Embedding层，而是所有出现大矩阵的地方，理论上都可以用到这样的分解。\",\"但是与Albert不同的是，Albert直接用两个小矩阵替换了原来的大矩阵，而LORA保留了原来的矩阵W，但是不让W参与训练，所以需要计算梯度的部分就只剩下旁支的A和B两个小矩阵。\",\"从论文中的公式来看，在加入LORA之前，模型训练的优化表示为：\",\"Φmax​(x,y)∈Z∑​t=1∑∣y∣​log(PΦ​(yt​∣x,y<t​))(2.1)\",\"其中，模型的参数用 Φ 表示。\",\"而加入了LORA之后，模型的优化表示为：\",\"Θmax​(x,y)∈Z∑​t=1∑∣y∣​log(pΦ0​+ΔΦ(Θ)​(yt​∣x,y<t​))(2.2)\",\"其中，模型原有的参数是Φ ，LORA新增的参数是Δ Φ ( Θ )。\",\"从第二个式子可以看到，尽管参数看起来增加了（多了Δ Φ ( Θ ) ），但是从前面的max的目标来看，需要优化的参数只有Θ ，而根据假设，Θ < < Φ，这就使得训练过程中，梯度计算量少了很多，所以就在低资源的情况下，我们可以只消耗Θ这部分的资源，这样一来就可以在单卡低显存的情况下训练大模型了。\",\"但是相应地，引入LoRA部分的参数，并不会在推理阶段加速，因为在前向计算的时候，Φ部分还是需要参与计算的，而Θ部分是凭空增加了的参数，所以理论上，推理阶段应该比原来的计算量增大一点。\",\"根据论文的研究结果分析，LoRA的微调质量与全模型微调相当。\"]},\"41\":{\"h\":\"2.2 AdaLoRA\",\"t\":[\"AdaLoRA，即自适应预算分配以实现参数有效的微调，是微软与佐治亚理工学院共同提出的一种微调优化方法。\",\"由于在不太重要的权重矩阵添加更多的参数会产生很少的收益，甚至会损害模型性能，因此论文提出了以下问题：\",\"如何根据模块的重要性自适应地分配参数预算，以提高参数高效微调的性能？\",\"为了回答这个问题，论文提出了一种新的方法——AdaLoRA（自适应的低秩自适应），该方法在类似LoRA的微调过程中在权重矩阵之间动态分配参数预算。具体而言，AdaLoRA调整增量矩阵的秩，以控制其预算。\",\"关键的增量矩阵被分配了高秩，这样它们可以捕获更细粒度和特定于任务的信息。不太重要的增量矩阵被修剪为具有较低的秩，以防止过度拟合并节省计算预算。\",\"AdaLoRA包含两个重要组成部分：\",\"（1）基于SVD的自适应，它以奇异值分解的形式表示增量矩阵∆；\",\"（2）重要性感知秩分配，它根据我们新设计的重要性度量修剪冗余奇异值。\",\"提示\",\"奇异值：特征值的平方根\",\"论文提出了两种重要性度量的方式，分别是：\",\"（1）基于奇异值的重要性度量\",\"（2）基于敏感性的重要性度量\",\"在AdaLoRA中，以奇异值分解的形式对权重矩阵的增量更新进行参数化。然后，根据新的重要性指标，通过操纵奇异值，在增量矩阵之间动态地分配参数预算。这种方法可以有效地提高模型性能和参数效率。\",\"AdaLoRA根据重要性评分自适应地分配参数预算，通过对权重矩阵进行重要性评分，有效地分配参数预算。\",\"在现有的矩阵近似文献中，有一些控制矩阵秩的方法（Cai等人，2010；Koltchinskii等人，2011；Toh & Yun，2010）。它们大多直接计算矩阵的奇异值分解（SVD），然后截断最小的奇异值。这样的操作可以显式地操纵秩，更重要的是，最小化结果矩阵和原始矩阵之间的差异。\",\"然而，对于微调大型模型，迭代地将SVD应用于大量高维权重矩阵会变得非常昂贵。因此，论文没有精确计算SVD，而是将∆参数化为∆=P∧Q，以模拟SVD。对角矩阵∧包含奇异值，而正交矩阵P和Q表示∆的左/右奇异向量。为了正则化P和Q的正交性，在训练损失中增加了额外的惩罚。这样的参数化避免了SVD的密集计算。此外，另一个优点是，该方法只需要在保持奇异向量的同时删除不重要的奇异值。这保留了未来恢复的可能性，并稳定了训练。\",\"基于SVD参数化，AdaLoRA通过重要性评分动态调整∆=P V Q的等级。\",\"具体来说，AdaLoRA将增量矩阵P∧Q划分为三元组，其中每个三元组Gi包含第i个奇异值和相应的奇异向量。为了量化三元组的重要性，AdaLoRA提出了一种新的重要性度量，它考虑了Gi中每个条目对模型性能的贡献。\",\"具有低重要性分数的三元组被授予低优先级，因此奇异值被清零。具有高度重要性的三元组会被保留，并进行微调。\"]},\"42\":{\"h\":\"2.3 prompt分类\",\"t\":[\"prompt分为hard prompt与soft prompt两种，这两种prompt的含义如下。\",\"（1）hard prompt 又称为 Discrete Prompt，离散prompt是一个实际的文本字符串\",\"（2）soft prompt 又称为 Continuous Prompts，连续prompt直接在底层语言模型的嵌入空间中进行描述\",\"prompt的制作分为手工创建prompt和自动化生成prompt，而自动化生成prompt又分为离散提示（又叫做硬提示）和连续提示（又叫做软提示）\"]},\"43\":{\"h\":\"2.4 Prefix Tuning\",\"t\":[\"前缀微调（Prefix-Tuning），是用于 生成任务(NLG) 的轻量微调。\",\"Prefix-Tuning与Full-finetuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。\",\"该方法其实和构造Prompt类似，只是利用多层感知编码prefix，注意多层感知机就是prefix的编码器，不再像Prompt是人为构造的“显式”的提示,并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。\",\"对于Decoder-Only的GPT，prefix只加在句首，[PREFIX, x, y]，对于Encoder-Decoder的BART，不同的prefix同时加在编码器和解码器的开头，[PREFIX, x, PREFIX', y]。在下游微调时，LM的参数被冻结，只有prefix部分的参数进行更新。不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新。\",\"Prefix-Tuning将一系列连续的task-specific向量添加到input前面，称之为前缀，如下图中的红色块所示。\",\"Prefix-Tuning的作者提出了Prefix Tuning，该方法冻结LM参数，并且只优化Prefix（红色前缀块）。因此，只需要为每个任务存储前缀，使前缀调优模块化并节省空间。\",\"与提示（prompt ）不同的是，前缀完全由自由参数组成，与真正的token不对应。相比于传统的微调，前缀微调只优化了前缀。因此，我们只需要存储一个大型Transformer和已知任务特定前缀的副本，对每个额外任务产生非常小的开销。\",\"原论文仅在以下任务中进行了比较：\",\"（1）table-to-text生成任务：GPT-2\",\"（2）生成式摘要任务：BART\",\"Prefix-tuning的prompt拼接方式\",\"Prefix-tuning是做生成任务，它根据不同的模型结构定义了不同的Prompt拼接方式，在GPT类的自回归模型上采用[PREFIX, x, y]，在T5类的encoder-decoder模型上采用[PREFIX, x, PREFIX', y]：\",\"值得注意的还有三个改动：\",\"（1）把预训练大模型freeze住，因为大模型参数量大，精调起来效率低，毕竟prompt的出现就是要解决大模型少样本的适配；\",\"（2）作者发现直接优化Prompt参数不太稳定，加了个更大的MLP，训练完只保存MLP变换后的参数就行了；\",\"（3）实验证实只加到embedding上的效果不太好，因此作者在每层都加了prompt的参数，改动较大。\"]},\"44\":{\"h\":\"2.5 Prompt Tuning\",\"t\":[\"Prompt-tuning 固定预训练参数，为每一个任务（a1、a2、b1、b2）额外添加一个或多个 embedding（A、B、C）。\",\"之后拼接 query 正常输入 LLM ，并只训练这些 embedding 。左图为单任务全参数微调，右图为 prompt tuning 。\",\"Prompt-tuning给每个任务定义了自己的Prompt，拼接到数据上作为输入，同时freeze预训练模型进行训练，在没有加额外层的情况下，可以看到随着模型体积增大效果越来越好，最终追上了精调的效果：\",\"同时，Prompt-tuning还提出了Prompt-ensembling，也就是在一个batch里同时训练同一个任务的不同prompt，这样相当于训练了不同「模型」，比模型集成的成本小多了。\"]},\"45\":{\"h\":\"2.6 P-Tuning\",\"t\":[\"Prompting最初由人工设计Prompt，自然语言提示本身十分脆弱（如下图所示，选择不同的Prompt对下游任务的性能影响较大），而且从优化角度无法达到最优。\",\"为消除这一影响，P Tuning技术应用而生：P-Tuning v1将自然语言提示的token，替换为可训练的嵌入，同时利用LSTM进行Reparamerization加速训练，并引入少量自然语言提示的锚字符（Anchor，例如Britain）进一步提升效果，如下图b所示：\",\"P-Tuning v1，对于BERT类双向语言模型采用模版(P1, x, P2, [MASK], P3)，对于单向语言模型采用(P1, x, P2, [MASK])。\",\"P-Tuning v2提升小模型上的Prompt Tuning，最关键的就是引入Prefix-tuning技术。\",\"Prefix-tuning（前缀微调）最开始应用在NLG任务上，由[Prefix, x, y]三部分构成，如上图所示：Prefix为前缀，x为输入，y为输出。Prefix-tuning将预训练参数固定，Prefix参数进行微调：不仅只在embedding上进行微调，也在TransFormer上的embedding输入每一层进行微调。\",\"P-Tuning v2将Prefix-tuning应用于在NLU任务，如下图所示：\",\"p tuning v2简单来说其实是soft prompt的一种改进。\",\"soft prompt是只作用在embedding层中，实际测试下来只作用在embedding层的话交互能力会变弱，而且冻结模型所有参数去学习插入token，改变量偏小使得效果有时候不太稳定，会差于微调。\",\"p tuning v2则不只是针对embedding层，而是将连续型token插入每一层，增大改变量和交互性。\",\"soft prompt比较依靠模型参数量，在参数量超过10B的模型上，效果追上了fine-tune，但是p tuning v2因为每层插入了token，增大模型训练的改变量，更加适用于小一点的模型。\"]},\"46\":{\"h\":\"2.7 各类提示微调对比\",\"t\":[\"模型：P-tuning （自动化地寻找连续空间中的知识模板） 特点：hard+soft 方法：传统离散prompt直接将模板T的每个token映射为对应的embedding，而P-Tuning将模板T中的Pi（Psedo Prompt）映射为一个可训练的参数 hi。使用BiLSTM对Pi序列进行表征，并加入锚字符（Anchor）提升效果。\",\"模型：Prefix-Tuning 特点：生成任务，soft prompt 方法：在每层transformer 之前加入prefix，Prefix不是真实的 token，而是连续向量 （soft prompt）。\",\"模型：Prompt tuning 特点：prefix-tuning的简化 方法：固定预训练模型，只对下游任务的输入添加额外的 k个 可学习的 token。\",\"模型：P-tuning v2 特点：prefix-tuning的deep形式 方法：prefix-tuning仅在transformer的 第一层加入soft prompt，p tuning v2 提出 Deep Prompt Tuning的方法，在transformer 的每一层之前都加入了soft prompt。\"]},\"47\":{\"h\":\"3 实验结果\",\"t\":[\"根据结果可以看出，在只训练1个epoch的情况下，只有LoRA与AdaLoRA的效果接近全参数微调，并且LoRA与全参数微调的差距不超过0.1%\"]},\"48\":{\"h\":\"4 参考文章\",\"t\":[\"[1] 使用PEFT微调LLMs\",\"[2] 《Prefix-Tuning: Optimizing Continuous Prompts for Generation》阅读笔记\",\"[3] Prefix-Tunning\",\"[4] 【prompt】什么是 Soft Prompt 和 Hard Prompt ?\",\"[5] 【调研】Soft Prompt Tuning 模型发展调研：P-tuning,Prefix-tuning,Prompt-tuning,P-tuning v2\",\"[6] prompt综述\",\"[7] Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning\"]},\"49\":{\"c\":[\"微调技术\"]},\"50\":{\"c\":[\"PEFT\",\"Hugging Face\",\"LoRA\",\"AdaLoRA\",\"Prefix Tuning\",\"P-Tuning\",\"Prompt Tuning\"]},\"51\":{\"h\":\"Int8量化技术原理讲解\",\"t\":[\"Int量化技术是一种节约大模型推理或训练的过程中占用的显存的技术。量化的目是为了减少计算时间和计算能耗 。在一些场景下对能耗和时间的要求，要高于模型的指标，所以在这种情况下量化是一个必然的选择。\"]},\"52\":{\"h\":\"1 公式解析\",\"t\":[\"基准：普通的Linear层：y=Wx+b\",\"x：tensor([1., 2., 3., 4.], device='cuda:0') W：tensor([[ 0.4753, 0.4548, -0.2720, 0.0310], [-0.3591, -0.4820, -0.3717, -0.2604]], device='cuda:0',requires_grad=True) b：tensor([-0.4314, 0.1237], device='cuda:0', requires_grad=True) y：tensor([ 0.2612, -3.3559], device='cuda:0', grad_fn=<AddBackward0>) \",\"（1）令W=TW′，其中T是一个对角矩阵，相当于W′的每行乘以一个系数。\",\"（2）选定T保证W′的每一行四舍五入到整型之后最大值为127或者最小值为−127即可，因此T完全由W决定。\",\"T的对角元素：tensor([0.0037, 0.0038], device='cuda:0', dtype=torch.float16)\",\"W'：tensor([[ 127, 122, -73, 8], [ -95, -127, -98, -69]], device='cuda:0', dtype=torch.int8) b：tensor([-0.4314, 0.1237], device='cuda:0', dtype=torch.float16) \",\"（3）前向传播的计算公式变成了 y=TW′x+b。\",\"（4）量化操作仅针对W，不针对b。量化之后，网络相当于舍弃了W，而保留了W′和T。W′由于变成了int8整型，因此对显存来说相当于多存了T的对角元素，少存了W的一半大小，总体上显存的压力是大大变小了。\",\"y：tensor([ 0.2571, -3.3652], device='cuda:0', dtype=torch.float16) \"]},\"53\":{\"h\":\"2 非对称量化\",\"t\":[\"以上描述的过程是对称量化，对称量化把每一行的绝对值的最大值变换到127，而非对称量化是把每一行的最大值变换到127，最小值变换到−128，因此非对称量化的W′=TW−p，除了多一个T的对角元素之外还多一个偏移向量。\"]},\"54\":{\"c\":[\"微调技术\"]},\"55\":{\"c\":[\"优化\",\"内存\",\"机器学习\"]},\"56\":{\"h\":\"微调技术\"},\"57\":{\"c\":[\"微调技术\"]},\"58\":{\"c\":[\"Finetune\"]},\"59\":{\"c\":[\"微调技术\"]},\"60\":{\"h\":\"大幅优化推理速度-ByteTransformer\",\"t\":[\"论文提出了字节跳动的GPU Transformer推理库——ByteTransformer。针对自然语言处理常见的可变长输入，论文提出了一套优化算法，这些算法在保证运算正确性的前提下，成功避免了传统实现中的冗余运算，实现了端到端的推理过程的大幅优化。\"]},\"61\":{\"h\":\"1 介绍\",\"t\":[\"图1.1 论文信息\",\"论文地址：https://arxiv.org/abs/2210.03052 代码地址：https://github.com/bytedance/ByteTransformer\",\"现有的一些深度学习框架，如Tensorflow，PyTorch，TVM以及NVIDIA TensorRT等，要求输入序列长度相同，才能利用批处理加速Transformer计算。然而，在实际场景中，输入序列通常是变长的，而零填充会引入大量的额外计算开销。字节跳动AML团队先前提出的“effective Transformer”，通过对输入的重排列，实现了 QKV projection 和 MLP 的 padding free，但 self attention 部分仍然需要 padding。 为了解决这个问题，字节跳动 AML 团队提出了 ByteTransformer，它实现了变长输入的 padding free 计算，并且实现了全面的 kernel fusion 以进一步提高性能。\"]},\"62\":{\"h\":\"2 优化算法\"},\"63\":{\"h\":\"2.1 Remove padding 算法\",\"t\":[\"这个算法源自字节跳动 AML 团队之前的工作 \\\"effective Transformer\\\"，在 NVIDIA 开源 FasterTransformer 中也有集成。ByteTransformer 同样使用该算法去除对 attention 外矩阵乘的额外计算。\",\"图2.1 Remove padding 算法\",\"算法步骤如下。\",\"（1）计算 attention mask 的前缀和，作为 offsets。\",\"（2）根据 offsets 把输入张量从 [batch_size, seqlen, hidden_size] 重排列为 [valid_seqlen, hidden_size] ，再参与后续的矩阵乘计算，实现 padding free。\"]},\"64\":{\"h\":\"2.2 融合的多头注意力\",\"t\":[\"旧版的多头注意力：多头注意力 (Multi-Head)，具体是在计算时对注意力做一些变形，每个输入产生多组 Q、K、V（生成几组就是几个头），每组各自计算互不影响，最后把输出拼接在一起作为总输出（可能要再乘一个矩阵来调整形状）。\",\"为了优化 attention 部分的性能，ByteTransformer 中实现了融合的多头注意力（Fused Multi-Head Attention）算子。对于 seqlen 长度，以 384 为界划分为两种实现方式。\",\"（1）对于短 seqlen, 因为可以把 QK 整行放在共享内存进行 softmax 操作，通过手写 kernel 的方式实现，矩阵乘通过调用 wmma 接口使用 TensorCore 保证高性能。\",\"（2）对于长 seqlen, 因为共享内存大小限制，不能在一个手写 kernel 中完成所有操作。基于高性能的 CUTLASS grouped GEMM, 分成两个 gemm kernel 实现，并把 add_bias, softmax 等操作 fused 到 GEMM kernel 中。\"]},\"65\":{\"h\":\"2.3 CUTLASS grouped GEMM\",\"t\":[\"NVIDIA 开发的 grouped GEMM 可以在一个 kernel 中完成多个独立矩阵乘问题的计算，利用这个性质可以实现 Attention 中的 padding free。\",\"（1）Attention 中的两次矩阵乘操作，都可以拆解为 batch_size x head_num 个独立的矩阵乘子问题。\",\"（2）每个矩阵乘子问题，把问题大小传入到 grouped GEMM，其中 seqlen 传递真实的 valid seqlen 即可。\",\"grouped GEMM 原理：kernel 中每个 threadblock (CTA) 固定分块大小，每个矩阵乘子问题根据问题大小和分块大小，拆解为不同数量的待计算块，再把这些块平均分配到每个 threadblock 中进行计算。\",\"图2.2 grouped GEMM 原理\",\"使用 grouped GEMM 实现 attention 时，由于子问题的数量 batch_size x head_num 通常较大，读取子问题参数会有不小的开销，因为从线程角度看，每个线程都需要遍历读取所有的子问题大小。为了解决这个问题，ByteTransformer 对 grouped GEMM 中读取子问题参数进行了性能优化，使其可以忽略不计。\",\"（1）共享子问题参数。对同一个输入，不同 head 的 valid seqlen 相同，problem size 也相同，通过共享使参数存储量从 batch_size x head_num 减少到 batch_size。\",\"（2）warp prefetch. 原始实现中，每个 CUDA thread 依次读取所有的子问题 problem size，效率很低。改为一个 warp 内线程读取连续的 32 个子问题参数，然后通过 warp 内线程通信交换数据，每个线程的读取次数降低到 1/32。\",\"图2.3 warp prefetch\"]},\"66\":{\"h\":\"3 变种 Transformer 支持\",\"t\":[\"目前，字节跳动 AML 团队已经在 GitHub 上开源了 ByteTransformer 的标准 BERT 实现。除此之外，字节内部版本还支持了许多 Transformer 变种，比如 Deberta, Roformer，T5 等等。代码实现易于拓展，并且上述各种优化手段也可以方便地应用到变种 Transformer 中。\"]},\"67\":{\"c\":[\"语言模型\"]},\"68\":{\"c\":[\"Transformer\",\"优化\",\"字节\"]},\"69\":{\"c\":[\"大幅优化推理速度-ByteTransformer\"]},\"70\":{\"h\":\"GPT论文分享：Improving Language Understanding by Generative Pre-Training\",\"t\":[\"作者证明了通过在大量未标注文本上对语言模型进行生成式预训练，然后在每个特定任务上进行歧视性微调，可以在这些任务上实现巨大收益。与以前的方法相比，他们在微调期间利用面向任务的输入转换来实现有效的转移，同时对模型架构所需的更改最小。\"]},\"71\":{\"h\":\"1 模型架构\",\"t\":[\"图1.1 GPT架构图\"]},\"72\":{\"h\":\"2 训练框架\"},\"73\":{\"h\":\"2.1 无监督预训练\",\"t\":[\"给定一个无监督的token语料库U={u1​,⋯,un​}，作者使用标准语言建模目标来最大化以下概率。\",\"其中k是上下文窗口的大小，条件概率P使用具有参数Θ的神经网络来建模。使用随机梯度下降训练这些参数。\",\"在作者的实验中，作者将多层Transformer decoder用于语言模型，这是Transformer的变体。该模型在输入上下文token上应用multi-headed self-attention操作，然后是position-wise前馈层，以在目标token上产生输出分布。\",\"其中U=(U−k,⋯,U−1)是token的上下文向量，n是层数，是token嵌入矩阵，Wp是position嵌入矩阵。\"]},\"74\":{\"h\":\"2.2 监督微调\",\"t\":[\"在预训练之后，作者将参数调整为受监督的目标任务。假设有一个标记的数据集C，其中每个实例由一系列输入token以及标签。输入通过作者的预训练模型，以获得最终Transformer块的激活，然后将其送到添加的具有参数的线性输出层来以预测。\",\"因此，优化目标变成了以下式子。\",\"作者还发现，将语言建模作为微调的辅助目标，通过以下方面体现。\",\"（1）改进监督模型的泛化；\",\"（2）加速收敛，有助于学习。\",\"之前的工作也观察到了这种辅助目标的改进性能。具体而言，作者优化了以下目标（带参数λ）。\"]},\"75\":{\"c\":[\"语言模型\"]},\"76\":{\"c\":[\"模型\",\"深度学习\"]},\"77\":{\"h\":\"LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享\",\"t\":[\"本文主要分享的内容为以下两点。 (1) LLM的信息压缩能力与其智能水平的关系 (2) GPT对知识的提取与存储方式\",\"知乎原文：https://zhuanlan.zhihu.com/p/632795115 版权归属原作者，如涉侵权，请联系删除\",\"一种观点认为GPT 4 这种 LLM 模型仅仅学会了语言中的单词共现等浅层的表面统计关系，其实并未具备智能，只是类似鹦鹉学舌的语言片段缝合怪而已；另外一种观点则认为：GPT 4 不仅学会了语言元素间的表面统计关系，而且学到了人类语言甚至包括物理世界的内在运行规律，文字是由内在智能产生的，所以 LLM 具备类人智能。\"]},\"78\":{\"h\":\"1 预备知识\"},\"79\":{\"h\":\"1.1 什么是NTP任务\",\"t\":[\"目前规模够大的 LLM 模型，在训练基座模型的时候，都采用「Next Token Prediction，NTP」 (后文为了书写简洁，有时会简称为 NTP) 任务。Next Token Prediction 如此简单的操作，就是通过语言中前面的单词，来产生下一个单词\"]},\"80\":{\"h\":\"1.2 利用 LLM 进行数据压缩\",\"t\":[\"如果大语言模型具备越强的数据压缩能力，是否意味着它具备越强的 AGI 智能呢？ 可以举个例子来解释这种数据压缩能力 把LLM看做函数，根据已有的token，计算下一个token的在词表中的概率分布，根据输出的下一个token的概率分布进行算术编码，使用编码后的数据进行数据传输\"]},\"81\":{\"h\":\"1.3 压缩即智能\",\"t\":[\"如果 GPT 模型智能程度越高，NTP 预测得越准确，则其压缩效率就越高。所以，我们可以根据模型的压缩效率来评估模型的智能程度，模型压缩效率越高，则模型智能程度越高，这是目前 OpenAI 照此思路推进大模型研发方向的一个核心理念。\",\"可以就这个思路深入思考两个相关问题。 （1）第一个问题： 上面讲述内容是以数据压缩的视角来看待 LLM 的智能水准，问题是为何模型压缩能力越强，就代表了它具备更高的智能呢？\",\"相对大量数据，数据内在规律的描述，自然就短得多，而模型若能给出越短的描述，说明这个模型学到了更多的内在规律，所以就越聪明。是这个逻辑，举个例子。 假设要传输的序列是连续质数数字序列 下面是gpt-3.5-turbo和oasst两个模型的回答结果。\",\"图1.1 两个模型针对质数概念理解的测试对比\",\"可以看出，gpt3.5 是学会了质数这种抽象概念的，否则这道题很难回答好，如果不理解这个概念，就会出现图右小模型这种不知所云的回答。这一方面说明大模型确实可以学习一些抽象概念，另一方面说明大模型在这方面表现确实比小模型要好。\",\"（2）第二个问题： 如果我们更严谨地来看，会发现尽管 LLM 训练过程可以看成是对数据的无损压缩，但是能够达成「无损」 的效果，并不单单靠 LLM，其实是「LLM + 算术编码」一起完成的。 数据无损压缩 = LLM 模型的有损数据压缩能力 + 算术编码的编码补偿能力\"]},\"82\":{\"h\":\"2 GPT 模型对知识的提取过程\",\"t\":[\"论文：Dissecting Recall of Factual Associations in Auto-Regressive Language Models 剖析自回归语言模型中事实关联的回忆\",\"图2.1 GPT模型对知识的提取归纳过程示意图\",\"经过研究，发现 GPT 在提取这条知识的时候，经历了明显的三阶段过程： （1） 主题补充 单词 「music」是描述这个实体最后的、也是最关键的词汇，它的信息在顺着 Transformer block 往上走的过程中，先通过 Attention 把之前的修饰语「beats」 相关信息集成到「music」 对应位置。之后，随着 Transformer 层数越来越高，通过每个 Transformer Block 的 FFN 层，不断往「music」对应的 Embedding 里增加信息，所以随着信息往上层流动，「music」这个单词对应层数的 Embedding，能够触发越来越多的与「Beat music」 相关 「属性」 词汇。这是第一个步骤，整个过程总体发生在 Transformer 的低层。 （2） 关系传播 GPT 模型在 「by」单词这个位置，也就是 NTP 要产生输出 token 的最后一个位置，通过 Attention 把单词「own」 的信息集成到最后位置。这里需要注意一下，最后一个单词对应的 Transformer 位置是比较关键的，因为在它的最上层会给出 Next Token 输出。在推理过程中，GPT 会把输入上文中的重要信息通过 Attention 逐步集成到这个位置上来。这个操作也发生在 Transformer 的低层。 （3） 关系抽取 在「by」 单词位置，也就是最后一个位置的 Transformer 高层，它在低层已经集成了单词「own」 的信息，这个信息在高层，通过 Attention 把「Beat music」 对应的属性「apple」 提取出来。具体提取动作是通过某个 Attention Head 来做到的，而且这篇文章证明了 Attention Head 里会编码 < 实体 - 属性 > 信息，具体例子可以参照下图，这点对应该是个新知识（过去一般认为 Attention 主要是用来进行信息比较和搬运的，它证明了 Attention 也会存储某种知识）\"]},\"83\":{\"h\":\"3 知识点在 Transformer 中的分布\",\"t\":[\"图3.1 单语义神经元与多语义神经元示意图\",\"（1）目前发现 LLM 中存在很多单个的神经元，它们各自只对输入里某个特殊的知识点产生响应，也就是说只会被特定输入模式激活，对其它无关输入保持沉默。\",\" 1）一个神经元编码一个知识，完美一一对应，这类 Transformer 中的神经元被称为 「单语义神经元」 2）很多不同语言含义的知识点都会激活某个神经元，这类神经元被称为「多语义神经元」。\",\"Superposition 概念的含义是：假设要编码的特征的数量 n 远远多于网络参数 d，可找到办法，来用 d 维神经元编码比 d 数量大得多的 n 个特征，这种编码机制被称为 superposition，所以它是被发现存在 Transformer 结构里的一种信息压缩编码机制。\",\"图3.2 重叠编码示意图\",\"Superposition 和「多语义神经元」 关系密切，目前发现 LLM 内部是这样做的（参考 Finding Neurons in a Haystack: Case Studies with Sparse Probing）：如上图所示，LLM 的 Superposition 机制是由多个「多语义神经元」 联合构成的，每个神经元会对输入中的多个不同知识点都有响应，所以仅仅通过一个多语义神经元是无法探测当前是对谁在做出响应，但是如果有多个对某个知识点都有响应的「多语义神经元」，在它们的响应之上做个线性组合，就能探测到输入中我们想识别的那个知识点（上图中蓝色部分)。也就是说，LLM 通过组合多个「多语义神经元」来对某个具体特征或知识点进行编码。所以，多语义神经元和知识点之间的关系是多对多的映射，一个知识点会激发很多对它进行编码的「多语义神经元」，而一个 「多语义神经元」也会对多个输入知识点产生响应。\",\"（2）另外，「Polysemanticity and Capacity in Neural Networks」这个文章指出了：在模型学习过程中，为了增加模型参数的利用效率，\\n  1）「单语义神经元」 会被分配给重要特征，\\n  2）「多语义神经元」会分配给不太重要的特征，\"]},\"84\":{\"c\":[\"语言模型\"]},\"85\":{\"c\":[\"LLM\"]},\"86\":{\"h\":\"PEARL: 长文档推理提示框架\",\"t\":[\"该文介绍了 PEARL 框架，旨在提升大型语言模型对长篇文档的理解能力，在 Zero-shot 情况下，性能比GPT-4高 10.5%！PEARL 被认为是利用语言模型进行复杂推理的重要步骤，为新的推理可能性打开了大门。\",\"提示\",\"GitHub CodeBase 代码目前还没放出来\"]},\"87\":{\"c\":[\"语言模型\"]},\"88\":{\"c\":[\"推理\",\"LLM\"]},\"89\":{\"h\":\"语言模型\"},\"90\":{\"c\":[\"语言模型\"]},\"91\":{\"c\":[\"LLM\"]},\"92\":{\"c\":[\"语言模型\"]},\"93\":{\"h\":\"机器学习之强化学习概述\",\"t\":[\"强化学习（Reinforcement Learning，RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法。与监督学习不同的是，强化学习不需要带标签的输入输出对，同时也无需对非最优解的精确地纠正。强化学习被广泛认为是实现通用人工智能(AGI)的关键技术之一。\"]},\"94\":{\"h\":\"1 基本概念\",\"t\":[\"所谓强化学习，简单来说是指智能体在复杂、不确定的环境中最大化它能获得的奖励，从而达到自主决策的目的。\",\"经典的强化学习模型可以总结为图1.1的形式，任何强化学习都包含这几个基本概念：智能体、行为、环境、状态、奖励。根据状态执行动作由模型决定，执行动作后转移到哪个状态由环境决定。\",\"图1.1 强化学习示意图\"]},\"95\":{\"h\":\"2 马尔科夫决策过程\",\"t\":[\"当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质，即P(St+1​∣St​)=P(St+1​∣S1​,…,St​)，而具有马尔可夫性质的随机过程便是马尔可夫过程。 为了后续推导的方便，我们引入两个重要的量。为了评估某个状态的整体上的好坏，引入了状态值函数V(s)，其定义为状态s未来累积奖励的期望，期望越大说明当前状态越有利。引入状态动作值函数Q(s,a)，其定义为状态下采取动作后未来累积奖励的期望。\",\"Vπ​(s)=Σa∈A​π(a∣s)Qπ​(s,a)(1.1)\",\"Qπ​(s,a)=R(s,a)+γΣs′∈S​P(s′∣s,a)Vπ​(s′)(1.2)\",\"图2.1 Q和V的关系\",\"显然模型的优化目标可以用V(s0​)表示。\"]},\"96\":{\"h\":\"3 强化学习分类\",\"t\":[\"强化学习算法种类繁多，可按图3.1所示类别粗略分类。\",\"图3.1 强化学习算法分类\",\"基于模型的强化学习的特点是对环境进行建模，具体而言就是已知P(s′∣s,a)和R(s,a)的取值。如果有对环境的建模，那么智能体便能在执行动作前得知状态转移的情况即P(s′∣s,a)和奖励R(s,a)，也就不需要实际执行动作收集这些数据；否则便需要进行采样，通过与环境的交互得到下一步的状态和奖励，然后依靠采样得到的数据更新策略。\",\"无模型的强化学习可以分为基于价值的和基于策略的。基于价值的强化学习方法会学习Q(s,a)并贪婪的选择Q值最大的动作，能够学习到确定性策略。基于策略的强化学习方法则对策略进行建模，直接对π(s,a)进行优化，一般得到的是随机性策略。\",\"图3.2 基于价值和基于策略的强化学习方法\",\"确定性策略π(s)是在任意状态s下均选择最优动作，它是将状态空间S映射到动作空间A的函数。它本身没有随机性质，因此通常会结合ϵ贪心算法或向动作值中加入高斯噪声的方法来增加策略的随机性。随机性策略π(at​∣st​)是在状态st​下按照一定概率分布选择动作。它本身带有随机性，获取动作时只需对概率分布进行采样即可。\"]},\"97\":{\"c\":[\"语言模型\"]},\"98\":{\"c\":[\"Reinforcement Learning\",\"OpenAI\"]},\"99\":{\"c\":[\"机器学习之强化学习概述\"]},\"100\":{\"h\":\"机器学习之强化学习中的策略学习\",\"t\":[\"基于价值的（Policy-Based）方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。\"]},\"101\":{\"h\":\"1 策略梯度算法\"},\"102\":{\"h\":\"1.1 算法核心思想\",\"t\":[\"参数为的θ策略接受状态s，输出动作概率分布，在动作概率分布中采样动作，执行动作(形成运动轨迹τ)，得到奖励，跳到下一个状态s′。 在这样的步骤下，可以使用策略π收集一批样本，然后使用梯度下降算法学习这些样本，不过当策略π的参数更新后，这些样本不能继续被使用，还要重新使用策略π与环境互动收集数据。 在ChatGPT中参数为θ的神经网络对应RL微调的SFT模型，参数为θ′的模型对应专门采样的另一个SFT模型，动作a可以理解为回答问题输出token，s为回答问题之前的状态，s′为回答问题之后的状态。\"]},\"103\":{\"h\":\"1.2 评价标准\",\"t\":[\"图1.1 智能体与环境交互示意图\",\"给定智能体或演员的策略参数θ，可以计算某一条轨迹τ发生的概率为轨迹τ来源于在特定的环境状态下采取特定动作的序列，而特定的状态、特定的动作又分别采样自智能体的动作概率分布pθ​(at​∣st​)、状态的转换概率分布p(st+1​∣st​,at​)。\",\"pθ​(τ)​=p(s1​)pθ​(a1​∣s1​)p(s2​∣s1​,a1​)pθ​(a2​∣s2​)p(s2​∣s1​,a1​)⋅⋅⋅=p(s1​)t=1∏T​pθ​(at​∣st​)p(st+1​∣st​,at​)​(1.1)\",\"由于每一个轨迹τ都有其对应的发生概率，对所有τ出现的概率与对应的奖励进行加权最后求和，即可得期望值。\",\"Rθ​=τ∑​R(τ)pθ​(τ)=Eτ∼pθ​(τ)​[R(τ)](1.2)\",\"图1.2 策略梯度的实现流程\",\"根据按照蒙特卡洛方法近似求期望的原则，可以采样N条轨迹τ并计算每一条轨迹的值，再把每一条轨迹的值加起来除以N取平均，即(τn上标n代表第n条轨迹，而、则atn​、stn​分别代表第n条轨迹里时刻t的动作、状态。\",\"由此可以推导出策略梯度定理\",\"(1)即在采样到的数据里面，采样到在某一个状态st​要执行某一个动作at​，(st​,at​)是在整个轨迹的里面的某一个状态和动作的对。\",\"(2)为了最大化奖励，假设在st​执行at​，最后发现的奖励是正的，就要增加概率。反之，如果在st​执行at​会导致的奖励变成负的，就要减少概率。\",\"(3)用梯度上升来更新参数，原来有一个参数θ，把θ加上梯度∇Rθ​，当然要有一个学习率η（类似步长、距离的含义），学习率可用 Adam、RMSProp等方法调整。\"]},\"104\":{\"h\":\"2 优势演员-评论家算法\",\"t\":[\"目的：为避免奖励总为正增加基线\",\"图2.1 AC原理\",\"假设某一状态下有三个动作，分别是a,b,c，奖励都是正的。根据公式,我们希望将这三个动作的概率以及对数概率都拉高，但是它们前面的权重不一样，有大有小，所以权重大的，上升的多一点；权重小的，上升的少一些，又因为对数概率是一个概率，三个动作的和要为0，那么在做完归一化后，上升多的才会上升，上升的少的就是下降的。\",\"为了解决奖励总是正的的问题，也为避免方差过大，需要在之前梯度计算的公式基础上加一个基准线b，此b指的baseline。\"]},\"105\":{\"h\":\"3. TRPO\",\"t\":[\"信任域策略优化：使用KL散度解决两个分布相差大或步长难以确定的问题。\",\"JTRP0θ′​(θ)=E(st​,at​)∼nθ′​​[pθ′​(at​∣st​)pθ​(at​∣st​)​Aθ′(st​,at​)],KL(θ,θ′)<δ(3.1)\"]},\"106\":{\"h\":\"4. PPO\",\"t\":[\"见PPO详解\"]},\"107\":{\"h\":\"参考\",\"t\":[\"[1] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz. Trust Region Policy Optimization. In: Proceedings of the 32nd International Conference on Machine Learning (ICML 2015), Lille, France, July 6-11, 2015, ACM, 2015:1889-1897\"]},\"108\":{\"c\":[\"语言模型\"]},\"109\":{\"c\":[\"Reinforcement Learning\",\"Policy-based\",\"OpenAI\"]},\"110\":{\"c\":[\"机器学习之强化学习中的策略学习\"]},\"111\":{\"h\":\"机器学习之强化学习中的价值学习\",\"t\":[\"基于价值的（Value-Based）方法输出的是动作的价值，选择价值最高的动作，也就是通过价值选动作。价值学习经典的算法有Sarsa和Q-learning算法。\"]},\"112\":{\"h\":\"1 SARSA\",\"t\":[\"图1.1 Sarsa伪代码\",\"SARSA（State-Action-Reward-State-Action）是一个学习马尔科夫决策过程策略的算法，从名称我们可以看出其学习更新函数依赖的5个值(s,a,r,s′,a′)。SARSA是on-policy的强化学习方法，目标策略与行为策略保持一致。\",\"图1.2 Sarsa策略更新\",\"根据状态图可以理解SARSA的更新规则。\"]},\"113\":{\"h\":\"2 Q-learning\",\"t\":[\"图2.1 Q-learning伪代码\",\"Q-learning同样根据下一步的状态更新Q值，和SARSA的区别在于直接用下一步的最大Q值作为估计来更新。\",\"图2.2 Q-learning策略更新\"]},\"114\":{\"h\":\"3 on-policy和off-policy\",\"t\":[\"最后来明确下on-policy和off-policy的概念。强化学习包含两个策略，行为策略，智能体遵循该策略选择动作。与之相对的目标策略是我们优化的对象，也是强化学习模型推断时使用的策略。\",\"SARSA的目标策略是优化Q值，根据公式我们知道SARSA是通过预估下一步的收益来更新自身的Q值，而且下一步是按照行为策略选出的，所以它的目标策略与行为策略保持一致，我们称SARSA是on-policy算法。\",\"而Q-learning算法的目标策略是优化下一步的Q表中的最大值，目标策略与行为策略并不一致，我们称Q-learning是off-policy算法。\",\"简单来说，就是看行为策略和目标策略是否相同。\"]},\"115\":{\"c\":[\"语言模型\"]},\"116\":{\"c\":[\"Reinforcement Learning\",\"Value-based\",\"OpenAI\"]},\"117\":{\"c\":[\"机器学习之强化学习中的价值学习\"]},\"118\":{\"h\":\"To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis\",\"t\":[\"新加坡国立大学的研究人员发布了一篇全新的论文《To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis》，研究了大语言模型的Epoch次数设置问题。文章讨论了在重复的数据集上进行多次训练对大语言模型性能的影响。作者指出，随着大语言模型的规模和训练数据集中Token数量的增加，模型性能受到很大的影响。然而，现有的数据集中的Token数量有限，模型参数规模的增长可能会导致Token不足的情况，被称为\\\"Token危机\\\"。\"]},\"119\":{\"h\":\"1 问题提出\",\"t\":[\"作者提出了一系列问题：\",\"预训练数据集重复的影响是什么？\",\"影响多次轮次（Epoch）训练效果下降的原因是什么？\",\"正则化可以降低多Epoch的影响吗\",\"通过混合专家模型（Mixture of Experts，MoE）扫描确定稠密模型的最佳超参数\",\"作者采用T5模型和C4数据集进行实验，得出结论。\"]},\"120\":{\"h\":\"2 背景\",\"t\":[\"在此前的研究中，大家发现大语言模型的规模和训练数据集中词元（Token）的数量对模型的性能有很大的影响。大模型扩展定律都认为模型的规模与训练数据的规模必须同时扩大才能让模型产生更好的性能。但是，Token数量似乎并不是很足够，如下图所示是作者研究的模型参数规模增长和目前互联网是可用的数据集Token数量增长情况。\",\"图2.1 模型参数规模增长和目前互联网是可用的数据集Token数量增长情况\",\"例如，Meta AI训练的LLaMA-65B模型用了1.4万亿Token，而2023年全球的Token估计只有9万亿！按照目前模型规模的发展情况，在2023年-2027年几年的时间里，我们的模型将把全球所有数据集的Token都训练完成，此后，我们很可能陷入缺少Token训练的地步，这被作者称为Token危机。\",\"大语言模型的训练Epoch通常都是1-2次，多的也都是个位数。2022年，Hoffmann的论文中提出用重复的Token训练大语言模型会让模型降低性能，而Taylor在训练Galactica模型时候发现Epoch次数达到4次也可以提升模型效果。显然，在重复数据集上训练多次对模型的影响目前还没有一个相对完善的研究。但是这个问题很重要！\"]},\"121\":{\"h\":\"3 实验结论\"},\"122\":{\"h\":\"3.1 模型参数规模与Token数量需要匹配\",\"t\":[\"首先是模型参数规模的增长与模型需要的Token数量基本是呈线性的。\",\"作者比较了在各种计算预算下掩码标记预测的验证准确性。当较大的模型优于较小的模型时，表明较小的模型已收到足够的Token。用于训练较小模型的Token数量可以被视为完整训练的Token要求。\",\"图3.1 模型参数与训练所需Token关系\",\"这意味如果你要充分训练一个大型语言模型（Large Language Model，LLM），需要根据它的参数数量来收集足够的Token。\"]},\"123\":{\"h\":\"3.2 多轮Epoch的训练会降低模型性能\",\"t\":[\"作者分别使用C4数据集的子集，然后只是用了其中一部分数据集，并通过设置多次Epoch来让模型总的训练过的Token差不多水平，观察模型的性能。\",\"如图3.2所示，可以看到，数据集重复的次数越多，模型的性能越差：\",\"图3.2 数据集重复的次数与模型的性能的关系\",\"此外，如果Token数量不够，模型参数规模越大，越容易出现过拟合的现象。\",\"尽管重复数据上的训练会降低预训练模型的效果，但是这种方式对于下游任务的影响也没有人探测过。因此，作者也继续做了这方面的研究，得到的结论是在下游任务上也会出现，即如果预训练模型在重复数据上进行，尽管训练的总的Token数量可能一致，但是，其下游任务的效果也是更差！\",\"因此，我们的下一个调查围绕着使用重复数据训练 LLM。 为了探索这一点，我们随机选择了 C4 数据集的几个子集，其中包含大约 235,229 和 2^27 个标记，导致每个标记分别重复 1、2^6 和 2^8 次。结果如图 3 所示，展示了预期的性能 使用重复标记训练 LLM 时的退化。 此外，我们观察到较大的模型在Token危机条件下更容易过度拟合。具体而言，在没有足够大的数据集的情况下进行训练时，T5-XL 尽管消耗更多的计算资源，但在访问 4x 数据时比 T5-Large 表现更差（ 2^29 对 2^27 个Token）\"]},\"124\":{\"h\":\"3.3 更大规模的数据集会缓解重复Epoch对模型性能下降的影响\",\"t\":[\"在这个实验中，作者将重复的次数固定，然后看模型在不同规模数据集上重复训练的性能影响。如图3.3所示。\",\"图3.3 重复训练的性能影响\",\"可以看到，当在227个Token和229个Token上重复训练28次之后发现，前者更容易出现过拟合，而229Token的数据集上重复训练，模型性能下降不明显。\"]},\"125\":{\"h\":\"3.4 提高数据集的质量也无法挽救重复训练带来的过拟合\",\"t\":[\"Taylor在训练银河战舰（Galactica）模型时候认为他之所以用4 Epoch能提高训练效果可能是因为他的数据集质量更好。然而，本文的作者发现，相对更高质量的数据集并不能降低重复训练带来的影响。\",\"图3.4 在C4数据集和Wikipedia数据集上分别训练模型的结果\",\"作者用相同的重复策略在C4数据集和维基（Wikipedia）数据集上分别训练模型，发现二者都会因为重复训练带来模型性能的下降。这里的Wikipedia数据集质量相对C4更好一点。说明相对提高数据集质量可能不会影响重复训练的负面效应。\"]},\"126\":{\"h\":\"3.5参数数量和FLOPs在重复训练上的影响\",\"t\":[\"模型规模的增长其实表现在2个方面，一个是模型参数，一个是模型所需要的计算量。模型参数相同的情况下，采用不同的模型架构所需要的浮点运算次数（Floating Point Operations，FLOPs）是不同的。作者对比了MoE架构，并采用参数共享（ParamShare）方法降低相同参数模型的FLOPs。\",\"图3.5 模型参数量与FLOPs对模型性能的影响\",\"经过测试发现，FLOPs较大的模型性能会更好一点，但是依然无法有效降低重复训练带来的模型损失。\"]},\"127\":{\"h\":\"3.6 小计算量模型的过拟合趋势与大计算量的差不多\",\"t\":[\"这是一个有趣的发现，尽管在前面的实验中，相同参数规模不同计算量的模型都会受到重复数据集训练的影响。但是二者在模型性能表现的趋势上类似。\",\"这意味着我们可以利用较低计算量的模型预估大模型的训练结果。在大语言模型的训练中，训练成本很高。采用类似的模型，但是更低的计算量来预估模型的表现将十分有价值！\"]},\"128\":{\"h\":\"3.7 多样的训练目标可以减轻多Epoch下降吗？\",\"t\":[\"目前大语言模型的训练目标有很多，例如预测下一个单词是神什么的生成式目标，也有把单词masked之后用来判断是什么单词的判别式目标。如果语言模型的训练目标多样化，那么实际上更加可能受到多Epoch带来的性能损失。\",\"例如，UL2这种模型就不适合多Epoch的训练，MLM这种模型受到的影响反而更小。\"]},\"129\":{\"h\":\"3.8 Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多Epoch的影响\",\"t\":[\"正则技术，如随机丢弃（Dropout）、路径随机失活（Droppath）、权重衰减（Weight Decay，WD）等都是常用的防止过拟合的技术。而多Epoch的负面影响也都是过拟合。因此，作者研究了这些正则技术是否可以降低多Epoch的影响。\",\"在目前超过100亿参数规模的大语言模型中，如GPT-3、PaLM、LLaMA等，都没有使用Dropout（可能是因为太慢了）。而前面说的Galactica训练使用了，这是Galactica能够训练4 Epoch提升性能的最重要的原因。\",\"图3.6 Dropout对模型性能的影响\"]},\"130\":{\"h\":\"3.9 在训练过程中逐渐使用Dropout是有效的策略\",\"t\":[\"在前面的讨论中，作者已经发现Dropout可以降低多Epoch的影响，但是Dropout会降低模型的性能。因此，作者考虑不在全部训练中使用Dropout，而是逐渐引入。\",\"最终发现，如果前期训练不用Dropout，在后续的迭代中使用Dropout也是有效的！\"]},\"131\":{\"h\":\"3.10 Dropout对不同规模模型的影响不同\",\"t\":[\"尽管前面已经证明Dropout使用可以降低多Epoch的影响，但是在不同规模模型下是不同的。对于规模较大的模型，Dropout不能有效降低多Epoch带来的坏处！\"]},\"132\":{\"h\":\"3.11 通过MoE扫描确定稠密模型的最佳超参数\",\"t\":[\"最后一个结论其实与Epoch关系不大，作者强调的是MoE的模型表现与大模型真正的训练有类似的趋势，因此用MoE去提前预估大模型的性能，做参数调优是一个非常好的思路。\"]},\"133\":{\"h\":\"4 总结\",\"t\":[\"根据前面的实验我们知道，如果在Token数量一定的数据集上做多Epoch的模型训练，会影响模型的性能，降低模型的效果。这在预训练和下游任务都会产生影响。但是，随着模型的发展，高质量数据集的Token数将很快用完。而采用正则技术虽然会影响模型训练效率，但是会降低这种影响。\",\"所有的一切表明，在不久的将来，我们会面临Token训练完的危机，这时候多Epoch显然不是好的方向，这意味着我们应该寻找新的大语言模型的方向，或者说可能很快我们也会达到现有LLM的天花板。\"]},\"134\":{\"c\":[\"语言模型\"]},\"135\":{\"c\":[\"模型\",\"深度学习\",\"机器学习\"]},\"136\":{\"h\":\"Unlimiformer 介绍\",\"t\":[\"上海人工智能实验室联合商汤科技共同提出一种新的 UniFormer（Unified Transformer）框架， 它能够将卷积与自注意力的优点通过 Transformer 进行无缝集成。UniFormer 模块的相关性聚合在浅层与深层分别配备了局部全局token，能够同时解决冗余与依赖问题，实现了高效的特征学习。\"]},\"137\":{\"h\":\"1 问题提出\",\"t\":[\"变换网络（Transformer）是时下最强大的序列到序列（Sequence-to-Sequence, Seq2Seq）架构。预训练 Transformer 通常具有 512（例如 BERT）或 1024 个（例如 BART）Token 的个上下文窗口，这对于目前许多文本摘要数据集（XSum、CNN/DM）来说是足够长的。\",\"但 16384 并不是生成所需上下文长度的上限：涉及长篇叙事的任务，如书籍摘要（Krys-´cinski et al.，2021）或叙事问答（Kociskýet al.，2018），通常输入超过 10 万个 Token。维基百科文章生成的挑战集（Liu*et al.，2018）包含超过 50 万个 Token 的输入。生成式问答中的开放域任务可以从更大的输入中综合信息，例如回答关于维基百科上所有健在作者的文章的聚合属性的问题。图 1 根据常见的上下文窗口长度绘制了几个流行的摘要和问答数据集的大小；最长的输入比 Longformer 的上下文窗口长 34 倍以上。\",\"图1.1 数据集Token统计\",\"在这些超长输入的情况下，普通变换网络（Vanilla Transformer, VT） 无法进行缩放，因为原生注意力机制具有平方级的复杂度。长输入 Transformer 虽然比标准 Transformer 更高效，但仍需要大量的计算资源，这些资源随着上下文窗口大小的增加而增加。此外，增加上下文窗口需要用新的上下文窗口大小从头开始重新训练模型，计算上和环境上的代价都不小。\",\"在「Unlimiformer: Long-Range Transformers with Unlimited Length Input」一文中，来自卡内基梅隆大学的研究者引入了 Unlimiformer。这是一种基于检索的方法，这种方法增强了预训练的语言模型，以在测试时接受无限长度的输入。\",\"论文链接：https://arxiv.org/pdf/2305.01625v1.pdf\",\"Unlimiformer 可以被注入到任何现有的编码器 - 解码器 Transformer 中，能够处理长度不限的输入。给定一个长的输入序列，Unlimiformer 可以在所有输入 Token 的隐藏状态上构建一个数据存储。然后，解码器的标准交叉注意力机制能够查询数据存储，并关注前 k 个输入 Token。数据存储可以存储在 GPU 或 CPU 内存中，能够次线性查询。\",\"Unlimiformer 可以直接应用于经过训练的模型，并且可以在没有任何进一步训练的情况下改进现有的 checkpoint。Unlimiformer 经过微调后，性能会得到进一步提高。本文证明，Unlimiformer 可以应用于多个基础模型，如 BART（Lewis et al.，2020a）或 PRIMERA（Xiao et al.，2022），且无需添加权重和重新训练。在各种长程 Seq2Seq 数据集中，Unlimiformer 不仅在这些数据集上比 Longformer（Beltagy et al.，2020b）、SLED（Ivgi et al.，2022）和记忆变换网络（Memorizing Transformers, MT）（Wu et al.，2021）等强长程 Transformer 表现更好，而且本文还发现 Unlimiform 可以应用于 Longformer 编码器模型之上，以进行进一步改进。\"]},\"138\":{\"h\":\"2 Unlimiformer技术原理\",\"t\":[\"由于编码器上下文窗口的大小是固定的，Transformer 的最大输入长度受到限制。然而，在解码过程中，不同的信息可能是相关的；此外，不同的注意力头可能会关注不同类型的信息（Clark et al.，2019）。因此，固定的上下文窗口可能会在注意力不那么关注的 Token 上浪费精力。\",\"在每个解码步骤中，Unlimiformer 中每个注意力头都会从全部输入中选择一个单独的上下文窗口。通过将 Unlimiformer 查找注入解码器来实现：在进入交叉注意力模块之前，该模型在外部数据存储中执行 k 最近邻 (kNN) 搜索，在每个解码器层中的每个注意力头中选一组 Token 来参与。\"]},\"139\":{\"h\":\"2.1 Unlimiformer编码\",\"t\":[\"为了将比模型的上下文窗口长度更长的输入序列进行编码，本文按照 Ivgi et al. (2022) 的方法对输入的重叠块进行编码 (Ivgi et al. ,2022)，只保留每个 chunk 的输出的中间一半，以确保编码过程前后都有足够的上下文。最后，本文使用 Faiss (Johnson et al., 2019) 等库对数据存储中的编码输入进行索引（Johnson et al.，2019）。\"]},\"140\":{\"h\":\"2.2 检索增强的交叉注意力机制\",\"t\":[\"在标准的交叉注意力机制中，Transformer 的解码器关注编码器的最终隐状态，编码器通常截断输入，并仅对输入序列中的前 k 个 Token 进行编码。\",\"本文不是只关注输入的这前 k 个 Token，对于每个交叉注意头，都检索更长的输入系列的前 k 个隐状态，并只关注这前 k 个。这样就能从整个输入序列中检索关键字，而不是截断关键字。在计算和 GPU 内存方面，本文的方法也比处理所有输入 Token 更便宜，同时通常还能保留 99% 以上的注意力性能。\",\"图 2 显示了本文对 Seq2Seq Transformer 架构的更改。使用编码器对完整输入进行块编码，并将其存储在数据存储中；然后，解码时查询编码的隐状态数据存储。kNN 搜索是非参数的，并且可以被注入到任何预训练的 Seq2Seq Transformer 中，详情如下。\",\"图2.1 Unlimiformer原理图\"]},\"141\":{\"h\":\"3 实验结果\"},\"142\":{\"h\":\"3.1 长文档摘要\",\"t\":[\"图3显示了长文本（4k 及 16k 的 Token 输入）摘要数据集中的结果。\",\"图3.1 长文本（4k 及 16k 的 Token 输入）摘要数据集中的结果\",\"在图 4 的训练方法中，Unlimiformer 能够在各项指标上达到最优。\",\"图3.2 使用长范围训练方法的试验结果\"]},\"143\":{\"h\":\"3.2 书籍摘要\",\"t\":[\"图 5 显示了在书籍摘要上的结果。可以看到，基于 BARTbase 和 PRIMERA，应用Unlimiformer 都能取得一定的改进效果。\",\"图3.3 书籍摘要的试验结果\",\"原文链接\"]},\"144\":{\"c\":[\"语言模型\"]},\"145\":{\"c\":[\"摘要\",\"Transformer\",\"机器学习\"]},\"146\":{\"h\":\"ChatGLM2架构升级\",\"t\":[\"ChatGLM2-6B使用了GLM的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B在MMLU（+23%）、CEval（+33%）、GSM8K（+571%）、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。\",\"（1）更强大的性能：基于ChatGLM初代模型的开发经验，官方全面升级了 ChatGLM2-6B 的基座模型。\",\"（2）更长的上下文：基于FlashAttention技术，官方将基座模型的上下文长度（Context Length）由ChatGLM-6B的2K扩展到了32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。但当前版本的ChatGLM2-6B对单轮超长文档的理解能力有限，官方会在后续迭代升级中着重进行优化。\",\"（3）更高效的推理：基于Multi-Query Attention技术，ChatGLM2-6B有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4量化下，6G显存支持的对话长度由1K提升到了8K。\",\"（4）更开放的协议：ChatGLM2-6B权重对学术研究完全开放，在获得官方的书面许可后，亦允许商业使用。如果您发现官方的开源模型对您的业务有用，官方欢迎您对下一代模型ChatGLM3研发的捐赠。\"]},\"147\":{\"h\":\"1 基座模型的升级\"},\"148\":{\"h\":\"1.1 Transformer架构\",\"t\":[\"Encoder-Decoder变成Decoder-only。\"]},\"149\":{\"h\":\"1.2 词汇表大小\",\"t\":[\"130344减小到64794。\",\"由于抛弃了NLU任务，只保留NLG生成任务，因此不再包含mask token。\"]},\"150\":{\"h\":\"1.3 模型结构\"},\"151\":{\"h\":\"1.3.1 总体架构\",\"t\":[\"ChatGLM-6B的总体架构如下所示。\",\"<bound method Module.modules of ChatGLMForConditionalGeneration( (Transformer): ChatGLMModel( (word_embeddings): Embedding(150528, 4096) (layers): ModuleList( (0-27): 28 x GLMBlock( (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True) (Attention): SelfAttention( (rotary_emb): RotaryEmbedding() (query_key_value): QuantizedLinear(in_features=4096, out_features=12288, bias=True) (dense): QuantizedLinear(in_features=4096, out_features=4096, bias=True) ) (post_Attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True) (mlp): GLU( (dense_h_to_4h): QuantizedLinear(in_features=4096, out_features=16384, bias=True) (dense_4h_to_h): QuantizedLinear(in_features=16384, out_features=4096, bias=True) ) ) ) (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=4096, out_features=150528, bias=False) )> \",\"ChatGLM2-6B的总体架构如下所示。\",\"ChatGLMForConditionalGeneration( (Transformer): ChatGLMModel( (embedding): Embedding( (word_embeddings): Embedding(65024, 4096) ) (rotary_pos_emb): RotaryEmbedding() (encoder): GLMTransformer( (layers): ModuleList( (0-27): 28 x GLMBlock( (input_layernorm): RMSNorm() (self_Attention): SelfAttention( (query_key_value): Linear(in_features=4096, out_features=4608, bias=True) (core_Attention): CoreAttention( (Attention_dropout): Dropout(p=0.0, inplace=False) ) (dense): Linear(in_features=4096, out_features=4096, bias=False) ) (post_Attention_layernorm): RMSNorm() (mlp): MLP( (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False) (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False) ) ) ) (final_layernorm): RMSNorm() ) (output_layer): Linear(in_features=4096, out_features=65024, bias=False) ) ) \"]},\"152\":{\"h\":\"1.3.2 参数量\",\"t\":[\"ChatGLM-6B的参数量如下所示。\",\"总参数量：6,255,206,400 Transformer：6,255,206,400 Transformer.word_embeddings：150,528*4,096=616,562,688 Transformer.layers：201,379,840*28=5,638,635,520 Transformer.layers.0：67,125,248+134,238,208+8192*2=201,379,840 Transformer.layers.0.input_layernorm：4,096*2=8,192 Transformer.layers.0.Attention：50,343,936+16,781,312=67,125,248 Transformer.layers.0.Attention.rotary_emb：0 Transformer.layers.0.Attention.query_key_value：4,096*12,288+12,288=50,343,936 Transformer.layers.0.Attention.dense：4,096*4,096+4,096=16,781,312 Transformer.layers.0.post_Attention_layernorm：4,096*2=8,192 Transformer.layers.0.mlp：67,125,248+67,112,960=134,238,208 Transformer.layers.0.mlp.dense_h_to_4h：4,096*16,384+16,384=67,125,248 Transformer.layers.0.mlp.dense_4h_to_h：16,384*4,096+4,096=67,112,960 Transformer.final_layernorm：4,096*2=8,192 lm_head：4,096*150,528=616,562,688 \",\"ChatGLM2-6B的参数量如下所示。\",\"总参数量:6243584000 Transformer:6243584000 Transformer.embedding:266,338,304 Transformer.embedding.word_embeddings:65024*4096=266,338,304 Transformer.rotary_pos_emb:0 Transformer.encoder:5,710,907,392 Transformer.encoder.layers:5710903296 Transformer.encoder.layers.0:203960832 Transformer.encoder.layers.0.input_layernorm:4096 Transformer.encoder.layers.0.self_Attention:35656192 Transformer.encoder.layers.0.self_Attention.query_key_value:18878976 Transformer.encoder.layers.0.self_Attention.core_Attention:0 Transformer.encoder.layers.0.self_Attention.core_Attention.Attention_dropout:0 Transformer.encoder.layers.0.self_Attention.dense:16777216 Transformer.encoder.layers.0.post_Attention_layernorm:4096 Transformer.encoder.layers.0.mlp:168296448 Transformer.encoder.layers.0.mlp.dense_h_to_4h:112197632 Transformer.encoder.layers.0.mlp.dense_4h_to_h:56098816 Transformer.encoder.final_layernorm:4096 Transformer.output_layer:266,338,304 \"]},\"153\":{\"h\":\"1.3.3 归一化层\",\"t\":[\"由LayerNorm变成RMSNorm。\",\"RMSNorm是对LayerNorm的一个改进，没有做re-center操作（移除了其中的均值项），可以看作LayerNorm在均值为0时的一个特例。论文通过实验证明，re-center操作不重要。\"]},\"154\":{\"h\":\"1.3.4 激活函数\",\"t\":[\"由GeLU变成SwiGLU。\"]},\"155\":{\"h\":\"2 FlashAttention\",\"t\":[\"这是一个在cuda编程层面提高模型训练速度的技术。\",\"FlashAttention主要是为了做训练提速的，当输入序列较长时，由于self-Attention的时间和内存困惑度会随着输入序列长度的增加成二次方增长，Transformer的计算过程缓慢且耗费内存，所以制约了长度的扩展。因此，如果能够把计算量降下去，长度就自然可以进行扩展。\",\"我们再深入到底层GPU运算。GPU中存储单元主要有HBM和SRAM，其中：HBM容量大但是访问速度慢，SRAM容量小却有着较高的访问速度。例如，A100 GPU有40-80GB的HBM，带宽为1.5-2.0TB/s；每108个流式多核处理器各有192KB的片上SRAM，带宽估计约为19TB/s。\",\"我们再来看看实际做Attention时做的运算，主要包括S=QK、P=softmax(S)、O=PV这三个反复执行的操作。就GPU内存利用而言，注意力层面临的主要问题是中间结果P、S和O的大小(n,n)，需要将它们保存至HBM中，并在注意力运算之间再次读取。因此，FlashAttentio算法，主要解决的是将P、S和O从HBM移动到SRAM，以及反向移动这个瓶颈，并最终减少对HBM的访问。\",\"具体的，其主要思想是将输入的Q、K和V矩阵划分成块（block），将这些块从HBM加载至SRAM中，然后根据这些块来计算注意力输出，这个过程被称为“切片（tiling）”。\",\"图2.1 FlashAttention原理示意图\",\"如上图所示，左图中FlashAttention使用切片技术，防止将大型n × n注意力矩阵（虚线框内）存储到HBM中。在外部循环（红色箭头）中，FlashAttention循环遍历K和V矩阵的块，并将它们加载到SRAM中。在每个块中，FlashAttention循环遍历Q矩阵的块（蓝色箭头），将它们加载到SRAM中，并将注意力计算的输出写回至HBM。\"]},\"156\":{\"h\":\"3 Multi-Query Attention\",\"t\":[\"该方案目的的是为了保证模型效果的同时加快Decoder生成token的速度。\",\"其实现的逻辑在于：原始的多头注意力（Multi-Head Attention，MHA）在每个注意力头都有单独的线性层用于K和V矩阵，在推理过程中，为了避免重复计算，解码器中之前的词元的键（key）和值（value）被缓存，因此每生成一个词元，GPU内存使用量都会增加。\",\"与此不同，Multi-Query Attention让所有的头之间共享同一份Key和Value矩阵，每个头只单独保留一份Query参数，即只需保留大小为(n,k)和(n,v)的两个矩阵，从而大大减少Key和Value矩阵的参数量。\",\"Multi-Query Attention计算中的维度变化如下所示。\",\"隐藏层输入：torch.Size([1, 1, 4096]) 经过QKV的线性层：Linear(in_features=4096, out_features=4608, bias=True) 变成QKV：torch.Size([1, 1, 4608]) 拆分成Q，K，V: query: torch.Size([1, 1, 4608]) key: torch.Size([1, 1, 256]) value: torch.Size([1, 1, 256]) Q,K,V分别拆分成多头： query: torch.Size([1, 1, 32, 128]) key: torch.Size([1, 1, 2, 128]) value: torch.Size([1, 1, 2, 128]) K，V分别复制头： key: torch.Size([1, 1, 2, 1, 128]) key: torch.Size([1, 1, 2, 16, 128]) key: torch.Size([1, 1, 32, 128]) 最终参与多头计算的Q，K，V： query: torch.Size([1, 1, 32, 128]) key: torch.Size([1, 1, 32, 128]) value: torch.Size([1, 1, 32, 128]) \"]},\"157\":{\"h\":\"4 测试结果\",\"t\":[\"图4.1 ChatGLM和ChatGLM2对比\"]},\"158\":{\"c\":[\"语言模型\"]},\"159\":{\"c\":[\"GLM\"]},\"160\":{\"h\":\"ChatGPT相关技术介绍\",\"t\":[\"首先回顾了GPT系列模型的发展历程，然后介绍了ChatGPT模型最重要的技术指令微调，最后介绍了上下文学习。\"]},\"161\":{\"h\":\"1 GPT系列模型发展历程\",\"t\":[\"2020年7月，OpenAI发布了模型索引为的davinci的初代GPT-3论文，从此它就开始不断进化。总体分为两大类，第一类是在代码上训练，称其为Codex系列；第二类是使用指令微调的InstructGPT系列。\",\"2022年5-6月发布的text-davinci-002是一个基于code-davinci-002的有监督指令微调（Supervised Instruction Tuning）模型。然后是text-davinci-003和 ChatGPT，它们都在2022年11月发布，是使用的基于人类反馈的强化学习的版本指令微调（Instruction Tuning with Reinforcement Learning from Human Feedback）模型的两种不同变体。\",\"图1.1 GPT系列模型树\"]},\"162\":{\"h\":\"2 指令微调\",\"t\":[\"指令微调（Instruction Tuning）的提出来自于Google的一篇论文[1]，结合了微调和提示两个范式的优点，即用prompt格式的训练数据进行finetune，以使模型具备人类倾向的回答问题能力。\",\"在 2022 年 3 月，OpenAI 发布了指令微调[2]的论文，其监督微调（Supervised Instruction Tuning，SFT）的部分对应了davinci-instruct-beta和text-davinci-001。\",\"We focus on fine-tuning approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF) to fine-tune GPT-3 to follow a broad class of written instructions.\"]},\"163\":{\"h\":\"3 模型的训练方法和数据集\",\"t\":[\"图3.1 模型训练步骤\",\"（1）SFT阶段，使用人工标注prompt数据集的答案用来finetune模型。这一步得到的模型是davinci-instruct-beta。\",\"（2）奖励模型阶段，通过对模型输出答案打分来训练奖励模型（Reward Model，RM）。RM就是基于第一步生成的SFT6B版本，去除最后一次反嵌入层，起到了扩充LLM模型高质量训练数据的作用。 推理打分：选择了一部分prompt，由SFT模型随机生成多个答案（4-9个），人工对这些答案从到坏进行排序。这构成了一个新的监督训练数据集，排序是这些数据的label。新的数据集被用来训练RM。--ChatGPT是如何工作的\",\"（3）PPO阶段，使用RM来更新ppo策略，从而使GPT产生的答案更偏向于标注人员的喜好。\",\"表3.1 InstructGPT的训练数据构成\",\"据推测，ChatGPT使用了和text-davinci-003相同的训练方法，采用了不同的数据集，而且更加注重生成答案的无害性和对话性。\",\"合理分析：OpenAI官网的ChatGPT的训练流程和InstructGPT基本一致，除了ChatGPT是基于GPT3.5系列的，再根据InstructGPT发布后半年多才发布ChatGPT，推测是因为初始PPO策略训练的模型太过随心所欲，不能满足无害性等要求，而在调试的过程中GPT3.5系列已经训练完成，所以直接基于GPT3.5系列进行训练。\"]},\"164\":{\"h\":\"4 上下文学习\",\"t\":[\"上下文学习（In-context Learning，ICL）[3]是从类比中学习，和人类的决策相似。\",\"ICL只存在一次前向传播中，还是会被模型记住？论文中ICL的测试数据，类似于下图所示，每次预测都需要结合之前的几个demonstration，由此推测ICL并不会被模型记住。结合对text-davinci-003的测试，在一次调用中教会它数学题，之后单独询问，模型并不能正确回答，由此可以证明ICL只存在于一次前向传播。\",\"图4.1 ICL和微调的区别\",\"ICL是一个元优化的过程，可以看做隐性微调。GPT首先根据演示示例生成元梯度，然后将这些元梯度应用于原始GPT以构建ICL模型。\",\"Considering that ICL directly takes effect on only the attention keys and values.\",\"ICL只对attention有影响。\"]},\"165\":{\"h\":\"5 参考\",\"t\":[\"[1] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, et al. Finetuned language models are zero-shot learners. In: Proceedings of the 10th International Conference on Learning Representations (ICLR 2022), Online, April 25-29, 2022, OpenReview.net, 2022: 1-46\",\"[2] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, et al. Training language models to follow instructions with human feedback. In: Advances in Neural Information Processing Systems 35 (NeurIPS 2022), New Orleans, Louisiana, USA, November 28-December 9, 2022, MIT Press, 2022: 27730-27744\",\"[3] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, et al. Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. arXiv, 2023\"]},\"166\":{\"c\":[\"语言模型\"]},\"167\":{\"c\":[\"OpenAI\",\"Google\",\"Instruct Tuning\",\"In-context Learning\",\"ChatGPT\"]},\"168\":{\"c\":[\"ChatGPT相关技术介绍\"]},\"169\":{\"h\":\"基于Encoder和Decoder的三种架构\",\"t\":[\"Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。\"]},\"170\":{\"h\":\"1 Encoder-Decoder\",\"t\":[\"图1.1 语言模型进化树\",\"其中Encoder单层包括Self-Attention和MLP，Decoder单层包括Self-Attention，Cross-Attention和MLP。 Cross-Attention的特殊之处在于输入的K和V来自Encoder的输出，而Q来自于自己的Self-Attention的输出。\",\"图1.2 标准transformer架构\",\"图1.3 Encoder的输出流向\"]},\"171\":{\"h\":\"1.1 T5\",\"t\":[\"T5模型的Encoder和Decoder区分的比较明确，在定义时就给出了。\",\"encoder_config = copy.deepcopy(config) encoder_config.is_decoder = False encoder_config.use_cache = False encoder_config.is_encoder_decoder = False self.encoder = T5Stack(encoder_config, self.shared) decoder_config = copy.deepcopy(config) decoder_config.is_decoder = True decoder_config.is_encoder_decoder = False decoder_config.num_layers = config.num_decoder_layers self.decoder = T5Stack(decoder_config, self.shared) \"]},\"172\":{\"h\":\"1.2 ChatGLM\",\"t\":[\"ChatGLM之所以是Decoder-Encoder架构，并非是由于结构的原因，而在于它的功能设计，事实上，ChatGLM的所有layer结构一致，并没有Encoder，Decoder之分。\",\"<输入><gmask><bos><输出><eos> \",\"特殊之处在于它的Attention mask，自开始直到gmask是一部分，自bos直到eos是另一部分，被分为两大部分，其中第一部分具有双向特性，左右的token都会影响模型对中间token的预测，符合类Bert模型的MaskLM的特性，因此偏向于Encoder自然语言理解的功能；而第二部分只是单向特性，仅左边token会影响模型对中间token的预测，而右边的不会，符合类GPT模型的AutoRegressiveLM的特性，因此偏向于Decoder自然语言生成的功能。\"]},\"173\":{\"h\":\"2 Encoder-only\",\"t\":[\"多个只有Self-Attention和mlp的Transformer层串联起来。\"]},\"174\":{\"h\":\"3 Decoder-only\",\"t\":[\"Decoder-only架构有两大与Encoder-only架构相区别的特征。\",\"（1）Cross-Attention：具有能接受Encoder输出的Cross-Attention作为中间层。\",\"（2）past_key_values：在进行生成任务时，可以直接在Decoder的每一个layer内的Self-Attention添加上一步key和value，进行concate然后计算Self-Attention。\",\"特征（1）发挥作用的时间在于Encoder计算完成后，Decoder计算过程中。特征（2）发挥作用的时间在于生成任务的循环中第2轮及以后Decoder的计算过程中。\"]},\"175\":{\"h\":\"3.1 GPT2\",\"t\":[\"既有特征（1）又有特征（2），但是特征（1）的使用需要用户从一开始传入Encoder层的结果，也就是只有接受Encoder输出的Cross-Attention，但自己没有产生Encoder输出的能力。当用户不提供Encoder的output时，Cross-Attention模块的计算就会被跳过。\"]},\"176\":{\"h\":\"3.2 Bloom\",\"t\":[\"只有特征（2）。\"]},\"177\":{\"h\":\"3.3 Llama\",\"t\":[\"只有特征（2）。\"]},\"178\":{\"h\":\"4 总结\",\"t\":[\"其实对Decoder-only和Encoder-only这两种，在Transformer的结构上已经近乎没有什么区别，Decoder最标志性的Cross-Attention往往不发挥作用甚至不存在。相比结构，更重要的是功能上的区别，即语义理解是双向性的还是单向性的，所做的任务是NLU还是NLG，Attention mask是对称阵还是上三角矩阵，这里才是决定一个模型所采用的架构的关键所在。\"]},\"179\":{\"c\":[\"语言模型\"]},\"180\":{\"c\":[\"Transformer\"]},\"181\":{\"h\":\"GPT2论文分享与架构分析\",\"t\":[\"GPT-2 模型由多层单向 Transformer 的解码器部分构成，本质上是自回归模型，自回归的意思是指，每次产生新单词后，将新单词加到原输入句后面，作为新的输入句。\",\"论文名称：Language Models are Unsupervised Multitask Learners\"]},\"182\":{\"h\":\"1 语言建模\",\"t\":[\"作者方法的核心是语言建模。语言建模通常被构造为来自一组示例(x1​,x2​,…,xn​)的无监督分布估计，每个示例由可变长度的符号序列(s1​,s2​,…,sn​)组成。由于语言具有自然的顺序性，因此通常将符号上的联合概率分解为条件概率的乘积。\",\"p(x)=i=1∏n​p(sn​∣s1​,…,sn−1​)(1.1)\",\"该方法允许从p(x)以及p(sn−k​,…,sn​∣s1​,…,sn−k−1​)形式的任何条件进行可追踪采样和估计。近年来，可以计算这些条件概率的模型的表达能力有了显著的提高，例如Transformer的Self-Attention架构。\",\"学习执行单个任务可以在概率框架中表示为估计一个条件概率p(output∣input)。由于一般的系统应该能够执行许多不同的任务，即使对于相同的输入，它不仅应该对输入进行调节，还应该对要执行的任务进行调节。也就是说，它应该建模为p(output∣input,task)。这在多任务和元学习环境中已被各种形式化。\"]},\"183\":{\"h\":\"2 模型架构\",\"t\":[\"该模型在很大程度上遵循OpenAI GPT模型的细节，同时有一些小的改动。LN层被移动到每个子block的输入端，类似于预激活残差网络，并且在最终的Self-Attention块之后添加了额外的LN层。使用修正的初始化，该初始化考虑了模型深度在残差路径上的累积。作者将初始化时残差层的权重按N​1​的因子进行缩放，其中N是残差层的数量。词汇表大小扩展到50257。作者还将上下文大小从512个token增加到1024个token，并使用更大的batch size 512。\",\"运行以下程序即可输出模型结构：\",\"from transformers import GPT2LMHeadModel model = GPT2LMHeadModel.from_pretrained('gpt2') print(model.modules) \",\"程序输出：\",\"<bound method Module.modules of GPT2LMHeadModel( (Transformer): GPT2Model( (wte): Embedding(50257, 768) (wpe): Embedding(1024, 768) (drop): Dropout(p=0.1, inplace=False) (h): ModuleList( (0-11): GPT2Block( (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): GPT2MLP( (c_fc): Conv1D() (c_proj): Conv1D() (act): NewGELUActivation() (dropout): Dropout(p=0.1, inplace=False) ) ) ) (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (lm_head): Linear(in_features=768, out_features=50257, bias=False) )> \"]},\"184\":{\"h\":\"3 模型架构解析\",\"t\":[\"结合GPT论文给出的模型架构，GPT2论文给出的模型架构改动，和GPT2模型的源码，总结出了如图3.1的GPT2模型结构图。\",\"图3.1 GPT2模型总架构图\"]},\"185\":{\"h\":\"3.1 LN\",\"t\":[\"对向量用以下函数进行了标准化。\",\"y=Var(x)+ϵ​x−E(x)​γ+β(3.1)\",\"其中是防止分母为0的超参数，，是可训练参数。\",\"一言以蔽之。BN是对batch的维度去做归一化，也就是针对不同样本的同一特征做操作。LN是对hidden的维度去做归一化，也就是针对单个样本的不同特征做操作。因此LN可以不受样本数的限制。\",\"下面举个例子，程序输入：\",\"import torch from torch import nn bn = nn.BatchNorm1d(5) # 实例化一个BN层 ln = nn.LayerNorm(5) # 实例化一个LN层 x = torch.Tensor([[1,2,3,4,5], [6,7,8,9,10]]) y = ln(x) z = bn(x) print(y) print(z) \",\"程序输出：\",\"tensor([[-1.4142, -0.7071, 0.0000, 0.7071, 1.4142], [-1.4142, -0.7071, 0.0000, 0.7071, 1.4142]], grad_fn=<NativeLayerNormBackward0>) tensor([[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000], [ 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<NativeBatchNormBackward0>) \"]},\"186\":{\"h\":\"3.2 Multi-head Self-Attention\",\"t\":[\"首先Self-Attention的计算式如式3.2所示。\",\"Attention(Q,K,V)=softmax(dk​​QKT​)V(3.2)\",\"图3.2 Self-Attention\",\"其中Q，K，V是三个矩阵分别与输入x做矩阵乘法的结果，本质上都是x的线性变换。是K的维度。\",\"而Multi-head Self-Attention结构如下图所示。\",\"图3.3 Multi-head Self-Attention\",\"他把Q，K，V在最后一个维度平等的拆分，然后平行地经过Self-Attention计算，再然后合并，最后经过一层线性层输出。\"]},\"187\":{\"h\":\"3.3 GPT2Attention\",\"t\":[\"首先结构如下所示。\",\"(attn): GPT2Attention( (c_attn): Conv1D() (c_proj): Conv1D() (attn_dropout): Dropout(p=0.1, inplace=False) (resid_dropout): Dropout(p=0.1, inplace=False) ) \",\"模型中的Conv1D层并非pytorch预设的卷积层torch.nn.Conv1d，而是OpenAI自定义的一个卷积层。\",\"定义如下所示。\",\"class Conv1D(nn.Module): def __init__(self, nf, nx): super().__init__() self.nf = nf w = torch.empty(nx, nf) nn.init.normal_(w, std=0.02) self.weight = nn.Parameter(w) self.bias = nn.Parameter(torch.zeros(nf)) def forward(self, x): size_out = x.size()[:-1] + (self.nf,) x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight) x = x.view(size_out) return x \",\"其中nf，nx是构造参数，weight和bias有可训练参数，总共nf*nx+nf个。\",\"对他进行了一下测试，测试程序如下所示。\",\"cv = Conv1D(18, 6) # 实例化一个Conv1D对象 x = torch.Tensor([[1, 2, 3, 4, 5, 6]]) y = cv(x) print('y:', y) \",\"程序输出如下所示。\",\"y: tensor([[ 0.0829, 0.2766, -0.0990, -0.1236, -0.0434, -0.0720, -0.0817, 0.1380, -0.2762, 0.1568, 0.1062, -0.0501, -0.2094, 0.1371, -0.3037, -0.0866, 0.2650, 0.1390]], grad_fn=<ViewBackward0>) \",\"输入1行6列的矩阵，输出了1行18列的矩阵。\",\"从代码来看，通过Attention层的第一个Conv1D，768列的矩阵会被扩增为为列的矩阵，然后马上会切分到三个768列的矩阵然后分别作为Q，K，V加入Self-Attention计算。因此，Attention层的第一个Conv1D相当于是集成了从输入x到Q，K，V的三个线性变换。\",\"在Attention层的两个Conv1D之间，进行了multi-headed Self-Attention的计算和拼接，此时拼接完之后已经变回了768列的矩阵。\",\"通过Attention层的第二个Conv1D，其源码参数nf，nx均为768，768列的矩阵向768列的矩阵进行了一个线性变换。该层执行了multi-head Self-Attention的最后的Linear层的工作。\"]},\"188\":{\"h\":\"3.4 参数量计算\",\"t\":[\"wte：50257*768=38,597,376 wpe：1024*768=786,432 每个Dropout：0 每个LN：768*2=1,536 每个NewGELUActivation：0 每个GPT2Attention中的第一个Conv1D：768*3*768+768*3=1,771,776 每个GPT2Attention中的第二个Conv1D：768*768+768=590,592 每个GPT2MLP中的第一个Conv1D：768*4*768+768*4=2,362,368 每个GPT2MLP中的第二个Conv1D：768*768*4+768=2,360,064 每个GPT2Attention：1,771,776+590,592=2,362,368 每个GPT2MLP：2,362,368+2,360,064=4,722,432 每个GPT2Block：2,362,368+4,722,432+1536*2=7,087,872 lm_head：768*50257=38,597,376 总参数量：wte+wpe+GPT2Block*12+LN+lm_head=124,439,808 \"]},\"189\":{\"c\":[\"语言模型\"]},\"190\":{\"c\":[\"GPT\"]},\"191\":{\"h\":\"PPO：从策略梯度算法到近端策略优化算法\",\"t\":[\"近端策略优化算法（Proximal Policy Optimization，PPO）是一种策略梯度优化算法，它对标准的策略梯度方法做了改进，使得训练更加稳定。PPO的主要思想是：在每个更新步骤中，我们要确保当前的策略参数不会偏离旧策略参数太远。\"]},\"192\":{\"h\":\"1 策略梯度算法\",\"t\":[\"策略梯度算法带来了原始算法和总体框架，它告诉我们只要以奖励的期望式1.1为优化目标，通过采样足够多的样本来用均值估算数学期望，再用这个估算值对分布做梯度上升求式1.1的极大值，就可以优化我们所要优化的分布θ。\",\"Rθ​=Eτ∼pθ​(τ)​R(τ)=τ∑​[R(τ)pθ​(τ)](1.1)\",\"∇Rθ​​=τ∑​[R(τ)∇pθ​(τ)]=τ∑​[R(τ)pθ​(τ)∇logpθ​(τ)]=Eτ∼pθ​(τ)​[R(τ)∇logpθ​(τ)]≈N1​i=1∑N​[R(τ)∇logpθ​(τ)]​(1.2)\",\"θ←θ+η∇Rθ​(1.3)\",\"但是策略梯度算法存在问题，每轮训练结束之后参数θ都要更新，导致下一轮计算均值前仍要重新采样大量数据，训练的时间开销集中在了数据采样。\"]},\"193\":{\"h\":\"2 重要性采样\",\"t\":[\"为了解决采样时间开销大的问题，引入了重要性采样，将式1.2换算成式2.1。这样我们可以对θ′采样一次之后，多次更新θ，大大节省了训练中采样数据的时间开销。\",\"∇Rθ​​=Eτ∼pθ′​(τ)​[pθ′​(τ)pθ​(τ)​R(τ)∇logpθ​(τ)]≈N1​i=1∑N​[pθ′​(τ)pθ​(τ)​R(τ)∇logpθ​(τ)]​(2.1)\",\"还原2.1式，得到我们的新的优化目标，如式2.2所示。\",\"Rθ​=Eτ∼pθ′​(τ)​[pθ′​(τ)pθ​(τ)​R(τ)](2.2)\"]},\"194\":{\"h\":\"3 优势函数\",\"t\":[\"式2.2的R(τ)是累积奖励，我们要优化的Rθ​函数的实际意义是奖励关于完整路径τ的数学期望，我们希望这个值正负参半，因为这样就可以衡量策略是好还是坏，而不是比较谁更好。定义A(τ)等于R(τ)减去一个与路径无关的基线函数，比如状态价值函数，是不影响等式的。最终我们的优化目标确定了，如式3.1所示。\",\"Rθ​=Eτ∼pθ′​(τ)​[pθ′​(τ)pθ​(τ)​A(τ)](3.1)\",\"总之，如果A(τ)是正的，那就用梯度调整策略θ增大τ出现的概率；反之，如果A(τ)是负的，那就用梯度调整策略θ减小τ出现的概率。\"]},\"195\":{\"h\":\"4 KL散度的外在约束\",\"t\":[\"在加入重要性采样之后，我们可以对θ′采样来计算θ的更新梯度了。在理想情况，即采样的次数足够多的情况下式1.2和式2.1是严格相等的，然而θ和θ′的分布有差异会带来估算结果差异很大的问题，因此必须有一个约束。TRPO算法引入了KL散度，并将其作为一个外在约束。KL散度可以计算两个分布的不相似度，两个完全相同时，它们的KL散度值为0，不相似度越高，KL散度也越高。TRPO算法的公式如式4.1所示。\",\"{Rθ​=Eτ∼pθ′​(τ)​[pθ′​(τ)pθ​(τ)​A(τ)]KL(θ,θ′)<δ​(4.1)\",\"但是TRPO算法也存在问题，因为它把 KL 散度约束当作一个额外的约束，没有放在目标里面，所以它处理起来非常困难。\"]},\"196\":{\"h\":\"5 KL惩罚\",\"t\":[\"我们现在既需要一个KL散度来约束θ和θ′分布的差异程度，又不能像TRPO算法那样将KL散度作为外在约束难以融入到梯度更新的操作中。因此考虑将KL散度加入到优化目标式3.1中，得到的新的优化目标如式5.1所示。\",\"Rθ​=Eτ∼pθ′​(τ)​[pθ′​(τ)pθ​(τ)​A(τ)]−βKL(θ,θ′)(5.1)\",\"我们的新优化目标和之前一样，也是越“大”，策略θ就越“好”。这个式子前半部分的数学期望，是之前3.1式给出的，用来计量策略θ′采样的好坏程度，对我们来说，这个值越大越好；而后半部分，是一个超参数β乘以θ和θ′的KL散度，用来计量θ和θ′的不相似程度，对我们来说，这个值越小越好。用梯度上升来优化这个新的优化目标，就是PPO算法。\",\"在这个基础上，还能对算法进一步改进，引入自适应KL惩罚（adaptive KL penalty），给出一个KL的可接受区间[KLmin​,KLmax​]，当KL散度小于最小值时，说明θ和θ′更新的幅度太小，即后面这一项效果太强了，应当减小β值；当KL散度大于最大值时，说明θ和θ′的差距过大，即后面这一项效果太弱了，需要增大β值。\",\"总之，KL惩罚的优势在于，新的优化目标既将原始的优化目标包含在内，又包含了一个描述θ和θ′分布的不相似度的值，减小了对θ′采样来估算θ的优化梯度的误差。\"]},\"197\":{\"h\":\"6 PPO裁剪（clip）\",\"t\":[\"近端策略优化裁剪是解决θ和θ′分布差异过大的另一种方法，它不使用KL散度来描述两种分布的不相似度，而是使用裁剪函数clip。近端策略优化裁剪的优化目标如式6.1所示。\",\"Rθ​≈N1​τ∑​min(pθ′​(τ)pθ​(τ)​A(τ),clip(pθ′​(τ)pθ​(τ)​,1−ϵ,1+ϵ)A(τ))(6.1)\",\"PPO裁剪实现的功能和KL惩罚一样，通过限定pθ′​pθ​​的范围来约束θ和θ′分布的差异程度。一般基于KL惩罚的PPO算法称为PPO1算法，基于clip的PPO算法称为PPO2算法。\"]},\"198\":{\"c\":[\"语言模型\"]},\"199\":{\"c\":[\"模型\",\"强化学习\"]},\"200\":{\"h\":\"LLM如何重映现实世界（二）：LLM中的信息回路\",\"t\":[\"本文主要介绍有关LLM中的知识回路以及一些相关的猜想，有关LLM在完成任务过程中，信息进行传播的方式。\",\"知乎原文：https://zhuanlan.zhihu.com/p/632795115 版权归属原作者，如涉侵权，请联系删除\"]},\"201\":{\"h\":\"1 GPT中知识回路存在的证据\",\"t\":[\"所谓「回路」，指的是某个任务的 Prompt 输入 Transformer 后，信息从底向上传播，直到 last token 最高层 Next Token 输出答案，在网络中存在一些完成这个任务的关键路径，信息主要沿着这条路径向上传播，在传播过程中不断进行信息传递或知识加工， 以此方式来通过 NTP 完成某项任务。\"]},\"202\":{\"h\":\"1.1 数学能力的知识回路\",\"t\":[\"提示\",\"论文：How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model[1]\",\"GPT-2 如何计算大于？：在预训练语言模型中解释数学能力\",\"图1.1 知识回路中信息传播示意图\",\"这个工作主要探讨：为何 GPT 模型能够通过预训练获得数学能力。 具体而言，用的是类似「The war lasted from the year 17YY to the year 17」的 Prompt，GPT 模型可以做到输出的 Next Token 的年份数字 XX 大于 YY，这说明它在预训练中学会了数字间的比较关系。通过探究，发现模型在预训练过程中形成了解决这个问题的知识回路，如图1.1所示。 有两个关键部分，第一个是中间层的某些 Attention Head，比如图中 a5.h5 代表 Transformer 第 5 层的第 5 个 Attention Head，这些 Attention Head 主要作用是聚焦到 YY 年份并向高层传播；另外一个关键是第 8 到 11 层的 MLP 层，这些层的 MLP 完成 「大于」运算，所以最后 GPT 能够正确输出结果。而且，中间层的 Attention Head 和上层 MLP 也有相对应的传递关系，比如第 9 层 MLP 主要接收信息来源于 a9.h1，而第 8 层 MLP 的信息来源则比较多。可以看出，信息从下到上形成了一个特定的传播路径。\",\"图1.2 知识回路数字比较示意图\",\"如果再深入探究，会发现是 MLP 中的一些关键神经元完成数学运算的，如图1.2所示，可以探测出第 10 层 MLP 中影响最大的 10 个神经元，这层只用这 10 个神经元就能大致完成 “大于” 运算，而左图则展示了 a7.h10 这个 Attention Head 主要聚焦于关键信息 “YY” 上。另外，该项研究还发现不仅仅上述 Prompt，如果变换 Prompt 形式，但是体现数字比较关系，发现被激活的也是这条回路，这说明这条回路可能专门用于对数字进行关系比较。\"]},\"203\":{\"h\":\"1.2 Induction Head回路\",\"t\":[\"图1.3 感应头回路示意图\",\"大部分知识回路应由 Attention 和 MLP 共同组成，但是也发现一些以 Attention 为主的知识回路。典型的例子就是「Induction Head」 回路，多项研究证明这个回路的存在。它的主要作用在于当 GPT 预测 Next Token 的时候，倾向于从上文找到类似的输出模式，并拷贝到后续 Token 输出。如图1.3所示句子，第二个「so」 是 last token，GPT 此时通过 NTP 将要产生后续 Token，「Induction Head」 回路倾向于从上文中找到相同的 「so」单词，并把上文中跟在「so」后面的单词 「bad」 当作 Next Token 输出。「Localizing Model Behavior with Path Patching」 这项研究探测了 Induction Head 的内在工作机制：当根据第二个单词 「so」 要预测 Next Token 的时候，「so」 本身的内容被拷贝到 Transformer 自己对应 Attention 的 < Query,Key,Value > 中的 Query，而上文内容中出现的 “bad” 单词，通过 PTH (Previous Token Head to key) 这个 Attention Head 将 “bad” 之前内容的语义集成到 “bad” 对应的 Key 里。结果在「so」做 Attention 的时候，两者就得到很高相似性，于是通过 Attention 把「bad」 拷贝到单词 so 的位置，这导致 Next Token 很容易输出 “bad”，就达成了从上文拷贝「so…bad」 的目的。\"]},\"204\":{\"h\":\"1.3 Attention 回路\",\"t\":[\"提示\",\"论文：Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small[2] 可解释性：GPT-2 small 中的间接对象识别回路\",\"图1.4 注意力回路示意图\",\"这个工作发现了 Transformer 中存在以 Attention 为主，用于识别 「Indirect Object Identification」的知识回路。所谓「Indirect Object Identification」 ，可以参考图1.4给出的例子，就是说输入有两个实体，一个重复实体，一个非重复实体，如何从中找到正确答案。从上图例子可看出 GPT 是可以输出正确答案 Mary 的，其原因就是模型学会了一个主要由 Attention Head 构成的复杂识别回路\",\"图1.5 间接对象识别示意图\",\"如图1.5所示，「Indirect Object Identification」知识回路识别正确答案，主要由三个步骤构成： 首先，Duplicate Token Heads 用于标识多次出现在句子中的 Token，而 Induction Heads 起到类似的作用； 其次，S-Inhibition Heads 在输出 Next Token 的位置发生作用，用于从 Name Mover Heads 的注意力中删除或者抑制重复出现的名字； 最后，输出剩余的名称 Token。 由上可看出，LLM 模型在预训练过程中，为了更好地进行 Next Token 预测，学习到了非常复杂的 Attention 知识回路，来执行对某些输入 Token 拷贝并在 Next Token Prediction 结果中输出。\"]},\"205\":{\"h\":\"2 回路竞争猜想\",\"t\":[\"图2.1 注意力回路示意图\",\"综合上述内容可看出，GPT 模型通过 NTP 任务从数据中学习知识，在模型内部建立起两类知识体系：层级化的知识结构以及各种任务回路，任务回路是在层级知识体系结构上建立起来的，是用于解决某个任务的、由知识点相互激发形成的固定通路。\",\"（1）知识点有不同的抽象层级。\",\"（2）某些知识点之间形成了由底向上的激发关系，激发路径是由下层不那么抽象的知识点逐层激发上层越来越抽象的知识点。\",\"我们在此基础上可以重新看待任务回路的形成。**任务回路应该是 GPT 为了更精准预测某种特殊类型数据的 Next Token，从 Transformer 的输入层开始，逐层关联相关的 “激发微结构”，从而形成了一个由低向上逐层激发，并最终关联到输出位置，**以决定输出 Token 概率的完整通路结构（可参考图2.1红线部分勾勒出的某个任务通路）。学会了这种任务回路，如果 GPT 后续再见到此类数据，则 Next Token 预测精准性增加，体现为 NTP 任务 Loss 的降低。比如如果训练数据里大量出现 「13+24=37」这种加减乘除的例子，大概率 GPT 会学会一个用于简单数学计算的任务回路，以此增加等号后数字的 Next Token 预测精准性。\"]},\"206\":{\"h\":\"3 参考\",\"t\":[\"Hanna M, Liu O, Variengien A. How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model[J]. arXiv preprint arXiv:2305.00586, 2023. ↩︎\",\"Wang K, Variengien A, Conmy A, et al. Interpretability in the wild: a circuit for indirect object identification in gpt-2 small[J]. arXiv preprint arXiv:2211.00593, 2022. ↩︎\"]},\"207\":{\"c\":[\"语言模型\",\"知识回路\"]},\"208\":{\"h\":\"Chain-of-Thought: 思维链\",\"t\":[\"该文介绍了 Chain-of-Thought: 思维链 框架，结合 in-context, few-shot prompting 以及多步中间推理，通过大模型来改善数学计算、常识推理的效果。\",\"提示\",\"论文题目：Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\\n作者：Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, Denny Zhou\\n机构：Google\"]},\"209\":{\"c\":[\"提示技术\"]},\"210\":{\"c\":[\"推理\",\"LLM\",\"CoT\"]},\"211\":{\"h\":\"提示技术\"},\"212\":{\"c\":[\"提示技术\"]},\"213\":{\"c\":[\"Prompt\"]},\"214\":{\"c\":[\"提示技术\"]},\"215\":{\"h\":\"RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text\",\"t\":[\"来自苏黎世联邦理工和波形智能的团队发布了 RecurrentGPT，一种让大语言模型 (如 ChatGPT 等) 能够模拟 RNN/LSTM，通过 Recurrent Prompting 来实现交互式超长文本生成，让利用 ChatGPT 进行长篇小说创作成为了可能。\"]},\"216\":{\"h\":\"1 问题提出\",\"t\":[\"基于变换器（Transformer）的大语言模型最明显的限制之一就是输入和输出的长度限制。虽然输入端的长度限制可以通过向量数据库（Vector Database ，VDB）等方式缓解，输出内容的长度限制始终是限制 ChatGPT 等大语言模型广泛应用于长内容生成的关键障碍。为解决这一问题，过去很多研究试图使用基于向量化的状态（State）或记忆（Memory）来让 Transformer 可以进行循环计算。这样的方法虽然在长文本建模上展现了一定的优势，但是却要求使用者拥有并可以修改模型的结构和参数，这在目前闭源模型遥遥领先的大语言模型时代中是不符合实际的。\",\"该文旨在解决GPT模型生成文本长度受限的问题，并且探索以自然语言模拟循环机制的可能性。这是一个新问题，因为当前的GPT模型只能生成有限长度的文本，而缺乏长文本生成的能力。\"]},\"217\":{\"h\":\"2 RecurrentGPT原理\",\"t\":[\"该文提出了一种名为循环生成式预训练变换器（Recurrent Generative Pre-trained Transformer，RecurrentGPT）的模型，使用自然语言模拟长短期记忆（Long Short-Term Memory，LSTM）神经网络中的长短期记忆机制，从而实现生成任意长度的文本。该模型每个时间步生成一个段落，并且将其存储在硬盘和提示中，以模拟记忆的更新。由于人类用户可以轻松观察和编辑自然语言记忆，因此RecurrentGPT是可解释的，并且可以进行交互式生成长文本。相比于当前领域的研究，本文的思路在于使用自然语言模拟循环机制，从而实现生成任意长度的文本，并且是可解释的。\",\"RecurrentGPT的语言模型是在大型语言模型（Large Language Model，LLM）如对话生成式预训练变换器（Chat Generative Pre-trained Transformer，ChatGPT）的基础上构建的，并使用自然语言来模拟LSTM中的长短期记忆机制。在每个时间步骤，RecurrentGPT生成一个段落的文本，并分别更新存储在硬盘和提示中的基于语言的长短期记忆。这种循环机制使得RecurrentGPT能够生成任意长度的文本而不会遗忘。由于人类用户可以轻松观察和编辑自然语言记忆，因此RecurrentGPT是可解释的，并且可以实现长文本的交互式生成。\",\"RecurrentGPT通过自然语言模拟了循环神经网络（Recurrent Neural Network，RNN）的循环计算机制。。在每一个时间步中，RecurrentGPT 会接收上一个时间步生成的内容、最近生成内容的摘要（短期记忆），历史生成内容中和当前时间步最相关的内容 (长期记忆)，以及一个对下一步生成内容的梗概。RecurrentGPT 根据这些内容生成一段内容，更新其长短时记忆，并最后生成几个对下一个时间步中生成内容的规划，并将当前时间步的输出作为下一个时间步的输入。这样的循环计算机制打破了常规Transformer 模型在生成长篇文本方面的限制，从而实现任意长度文本的生成，而不遗忘过去的信息。\",\"图2.1 RecurrentGPT架构图\",\"图2.2 RecurrentGPT Prompt 设计\",\"首先指明任务，比如写小说，并说明在输入部分会给出的内容：上一步生成的段落、当前维持的近期生成内容的摘要，即短期记忆，所有生成内容中和当前时间步相关程度最高的几个段落，即短期记忆，以及对接下来生成内容的规划。\",\"接着在提示（Prompt）中给 ChatGPT 提出要求：首先基于当前的输入生成一个新的段落，接着对维护的短期记忆进行修改，同时在对短期记忆修改时作者们指示大语言模型首先分析短期记忆中哪些内容对于后续创作不再重要以及新生成的内容中哪些会对后续生成有所影响，之后相应地在地短期记忆库中去去除无用的信息并增添新的信息，从而保持短期记忆不会因为迭代的轮数增加而变得过长。最后要求 ChatGPT 基于当前的情节铺设，给出三个逻辑顺承又有趣的新的情节的规划。\",\"在提出要求后，作者在结尾再次精心设计了 Prompt 来规范 ChatGPT 的输出，并重申了当前小说写作的情景。这个好处是让 ChatGPT 生成的内容更具备像小说那样的细节，而不是在每一轮的迭代中，快速地完成情节的叙述。\",\"在实际使用中，内容创作者只需先选择一个主题，然后简单地描述一下要生成的内容的背景设定和大纲，剩下的工作就可以交给 RecurrentGPT。每一个它将自动生成第一段，并提供几个可能的选项供创作者继续写故事。创作者可以选择一个选项、对某个选项进行修改或者自己编辑一个新的选项。这个流程能显著提高内容创作者的效率。\",\"这个新的长文本生成范式将带给所有内容创作者和读者一种全新的体验。首先，相比现有的方法，RecurrentGPT 有更强的可解释性，因为用户可以观察和编辑自然语言记忆，这使得用户可以更清晰地理解这个框架是如何工作的。其次，用户可以直接影响生成内容的方向，让整个写作过程变得更加有趣。\"]},\"218\":{\"h\":\"3 在线演示\",\"t\":[\"除了生成AI生成内容（AIGC）外，我们还展示了使用RecurrentGPT作为与消费者直接交互的交互式小说的可能性。我们称这种生成模型的用法为\\\"AI作为内容\\\"（AIAC），这是传统AIGC的下一形式。此外，我们还展示了使用RecurrentGPT创建个性化交互式小说的可能性，这些小说直接与读者交互而不是与作家交互。总的来说，RecurrentGPT展示了从认知科学和深度学习中流行的模型设计中借鉴思想对LLMs进行提示的效用。他们的代码可以在该网站上找到，同时还提供了在线演示。\",\"图3.1 在线演示界面\"]},\"219\":{\"h\":\"4 相关研究\",\"t\":[\"近期的相关研究包括《Long Text Generation via Adversarial Training with Leaked Information》（Jingjing Xu等，南京大学）、《Towards Controlled Generation of Text》（Sumanth Dathathri等，斯坦福大学）、《GPT-2: Language Models are Unsupervised Multitask Learners》（Alec Radford等，OpenAI）等。\"]},\"220\":{\"c\":[\"提示技术\"]},\"221\":{\"c\":[\"Memory\",\"LLM\",\"ChatGPT\"]},\"222\":{\"h\":\"Token\"},\"223\":{\"c\":[\"token\"]},\"224\":{\"c\":[\"token\"]},\"225\":{\"c\":[\"Token\"]},\"226\":{\"h\":\"BPE分词器\",\"t\":[\"字节对编码（BPE, Byte Pair Encoder），又称 digram coding 双字母组合编码，是一种数据压缩算法，用来在固定大小的词表中实现可变⻓度的子词。该算法简单有效，因而目前它是最流行的方法。\",\"BPE 首先将词分成单个字符，然后依次用另一个字符替换频率最高的一对字符 ，直到循环次数结束。\"]},\"227\":{\"h\":\"1 分词算法\",\"t\":[\"（1）准备语料库，确定期望的 merge 词表大小等参数。\",\"（2）统计每个单词出现的频率。\",\"（3）将语料库中所有单词拆分为单个字符，用所有单个字符建立最初的词典，并统计每个字符的频率。挑出频次最高的符号对 ，将新字符加入词表，然后将语料中所有该字符对融合（merge）。\",\"注：新字符依然可以参与后续的 merge，有点类似哈夫曼树，BPE 实际上就是一种贪心算法 。\",\"图1.1 字节对算法流程\",\"（4）重复遍历 2 和 3 操作，直到词表中单词数达到设定量或下一个最高频数为 1 ，如果已经达到设定量，其余的词汇直接丢弃。\"]},\"228\":{\"h\":\"2 一个示例\",\"t\":[\"比如我们想编码：\",\"aaabdaaabac\",\"我们会发现这里的aa出现的词频最高（我们这里只看两个字符的频率），那么用这里没有的字符Z来替代aa：\",\"ZabdZabac\",\"Z=aa\",\"此时，又发现ab出现的频率最高，那么同样的，Y来代替ab：\",\"ZYdZYac\",\"Y=ab\",\"Z=aa\",\"同样的，ZY出现的频率大，我们用X来替代ZY：\",\"XdXac\",\"X=ZY\",\"Y=ab\",\"Z=aa\",\"最后，连续两个字符的频率都为1了，算法也就结束了。\"]},\"229\":{\"h\":\"3 GPT2tokenizer\",\"t\":[\"GPT2tokenizer同时也是gpt3的tokenizer，它的词汇表由256个单字节符号+50000个merge词+1个<|endoftext|>组成。\",\"我们需要知道，词汇表是一个键为字节串值为token_id的字典，编码的过程和构造merge词表的过程相差无几，唯一的区别是结束的条件不同，而解码的过程则就是编码的反向过程。\",\"尽管词汇表里面已经包含所有的merge词，但是GPT2tokenizer还是需要一个merges.txt来记录所有对merge词对，从下面算法流程就能明白原因了。\"]},\"230\":{\"h\":\"3.1 训练\",\"t\":[\"训练的步骤与前面所提到的BPE原始步骤基本一致，除了一个在GPT2论文中提到的一个额外限制。由于dog有很多变体“dog.”、“dog!”出现的频率非常高，但是它对语言建模而言是次优的，因此官方制定了一条限制——不能跨符号类别进行merge操作。在加入这个限制的BPE算法下GPT2tokenizer诞生了。\"]},\"231\":{\"h\":\"3.2 编码\",\"t\":[\"（1）把所有字符通过utf-8规则转换成字节串。\",\"（2）扫描所有2-gram，检索merges.txt，选择优先级最高的词对（在merges.txt中位置越靠前优先级越高），进行merge操作。\",\"（3）循环第2步，直到某一轮扫描，所有2-gram都不是merge词对为止。\",\"（4）对这个经过merge操作的新串，使用词汇表映射到token_id。\"]},\"232\":{\"h\":\"3.3 解码\",\"t\":[\"（1）对所有token_id列表，使用键值互换的反向词汇表映射到一个字节串列表。\",\"（2）合并这个字节串列表为一个字节串。\",\"（3）使用utf-8规则将字节串解码为人类可以理解的自然语言字符串。\",\"下面举例说明一下，解码的步骤。\",\"首先下面是utf-8从字节解码到字符的规则。\",\"（1）0xxxxxxx(0-7) 单独成字符\",\"（2）10xxxxxx(8-B) 作为后缀字节\",\"（3）110xxxxx(C-D) 有一个后缀字节\",\"（4）1110xxxx(E) 有两个后缀字节\",\"（5）1111xxxx(F) 有三个后缀字节\",\"下面演示了从输入token序列[4399, 2572, 3461]到字符串的完整过程。\",\"（1）[4399, 2572, 3461]\",\"（2）[[2325, 168], [201, 234], [102, 129]]\",\"（3）[[[101, 104], 168], [201, 234], [102, 129]]\",\"（4）[101, 104, 168, 201, 234, 102, 129]\",\"（5）\\\\xc2\\\\xa1\\\\x65\\\\xe6\\\\x93\\\\x84\\\\x42\",\"（6）[\\\\xc2\\\\xa1, \\\\x65, \\\\xe6\\\\x93\\\\x84, \\\\x42]\",\"（7）你a他4\",\"大概过程就是token返回到字节，再根据字节高四位来唯一编码，比如\\\\xc2高四位是c，那后面就有一位字节和他一起编码到字符。\"]},\"233\":{\"h\":\"3.4 总结\",\"t\":[\"词汇表中有大量的英文单词，但也有很多光看词汇表看不出来是哪国语言的奇异符号，其实把它们通过utf-8规则解码到字符串我们才能发现，词汇表是包括了一些汉字，日文假名和其他国的一些高频词汇的。至于不在词汇表的字词，只能通过词汇表上的字节或字节串来“碎片”地表示了，这也就是BPE分词器解决OOV问题的一种思路。至于为什么英文单词那么多，因为BPE算法训练tokenizer的语料库以英文语料库为主。\",\"值得注意的是，词汇表中“cat”前有没有空格是不算作同一个token的。其中有空格代表一个英文单词或者是一个英文单词前缀，而没有空格则代表了cat作为英文单词的中间片段或者后缀。\"]},\"234\":{\"c\":[\"Token\"]},\"235\":{\"c\":[\"分词器\",\"强化学习\"]}},\"dirtCount\":0,\"index\":[[\"值得注意的是\",{\"1\":{\"233\":1}}],[\"值得注意的还有三个改动\",{\"1\":{\"43\":1}}],[\"至于为什么英文单词那么多\",{\"1\":{\"233\":1}}],[\"至于不在词汇表的字词\",{\"1\":{\"233\":1}}],[\"地表示了\",{\"1\":{\"233\":1}}],[\"碎片\",{\"1\":{\"233\":1}}],[\"日文假名和其他国的一些高频词汇的\",{\"1\":{\"233\":1}}],[\"你a他4\",{\"1\":{\"232\":1}}],[\"合并这个字节串列表为一个字节串\",{\"1\":{\"232\":1}}],[\"合理分析\",{\"1\":{\"163\":1}}],[\"循环第2步\",{\"1\":{\"231\":1}}],[\"检索merges\",{\"1\":{\"231\":1}}],[\"检索增强的交叉注意力机制\",{\"0\":{\"140\":1}}],[\"扫描所有2\",{\"1\":{\"231\":1}}],[\"扫描确定稠密模型的最佳超参数\",{\"1\":{\"119\":1}}],[\"出现的频率非常高\",{\"1\":{\"230\":1}}],[\"唯一的区别是结束的条件不同\",{\"1\":{\"229\":1}}],[\"编码\",{\"0\":{\"231\":1}}],[\"编码的过程和构造merge词表的过程相差无几\",{\"1\":{\"229\":1}}],[\"编码器通常截断输入\",{\"1\":{\"140\":1}}],[\"编码器模型之上\",{\"1\":{\"137\":1}}],[\"连续两个字符的频率都为1了\",{\"1\":{\"228\":1}}],[\"连续prompt直接在底层语言模型的嵌入空间中进行描述\",{\"1\":{\"42\":1}}],[\"挑出频次最高的符号对\",{\"1\":{\"227\":1}}],[\"统计每个单词出现的频率\",{\"1\":{\"227\":1}}],[\"词表大小等参数\",{\"1\":{\"227\":1}}],[\"词汇表中\",{\"1\":{\"233\":1}}],[\"词汇表中有大量的英文单词\",{\"1\":{\"233\":1}}],[\"词汇表是包括了一些汉字\",{\"1\":{\"233\":1}}],[\"词汇表是一个键为字节串值为token\",{\"1\":{\"229\":1}}],[\"词汇表大小扩展到50257\",{\"1\":{\"183\":1}}],[\"词汇表大小\",{\"0\":{\"149\":1}}],[\"词汇\",{\"1\":{\"82\":1}}],[\"确定期望的\",{\"1\":{\"227\":1}}],[\"确定性策略π\",{\"1\":{\"96\":1}}],[\"准备语料库\",{\"1\":{\"227\":1}}],[\"准确率都低于随机结果\",{\"1\":{\"19\":1}}],[\"双字母组合编码\",{\"1\":{\"226\":1}}],[\"南京大学\",{\"1\":{\"219\":1}}],[\"外\",{\"1\":{\"218\":1}}],[\"外矩阵乘的额外计算\",{\"1\":{\"63\":1}}],[\"让整个写作过程变得更加有趣\",{\"1\":{\"217\":1}}],[\"让利用\",{\"1\":{\"215\":1}}],[\"创作者可以选择一个选项\",{\"1\":{\"217\":1}}],[\"剩下的工作就可以交给\",{\"1\":{\"217\":1}}],[\"快速地完成情节的叙述\",{\"1\":{\"217\":1}}],[\"接着对维护的短期记忆进行修改\",{\"1\":{\"217\":1}}],[\"接着在提示\",{\"1\":{\"217\":1}}],[\"接口使用\",{\"1\":{\"64\":1}}],[\"设计\",{\"1\":{\"217\":1}}],[\"短期记忆\",{\"1\":{\"217\":1}}],[\"神经网络中的长短期记忆机制\",{\"1\":{\"217\":1}}],[\"过去很多研究试图使用基于向量化的状态\",{\"1\":{\"216\":1}}],[\"过去一般认为\",{\"1\":{\"82\":1}}],[\"常识推理的效果\",{\"1\":{\"208\":1}}],[\"常见的方法有policy\",{\"1\":{\"100\":1}}],[\"思维链\",{\"0\":{\"208\":1},\"1\":{\"208\":1}}],[\"↩︎\",{\"1\":{\"206\":2}}],[\"体现为\",{\"1\":{\"205\":1}}],[\"学会了这种任务回路\",{\"1\":{\"205\":1}}],[\"学习到了非常复杂的\",{\"1\":{\"204\":1}}],[\"学习执行单个任务可以在概率框架中表示为估计一个条件概率p\",{\"1\":{\"182\":1}}],[\"学习率可用\",{\"1\":{\"103\":1}}],[\"概率的完整通路结构\",{\"1\":{\"205\":1}}],[\"概念的含义是\",{\"1\":{\"83\":1}}],[\"逐层关联相关的\",{\"1\":{\"205\":1}}],[\"逐步集成到这个位置上来\",{\"1\":{\"82\":1}}],[\"激发微结构\",{\"1\":{\"205\":1}}],[\"激发路径是由下层不那么抽象的知识点逐层激发上层越来越抽象的知识点\",{\"1\":{\"205\":1}}],[\"激活函数\",{\"0\":{\"154\":1}}],[\"某些知识点之间形成了由底向上的激发关系\",{\"1\":{\"205\":1}}],[\"综合上述内容可看出\",{\"1\":{\"205\":1}}],[\"拷贝并在\",{\"1\":{\"204\":1}}],[\"拷贝到单词\",{\"1\":{\"203\":1}}],[\"起到类似的作用\",{\"1\":{\"204\":1}}],[\"起到了扩充llm模型高质量训练数据的作用\",{\"1\":{\"163\":1}}],[\"间接对象识别示意图\",{\"1\":{\"204\":1}}],[\"构成的复杂识别回路\",{\"1\":{\"204\":1}}],[\"构建了\",{\"1\":{\"8\":1}}],[\"很容易输出\",{\"1\":{\"203\":1}}],[\"很多不同语言含义的知识点都会激活某个神经元\",{\"1\":{\"83\":1}}],[\"于是通过\",{\"1\":{\"203\":1}}],[\"倾向于从上文找到类似的输出模式\",{\"1\":{\"203\":1}}],[\"典型的例子就是\",{\"1\":{\"203\":1}}],[\"感应头回路示意图\",{\"1\":{\"203\":1}}],[\"运算\",{\"1\":{\"202\":2}}],[\"运行以下程序即可输出模型结构\",{\"1\":{\"183\":1}}],[\"代表\",{\"1\":{\"202\":1}}],[\"代码目前还没放出来\",{\"1\":{\"86\":1}}],[\"代码实现易于拓展\",{\"1\":{\"66\":1}}],[\"代码地址\",{\"1\":{\"37\":1,\"61\":1}}],[\"完成\",{\"1\":{\"202\":1}}],[\"完成某项任务\",{\"1\":{\"201\":1}}],[\"完美一一对应\",{\"1\":{\"83\":1}}],[\"后续再见到此类数据\",{\"1\":{\"205\":1}}],[\"后面的单词\",{\"1\":{\"203\":1}}],[\"后\",{\"1\":{\"201\":1}}],[\"后文为了书写简洁\",{\"1\":{\"79\":1}}],[\"指的是某个任务的\",{\"1\":{\"201\":1}}],[\"指令微调\",{\"0\":{\"162\":1},\"1\":{\"6\":1,\"8\":6,\"162\":1}}],[\"回路竞争猜想\",{\"0\":{\"205\":1}}],[\"回路倾向于从上文中找到相同的\",{\"1\":{\"203\":1}}],[\"回路\",{\"0\":{\"204\":1},\"1\":{\"201\":1,\"203\":1}}],[\"二\",{\"0\":{\"200\":1}}],[\"给出三个逻辑顺承又有趣的新的情节的规划\",{\"1\":{\"217\":1}}],[\"给出一个kl的可接受区间\",{\"1\":{\"196\":1}}],[\"给定一个长的输入序列\",{\"1\":{\"137\":1}}],[\"给定一个无监督的token语料库u=\",{\"1\":{\"73\":1}}],[\"给定智能体或演员的策略参数θ\",{\"1\":{\"103\":1}}],[\"好\",{\"1\":{\"196\":1}}],[\"好于经过二十万指令微调的\",{\"1\":{\"19\":1}}],[\"策略θ就越\",{\"1\":{\"196\":1}}],[\"策略梯度的实现流程\",{\"1\":{\"103\":1}}],[\"策略梯度算法带来了原始算法和总体框架\",{\"1\":{\"192\":1}}],[\"策略梯度算法\",{\"0\":{\"101\":1,\"192\":1}}],[\"−βkl\",{\"1\":{\"196\":1}}],[\"没有放在目标里面\",{\"1\":{\"195\":1}}],[\"没有做re\",{\"1\":{\"153\":1}}],[\"散度约束当作一个额外的约束\",{\"1\":{\"195\":1}}],[\"那后面就有一位字节和他一起编码到字符\",{\"1\":{\"232\":1}}],[\"那就用梯度调整策略θ减小τ出现的概率\",{\"1\":{\"194\":1}}],[\"那就用梯度调整策略θ增大τ出现的概率\",{\"1\":{\"194\":1}}],[\"那么同样的\",{\"1\":{\"228\":1}}],[\"那么用这里没有的字符z来替代aa\",{\"1\":{\"228\":1}}],[\"那么实际上更加可能受到多epoch带来的性能损失\",{\"1\":{\"128\":1}}],[\"那么在做完归一化后\",{\"1\":{\"104\":1}}],[\"那么智能体便能在执行动作前得知状态转移的情况即p\",{\"1\":{\"96\":1}}],[\"减小了对θ\",{\"1\":{\"196\":1}}],[\"减去一个与路径无关的基线函数\",{\"1\":{\"194\":1}}],[\"减少到\",{\"1\":{\"65\":1}}],[\"定义a\",{\"1\":{\"194\":1}}],[\"定义如下所示\",{\"1\":{\"187\":1}}],[\"式2\",{\"1\":{\"194\":1}}],[\"导致下一轮计算均值前仍要重新采样大量数据\",{\"1\":{\"192\":1}}],[\"导致每个标记分别重复\",{\"1\":{\"123\":1}}],[\"≈n1​i=1∑n​\",{\"1\":{\"192\":1,\"193\":1}}],[\"∇rθ​​=eτ∼pθ\",{\"1\":{\"193\":1}}],[\"∇rθ​​=τ∑​\",{\"1\":{\"192\":1}}],[\"∇logpθ​\",{\"1\":{\"192\":3,\"193\":2}}],[\"∇pθ​\",{\"1\":{\"192\":1}}],[\"近期的相关研究包括\",{\"1\":{\"219\":1}}],[\"近端策略优化裁剪的优化目标如式6\",{\"1\":{\"197\":1}}],[\"近端策略优化裁剪是解决θ和θ\",{\"1\":{\"197\":1}}],[\"近端策略优化算法\",{\"1\":{\"191\":1}}],[\"近年来\",{\"1\":{\"182\":1}}],[\"他们的代码可以在该网站上找到\",{\"1\":{\"218\":1}}],[\"他们在微调期间利用面向任务的输入转换来实现有效的转移\",{\"1\":{\"70\":1}}],[\"他把q\",{\"1\":{\"186\":1}}],[\"程序输入\",{\"1\":{\"185\":1}}],[\"程序输出如下所示\",{\"1\":{\"187\":1}}],[\"程序输出\",{\"1\":{\"183\":1,\"185\":1}}],[\"下面演示了从输入token序列\",{\"1\":{\"232\":1}}],[\"下面举例说明一下\",{\"1\":{\"232\":1}}],[\"下面举个例子\",{\"1\":{\"185\":1}}],[\"下面是gpt\",{\"1\":{\"81\":1}}],[\"形式\",{\"1\":{\"202\":1}}],[\"形式的任何条件进行可追踪采样和估计\",{\"1\":{\"182\":1}}],[\"形成运动轨迹τ\",{\"1\":{\"102\":1}}],[\"既有特征\",{\"1\":{\"175\":1}}],[\"符合类gpt模型的autoregressivelm的特性\",{\"1\":{\"172\":1}}],[\"符合类bert模型的masklm的特性\",{\"1\":{\"172\":1}}],[\"仅左边token会影响模型对中间token的预测\",{\"1\":{\"172\":1}}],[\"左右的token都会影响模型对中间token的预测\",{\"1\":{\"172\":1}}],[\"左图中flashattention使用切片技术\",{\"1\":{\"155\":1}}],[\"左图为单任务全参数微调\",{\"1\":{\"44\":1}}],[\"标准transformer架构\",{\"1\":{\"170\":1}}],[\"哈佛的nlp团队也实现了一个基于pytorch的版本\",{\"1\":{\"169\":1}}],[\"现在是谷歌云tpu推荐的参考模型\",{\"1\":{\"169\":1}}],[\"现有的数据集中的token数量有限\",{\"1\":{\"118\":1}}],[\"现有的一些深度学习框架\",{\"1\":{\"61\":1}}],[\"推测是因为初始ppo策略训练的模型太过随心所欲\",{\"1\":{\"163\":1}}],[\"推理打分\",{\"1\":{\"163\":1}}],[\"推理速度相比初代提升了\",{\"1\":{\"146\":1}}],[\"推理\",{\"2\":{\"88\":1,\"210\":1}}],[\"推理阶段应该比原来的计算量增大一点\",{\"1\":{\"40\":1}}],[\"据推测\",{\"1\":{\"163\":1}}],[\"新字符依然可以参与后续的\",{\"1\":{\"227\":1}}],[\"新的优化目标既将原始的优化目标包含在内\",{\"1\":{\"196\":1}}],[\"新的数据集被用来训练rm\",{\"1\":{\"163\":1}}],[\"新加坡国立大学的研究人员发布了一篇全新的论文\",{\"1\":{\"118\":1}}],[\"排序是这些数据的label\",{\"1\":{\"163\":1}}],[\"人工对这些答案从到坏进行排序\",{\"1\":{\"163\":1}}],[\"人文科学\",{\"1\":{\"27\":1}}],[\"去除最后一次反嵌入层\",{\"1\":{\"163\":1}}],[\"发挥作用的时间在于生成任务的循环中第2轮及以后decoder的计算过程中\",{\"1\":{\"174\":1}}],[\"发挥作用的时间在于encoder计算完成后\",{\"1\":{\"174\":1}}],[\"发布了指令微调\",{\"1\":{\"162\":1}}],[\"发现被激活的也是这条回路\",{\"1\":{\"202\":1}}],[\"发现模型在预训练过程中形成了解决这个问题的知识回路\",{\"1\":{\"202\":1}}],[\"发现二者都会因为重复训练带来模型性能的下降\",{\"1\":{\"125\":1}}],[\"发现\",{\"1\":{\"82\":1}}],[\"月\",{\"1\":{\"162\":1}}],[\"年份并向高层传播\",{\"1\":{\"202\":1}}],[\"年\",{\"1\":{\"162\":1}}],[\"年末开发了promptsource项目\",{\"1\":{\"8\":1}}],[\"称其为codex系列\",{\"1\":{\"161\":1}}],[\"称之为前缀\",{\"1\":{\"43\":1}}],[\"首先下面是utf\",{\"1\":{\"232\":1}}],[\"首先将词分成单个字符\",{\"1\":{\"226\":1}}],[\"首先基于当前的输入生成一个新的段落\",{\"1\":{\"217\":1}}],[\"首先指明任务\",{\"1\":{\"217\":1}}],[\"首先\",{\"1\":{\"204\":1,\"217\":1}}],[\"首先结构如下所示\",{\"1\":{\"187\":1}}],[\"首先self\",{\"1\":{\"186\":1}}],[\"首先回顾了gpt系列模型的发展历程\",{\"1\":{\"160\":1}}],[\"首先是模型参数规模的增长与模型需要的token数量基本是呈线性的\",{\"1\":{\"122\":1}}],[\"拆分成q\",{\"1\":{\"156\":1}}],[\"拆解为不同数量的待计算块\",{\"1\":{\"65\":1}}],[\"隐藏层输入\",{\"1\":{\"156\":1}}],[\"隐式\",{\"1\":{\"43\":1}}],[\"蓝色箭头\",{\"1\":{\"155\":1}}],[\"红色箭头\",{\"1\":{\"155\":1}}],[\"红色前缀块\",{\"1\":{\"43\":1}}],[\"存储到hbm中\",{\"1\":{\"155\":1}}],[\"虚线框内\",{\"1\":{\"155\":1}}],[\"×\",{\"1\":{\"155\":1}}],[\"防止将大型n\",{\"1\":{\"155\":1}}],[\"切片\",{\"1\":{\"155\":1}}],[\"带宽估计约为19tb\",{\"1\":{\"155\":1}}],[\"带宽为1\",{\"1\":{\"155\":1}}],[\"带参数λ\",{\"1\":{\"74\":1}}],[\"移除了其中的均值项\",{\"1\":{\"153\":1}}],[\"归一化层\",{\"0\":{\"153\":1}}],[\"亦允许商业使用\",{\"1\":{\"146\":1}}],[\"允许更多轮次的对话\",{\"1\":{\"146\":1}}],[\"官方欢迎您对下一代模型chatglm3研发的捐赠\",{\"1\":{\"146\":1}}],[\"官方会在后续迭代升级中着重进行优化\",{\"1\":{\"146\":1}}],[\"官方将基座模型的上下文长度\",{\"1\":{\"146\":1}}],[\"官方全面升级了\",{\"1\":{\"146\":1}}],[\"摘要\",{\"2\":{\"145\":1}}],[\"摘要数据集中的结果\",{\"1\":{\"142\":2}}],[\"应当减小β值\",{\"1\":{\"196\":1}}],[\"应用unlimiformer\",{\"1\":{\"143\":1}}],[\"应该可以成功解决任何看不见的任务\",{\"1\":{\"7\":1}}],[\"书籍摘要的试验结果\",{\"1\":{\"143\":1}}],[\"书籍摘要\",{\"0\":{\"143\":1}}],[\"及\",{\"1\":{\"142\":2}}],[\"详情如下\",{\"1\":{\"140\":1}}],[\"搜索是非参数的\",{\"1\":{\"140\":1}}],[\"搜索\",{\"1\":{\"138\":1}}],[\"查找注入解码器来实现\",{\"1\":{\"138\":1}}],[\"且无需添加权重和重新训练\",{\"1\":{\"137\":1}}],[\"性能会得到进一步提高\",{\"1\":{\"137\":1}}],[\"性能比gpt\",{\"1\":{\"86\":1}}],[\"解码的步骤\",{\"1\":{\"232\":1}}],[\"解码\",{\"0\":{\"232\":1}}],[\"解码时查询编码的隐状态数据存储\",{\"1\":{\"140\":1}}],[\"解码器中之前的词元的键\",{\"1\":{\"156\":1}}],[\"解码器的标准交叉注意力机制能够查询数据存储\",{\"1\":{\"137\":1}}],[\"解码器\",{\"1\":{\"137\":1}}],[\"解决问题\",{\"1\":{\"8\":1}}],[\"增加上下文窗口需要用新的上下文窗口大小从头开始重新训练模型\",{\"1\":{\"137\":1}}],[\"增大模型训练的改变量\",{\"1\":{\"45\":1}}],[\"增大改变量和交互性\",{\"1\":{\"45\":1}}],[\"虽然输入端的长度限制可以通过向量数据库\",{\"1\":{\"216\":1}}],[\"虽然比标准\",{\"1\":{\"137\":1}}],[\"虽然慢\",{\"0\":{\"129\":1}}],[\"普通变换网络\",{\"1\":{\"137\":1}}],[\"普通的linear层\",{\"1\":{\"52\":1}}],[\"倍以上\",{\"1\":{\"137\":1}}],[\"维基百科文章生成的挑战集\",{\"1\":{\"137\":1}}],[\"维神经元编码比\",{\"1\":{\"83\":1}}],[\"万个\",{\"1\":{\"137\":2}}],[\"´cinski\",{\"1\":{\"137\":1}}],[\"涉及长篇叙事的任务\",{\"1\":{\"137\":1}}],[\"涉及人文\",{\"1\":{\"16\":1}}],[\"或记忆\",{\"1\":{\"216\":1}}],[\"或叙事问答\",{\"1\":{\"137\":1}}],[\"或\",{\"1\":{\"137\":3}}],[\"或者说可能很快我们也会达到现有llm的天花板\",{\"1\":{\"133\":1}}],[\"架构的更改\",{\"1\":{\"140\":1}}],[\"架构\",{\"1\":{\"137\":1}}],[\"降低模型的效果\",{\"1\":{\"133\":1}}],[\"总的来说\",{\"1\":{\"218\":1}}],[\"总之\",{\"1\":{\"194\":1,\"196\":1}}],[\"总共nf\",{\"1\":{\"187\":1}}],[\"总参数量\",{\"1\":{\"152\":2,\"188\":1}}],[\"总体分为两大类\",{\"1\":{\"161\":1}}],[\"总体架构\",{\"0\":{\"151\":1}}],[\"总体上显存的压力是大大变小了\",{\"1\":{\"52\":1}}],[\"总结出了如图3\",{\"1\":{\"184\":1}}],[\"总结\",{\"0\":{\"133\":1,\"178\":1,\"233\":1}}],[\"做\",{\"1\":{\"203\":1}}],[\"做参数调优是一个非常好的思路\",{\"1\":{\"132\":1}}],[\"做打破彼此任务之间的边界的第一次简单尝试\",{\"1\":{\"8\":1}}],[\"权重衰减\",{\"1\":{\"129\":1}}],[\"权重小的\",{\"1\":{\"104\":1}}],[\"路径随机失活\",{\"1\":{\"129\":1}}],[\"小计算量模型的过拟合趋势与大计算量的差不多\",{\"0\":{\"127\":1}}],[\"小学和初中的知识或考点存在明显的差异\",{\"1\":{\"17\":1}}],[\"采样来估算θ的优化梯度的误差\",{\"1\":{\"196\":1}}],[\"采样来计算θ的更新梯度了\",{\"1\":{\"195\":1}}],[\"采样的好坏程度\",{\"1\":{\"196\":1}}],[\"采样一次之后\",{\"1\":{\"193\":1}}],[\"采样到在某一个状态st​要执行某一个动作at​\",{\"1\":{\"103\":1}}],[\"采用了不同的数据集\",{\"1\":{\"163\":1}}],[\"采用类似的模型\",{\"1\":{\"127\":1}}],[\"采用不同的模型架构所需要的浮点运算次数\",{\"1\":{\"126\":1}}],[\"说明θ和θ\",{\"1\":{\"196\":2}}],[\"说明相对提高数据集质量可能不会影响重复训练的负面效应\",{\"1\":{\"125\":1}}],[\"说明这个模型学到了更多的内在规律\",{\"1\":{\"81\":1}}],[\"展示了预期的性能\",{\"1\":{\"123\":1}}],[\"次\",{\"1\":{\"123\":1}}],[\"尽管词汇表里面已经包含所有的merge词\",{\"1\":{\"229\":1}}],[\"尽管前面已经证明dropout使用可以降低多epoch的影响\",{\"1\":{\"131\":1}}],[\"尽管在前面的实验中\",{\"1\":{\"127\":1}}],[\"尽管消耗更多的计算资源\",{\"1\":{\"123\":1}}],[\"尽管训练的总的token数量可能一致\",{\"1\":{\"123\":1}}],[\"尽管重复数据上的训练会降低预训练模型的效果\",{\"1\":{\"123\":1}}],[\"尽管参数看起来增加了\",{\"1\":{\"40\":1}}],[\"越容易出现过拟合的现象\",{\"1\":{\"123\":1}}],[\"观察模型的性能\",{\"1\":{\"123\":1}}],[\"按照目前模型规模的发展情况\",{\"1\":{\"120\":1}}],[\"背景\",{\"0\":{\"120\":1}}],[\"得出结论\",{\"1\":{\"119\":1}}],[\"得到的新的优化目标如式5\",{\"1\":{\"196\":1}}],[\"得到的结论是在下游任务上也会出现\",{\"1\":{\"123\":1}}],[\"得到我们的新的优化目标\",{\"1\":{\"193\":1}}],[\"得到奖励\",{\"1\":{\"102\":1}}],[\"得到输出的hidden\",{\"1\":{\"40\":1}}],[\"正则技术\",{\"1\":{\"129\":1}}],[\"正则化可以降低多epoch的影响吗\",{\"1\":{\"119\":1}}],[\"正常输入\",{\"1\":{\"44\":1}}],[\"影响多次轮次\",{\"1\":{\"119\":1}}],[\"被分为两大部分\",{\"1\":{\"172\":1}}],[\"被缓存\",{\"1\":{\"156\":1}}],[\"被称为\",{\"1\":{\"118\":1}}],[\"被认为是利用语言模型进行复杂推理的重要步骤\",{\"1\":{\"86\":1}}],[\"价值学习经典的算法有sarsa和q\",{\"1\":{\"111\":1}}],[\"jingjing\",{\"1\":{\"219\":1}}],[\"jiang\",{\"1\":{\"165\":1}}],[\"j\",{\"1\":{\"206\":2}}],[\"jeff\",{\"1\":{\"165\":1}}],[\"jason\",{\"1\":{\"165\":1,\"208\":1}}],[\"july\",{\"1\":{\"107\":1}}],[\"jordan\",{\"1\":{\"107\":1}}],[\"johnson\",{\"1\":{\"139\":2}}],[\"john\",{\"1\":{\"107\":1}}],[\"jtrp0θ\",{\"1\":{\"105\":1}}],[\"见ppo详解\",{\"1\":{\"106\":1}}],[\"∼nθ\",{\"1\":{\"105\":1}}],[\"信任域策略优化\",{\"1\":{\"105\":1}}],[\"信息从下到上形成了一个特定的传播路径\",{\"1\":{\"202\":1}}],[\"信息从底向上传播\",{\"1\":{\"201\":1}}],[\"信息主要沿着这条路径向上传播\",{\"1\":{\"201\":1}}],[\"信息进行传播的方式\",{\"1\":{\"200\":1}}],[\"信息\",{\"1\":{\"82\":1}}],[\"此时\",{\"1\":{\"228\":1}}],[\"此时通过\",{\"1\":{\"203\":1}}],[\"此时拼接完之后已经变回了768列的矩阵\",{\"1\":{\"187\":1}}],[\"此后\",{\"1\":{\"120\":1}}],[\"此b指的baseline\",{\"1\":{\"104\":1}}],[\"此外\",{\"1\":{\"19\":1,\"27\":1,\"41\":1,\"123\":2,\"137\":1,\"138\":1,\"218\":1}}],[\"需要增大β值\",{\"1\":{\"196\":1}}],[\"需要将它们保存至hbm中\",{\"1\":{\"155\":1}}],[\"需要根据它的参数数量来收集足够的token\",{\"1\":{\"122\":1}}],[\"需要在之前梯度计算的公式基础上加一个基准线b\",{\"1\":{\"104\":1}}],[\"需要优化的参数只有θ\",{\"1\":{\"40\":1}}],[\"三个动作的和要为0\",{\"1\":{\"104\":1}}],[\"三部分构成\",{\"1\":{\"45\":1}}],[\"优势函数\",{\"0\":{\"194\":1}}],[\"优势演员\",{\"0\":{\"104\":1}}],[\"优化目标变成了以下式子\",{\"1\":{\"74\":1}}],[\"优化算法\",{\"0\":{\"62\":1}}],[\"优化\",{\"2\":{\"55\":1,\"68\":1}}],[\"距离的含义\",{\"1\":{\"103\":1}}],[\"当前维持的近期生成内容的摘要\",{\"1\":{\"217\":1}}],[\"当根据第二个单词\",{\"1\":{\"203\":1}}],[\"当作\",{\"1\":{\"203\":1}}],[\"当kl散度大于最大值时\",{\"1\":{\"196\":1}}],[\"当kl散度小于最小值时\",{\"1\":{\"196\":1}}],[\"当用户不提供encoder的output时\",{\"1\":{\"175\":1}}],[\"当输入序列较长时\",{\"1\":{\"155\":1}}],[\"当在227个token和229个token上重复训练28次之后发现\",{\"1\":{\"124\":1}}],[\"当较大的模型优于较小的模型时\",{\"1\":{\"122\":1}}],[\"当然要有一个学习率η\",{\"1\":{\"103\":1}}],[\"当且仅当某时刻的状态只取决于上一时刻的状态时\",{\"1\":{\"95\":1}}],[\"用所有单个字符建立最初的词典\",{\"1\":{\"227\":1}}],[\"用户可以直接影响生成内容的方向\",{\"1\":{\"217\":1}}],[\"用于从\",{\"1\":{\"204\":1}}],[\"用于标识多次出现在句子中的\",{\"1\":{\"204\":1}}],[\"用于识别\",{\"1\":{\"204\":1}}],[\"用于训练较小模型的token数量可以被视为完整训练的token要求\",{\"1\":{\"122\":1}}],[\"用的是类似\",{\"1\":{\"202\":1}}],[\"用梯度上升来优化这个新的优化目标\",{\"1\":{\"196\":1}}],[\"用梯度上升来更新参数\",{\"1\":{\"103\":1}}],[\"用来在固定大小的词表中实现可变⻓度的子词\",{\"1\":{\"226\":1}}],[\"用来计量θ和θ\",{\"1\":{\"196\":1}}],[\"用来计量策略θ\",{\"1\":{\"196\":1}}],[\"用来模拟embedding矩阵的效果\",{\"1\":{\"40\":1}}],[\"反之\",{\"1\":{\"103\":1,\"194\":1}}],[\"⋅⋅⋅=p\",{\"1\":{\"103\":1}}],[\"τn上标n代表第n条轨迹\",{\"1\":{\"103\":1}}],[\"τ\",{\"1\":{\"103\":5,\"192\":14,\"193\":13,\"194\":9,\"195\":4,\"196\":4,\"197\":6}}],[\"动作a可以理解为回答问题输出token\",{\"1\":{\"102\":1}}],[\"还能对算法进一步改进\",{\"1\":{\"196\":1}}],[\"还原2\",{\"1\":{\"193\":1}}],[\"还应该对要执行的任务进行调节\",{\"1\":{\"182\":1}}],[\"还是会被模型记住\",{\"1\":{\"164\":1}}],[\"还是会从整体进行考虑\",{\"1\":{\"100\":1}}],[\"还要重新使用策略π与环境互动收集数据\",{\"1\":{\"102\":1}}],[\"跳到下一个状态s\",{\"1\":{\"102\":1}}],[\"执行动作\",{\"1\":{\"102\":1}}],[\"执行动作后转移到哪个状态由环境决定\",{\"1\":{\"94\":1}}],[\"适用于非连续和连续的动作\",{\"1\":{\"100\":1}}],[\"获取动作时只需对概率分布进行采样即可\",{\"1\":{\"96\":1}}],[\"随机性策略π\",{\"1\":{\"96\":1}}],[\"随着模型的发展\",{\"1\":{\"133\":1}}],[\"随着大语言模型的规模和训练数据集中token数量的增加\",{\"1\":{\"118\":1}}],[\"随着\",{\"1\":{\"82\":1}}],[\"能够模拟\",{\"1\":{\"215\":1}}],[\"能够正确输出结果\",{\"1\":{\"202\":1}}],[\"能够在各项指标上达到最优\",{\"1\":{\"142\":1}}],[\"能够次线性查询\",{\"1\":{\"137\":1}}],[\"能够处理长度不限的输入\",{\"1\":{\"137\":1}}],[\"能够同时解决冗余与依赖问题\",{\"1\":{\"136\":1}}],[\"能够学习到确定性策略\",{\"1\":{\"96\":1}}],[\"能够触发越来越多的与\",{\"1\":{\"82\":1}}],[\"否则便需要进行采样\",{\"1\":{\"96\":1}}],[\"否则这道题很难回答好\",{\"1\":{\"81\":1}}],[\"显示了在书籍摘要上的结果\",{\"1\":{\"143\":1}}],[\"显示了本文对\",{\"1\":{\"140\":1}}],[\"显然\",{\"1\":{\"120\":1}}],[\"显然模型的优化目标可以用v\",{\"1\":{\"95\":1}}],[\"显式\",{\"1\":{\"43\":1}}],[\"∣s\",{\"1\":{\"95\":1,\"96\":2}}],[\"∈s​p\",{\"1\":{\"95\":1}}],[\"∈z∑​t=1∑∣y∣​log\",{\"1\":{\"40\":2}}],[\"期望越大说明当前状态越有利\",{\"1\":{\"95\":1}}],[\"马尔科夫决策过程\",{\"0\":{\"95\":1}}],[\"奖励模型阶段\",{\"1\":{\"163\":1}}],[\"奖励都是正的\",{\"1\":{\"104\":1}}],[\"奖励\",{\"1\":{\"94\":1}}],[\"状态的转换概率分布p\",{\"1\":{\"103\":1}}],[\"状态\",{\"1\":{\"94\":1,\"103\":1}}],[\"环境\",{\"1\":{\"94\":1}}],[\"行为策略\",{\"1\":{\"114\":1}}],[\"行为\",{\"1\":{\"94\":1}}],[\"智能体遵循该策略选择动作\",{\"1\":{\"114\":1}}],[\"智能体与环境交互示意图\",{\"1\":{\"103\":1}}],[\"智能体\",{\"1\":{\"94\":1}}],[\"智能呢\",{\"1\":{\"80\":1}}],[\"任何强化学习都包含这几个基本概念\",{\"1\":{\"94\":1}}],[\"任务回路应该是\",{\"1\":{\"205\":1}}],[\"任务回路是在层级知识体系结构上建立起来的\",{\"1\":{\"205\":1}}],[\"任务从数据中学习知识\",{\"1\":{\"205\":1}}],[\"任务\",{\"1\":{\"8\":1,\"79\":1,\"205\":1}}],[\"任务的超过\",{\"1\":{\"8\":1}}],[\"任务构建\",{\"1\":{\"8\":1}}],[\"简单来说\",{\"1\":{\"114\":1}}],[\"简单来说是指智能体在复杂\",{\"1\":{\"94\":1}}],[\"简介\",{\"1\":{\"8\":1}}],[\"强调如何基于环境而行动\",{\"1\":{\"93\":1}}],[\"强化学习包含两个策略\",{\"1\":{\"114\":1}}],[\"强化学习算法分类\",{\"1\":{\"96\":1}}],[\"强化学习算法种类繁多\",{\"1\":{\"96\":1}}],[\"强化学习分类\",{\"0\":{\"96\":1}}],[\"强化学习示意图\",{\"1\":{\"94\":1}}],[\"强化学习被广泛认为是实现通用人工智能\",{\"1\":{\"93\":1}}],[\"强化学习不需要带标签的输入输出对\",{\"1\":{\"93\":1}}],[\"强化学习是除了监督学习和非监督学习之外的第三种基本的机器学习方法\",{\"1\":{\"93\":1}}],[\"强化学习\",{\"1\":{\"93\":1},\"2\":{\"199\":1,\"235\":1}}],[\"情况下\",{\"1\":{\"86\":1}}],[\"zy出现的频率大\",{\"1\":{\"228\":1}}],[\"zydzyac\",{\"1\":{\"228\":1}}],[\"z=aa\",{\"1\":{\"228\":3}}],[\"zabdzabac\",{\"1\":{\"228\":1}}],[\"z\",{\"1\":{\"185\":2}}],[\"zeros\",{\"1\":{\"187\":1}}],[\"zero\",{\"1\":{\"86\":1,\"165\":1}}],[\"zhou\",{\"1\":{\"208\":1}}],[\"zhifang\",{\"1\":{\"165\":1}}],[\"zhihu\",{\"1\":{\"77\":1,\"200\":1}}],[\"zhao\",{\"1\":{\"165\":1}}],[\"zhuanlan\",{\"1\":{\"77\":1,\"200\":1}}],[\"框架\",{\"1\":{\"86\":1,\"136\":1,\"208\":1}}],[\"长期记忆\",{\"1\":{\"217\":1}}],[\"长文本\",{\"1\":{\"142\":1}}],[\"长文档摘要\",{\"0\":{\"142\":1}}],[\"长文档推理提示框架\",{\"0\":{\"86\":1}}],[\"长输入\",{\"1\":{\"137\":1}}],[\"长度就自然可以进行扩展\",{\"1\":{\"155\":1}}],[\"长度\",{\"1\":{\"64\":1}}],[\"联合构成的\",{\"1\":{\"83\":1}}],[\"远远多于网络参数\",{\"1\":{\"83\":1}}],[\"单独成字符\",{\"1\":{\"232\":1}}],[\"单语义神经元\",{\"1\":{\"83\":2}}],[\"单语义神经元与多语义神经元示意图\",{\"1\":{\"83\":1}}],[\"单词位置\",{\"1\":{\"82\":1}}],[\"单词这个位置\",{\"1\":{\"82\":1}}],[\"单词\",{\"1\":{\"82\":1,\"203\":2}}],[\"知识点有不同的抽象层级\",{\"1\":{\"205\":1}}],[\"知识点在\",{\"0\":{\"83\":1}}],[\"知识回路\",{\"1\":{\"204\":1},\"2\":{\"207\":1}}],[\"知识回路识别正确答案\",{\"1\":{\"204\":1}}],[\"知识回路数字比较示意图\",{\"1\":{\"202\":1}}],[\"知识回路中信息传播示意图\",{\"1\":{\"202\":1}}],[\"知乎原文\",{\"1\":{\"77\":1,\"200\":1}}],[\">\",{\"1\":{\"82\":1,\"151\":1,\"183\":1,\"203\":1}}],[\"里\",{\"1\":{\"203\":1}}],[\"里会编码\",{\"1\":{\"82\":1}}],[\"里增加信息\",{\"1\":{\"82\":1}}],[\"来规范\",{\"1\":{\"217\":1}}],[\"来让\",{\"1\":{\"216\":1}}],[\"来实现交互式超长文本生成\",{\"1\":{\"215\":1}}],[\"来自苏黎世联邦理工和波形智能的团队发布了\",{\"1\":{\"215\":1}}],[\"来自卡内基梅隆大学的研究者引入了\",{\"1\":{\"137\":1}}],[\"来执行对某些输入\",{\"1\":{\"204\":1}}],[\"来参与\",{\"1\":{\"138\":1}}],[\"来说是足够长的\",{\"1\":{\"137\":1}}],[\"来对某个具体特征或知识点进行编码\",{\"1\":{\"83\":1}}],[\"来用\",{\"1\":{\"83\":1}}],[\"来做到的\",{\"1\":{\"82\":1}}],[\"来产生下一个单词\",{\"1\":{\"79\":1}}],[\"输出内容的长度限制始终是限制\",{\"1\":{\"216\":1}}],[\"输出剩余的名称\",{\"1\":{\"204\":1}}],[\"输出答案\",{\"1\":{\"201\":1}}],[\"输出了1行18列的矩阵\",{\"1\":{\"187\":1}}],[\"输出动作概率分布\",{\"1\":{\"102\":1}}],[\"输出\",{\"1\":{\"82\":1,\"203\":2}}],[\"输入1行6列的矩阵\",{\"1\":{\"187\":1}}],[\"输入\",{\"1\":{\"142\":2,\"201\":1}}],[\"输入通过作者的预训练模型\",{\"1\":{\"74\":1}}],[\"输入序列通常是变长的\",{\"1\":{\"61\":1}}],[\"位置是比较关键的\",{\"1\":{\"82\":1}}],[\"关系密切\",{\"1\":{\"83\":1}}],[\"关系抽取\",{\"1\":{\"82\":1}}],[\"关系传播\",{\"1\":{\"82\":1}}],[\"关键的增量矩阵被分配了高秩\",{\"1\":{\"41\":1}}],[\"整个过程总体发生在\",{\"1\":{\"82\":1}}],[\"整行放在共享内存进行\",{\"1\":{\"64\":1}}],[\"属性\",{\"1\":{\"82\":2}}],[\"层级化的知识结构以及各种任务回路\",{\"1\":{\"205\":1}}],[\"层的\",{\"1\":{\"202\":1}}],[\"层的第\",{\"1\":{\"202\":1}}],[\"层\",{\"1\":{\"82\":1,\"202\":4}}],[\"层数越来越高\",{\"1\":{\"82\":1}}],[\"先通过\",{\"1\":{\"82\":1}}],[\"往上走的过程中\",{\"1\":{\"82\":1}}],[\"主要由三个步骤构成\",{\"1\":{\"204\":1}}],[\"主要聚焦于关键信息\",{\"1\":{\"202\":1}}],[\"主要接收信息来源于\",{\"1\":{\"202\":1}}],[\"主要作用是聚焦到\",{\"1\":{\"202\":1}}],[\"主要解决的是将p\",{\"1\":{\"155\":1}}],[\"主要包括s=qk\",{\"1\":{\"155\":1}}],[\"主要是用来进行信息比较和搬运的\",{\"1\":{\"82\":1}}],[\"主题补充\",{\"1\":{\"82\":1}}],[\"主页\",{\"0\":{\"0\":1},\"2\":{\"1\":1}}],[\"剖析自回归语言模型中事实关联的回忆\",{\"1\":{\"82\":1}}],[\"=τ∑​\",{\"1\":{\"192\":2}}],[\"=softmax\",{\"1\":{\"186\":1}}],[\"=i=1∏n​p\",{\"1\":{\"182\":1}}],[\"=e\",{\"1\":{\"105\":1}}],[\"=eτ∼pθ​\",{\"1\":{\"103\":1,\"192\":1}}],[\"=r\",{\"1\":{\"95\":1}}],[\"=σa∈a​π\",{\"1\":{\"95\":1}}],[\"=p\",{\"1\":{\"95\":1}}],[\"=\",{\"1\":{\"81\":1,\"171\":10,\"183\":1,\"185\":5,\"187\":10}}],[\"=tw−p\",{\"1\":{\"53\":1}}],[\"无法进行缩放\",{\"1\":{\"137\":1}}],[\"无模型的强化学习可以分为基于价值的和基于策略的\",{\"1\":{\"96\":1}}],[\"无损\",{\"1\":{\"81\":1}}],[\"无监督预训练\",{\"0\":{\"73\":1}}],[\"第\",{\"1\":{\"202\":1}}],[\"第二个\",{\"1\":{\"203\":1}}],[\"第二个问题\",{\"1\":{\"81\":1}}],[\"第二类是使用指令微调的instructgpt系列\",{\"1\":{\"161\":1}}],[\"第一个是中间层的某些\",{\"1\":{\"202\":1}}],[\"第一个问题\",{\"1\":{\"81\":1}}],[\"第一类是在代码上训练\",{\"1\":{\"161\":1}}],[\"第一层加入soft\",{\"1\":{\"46\":1}}],[\"两者就得到很高相似性\",{\"1\":{\"203\":1}}],[\"两个完全相同时\",{\"1\":{\"195\":1}}],[\"两个模型针对质数概念理解的测试对比\",{\"1\":{\"81\":1}}],[\"两种设置\",{\"0\":{\"28\":1}}],[\"假设某一状态下有三个动作\",{\"1\":{\"104\":1}}],[\"假设在st​执行at​\",{\"1\":{\"103\":1}}],[\"假设要编码的特征的数量\",{\"1\":{\"83\":1}}],[\"假设要传输的序列是连续质数数字序列\",{\"1\":{\"81\":1}}],[\"假设有一个标记的数据集c\",{\"1\":{\"74\":1}}],[\"举个例子\",{\"1\":{\"81\":1}}],[\"就达成了从上文拷贝\",{\"1\":{\"203\":1}}],[\"就可以优化我们所要优化的分布θ\",{\"1\":{\"192\":1}}],[\"就gpu内存利用而言\",{\"1\":{\"155\":1}}],[\"就是说输入有两个实体\",{\"1\":{\"204\":1}}],[\"就是ppo算法\",{\"1\":{\"196\":1}}],[\"就是看行为策略和目标策略是否相同\",{\"1\":{\"114\":1}}],[\"就是通过语言中前面的单词\",{\"1\":{\"79\":1}}],[\"就要减少概率\",{\"1\":{\"103\":1}}],[\"就要增加概率\",{\"1\":{\"103\":1}}],[\"就能探测到输入中我们想识别的那个知识点\",{\"1\":{\"83\":1}}],[\"就会出现图右小模型这种不知所云的回答\",{\"1\":{\"81\":1}}],[\"就代表了它具备更高的智能呢\",{\"1\":{\"81\":1}}],[\"照此思路推进大模型研发方向的一个核心理念\",{\"1\":{\"81\":1}}],[\"则\",{\"1\":{\"205\":1}}],[\"则atn​\",{\"1\":{\"103\":1}}],[\"则模型智能程度越高\",{\"1\":{\"81\":1}}],[\"则其压缩效率就越高\",{\"1\":{\"81\":1}}],[\"预测精准性\",{\"1\":{\"205\":1}}],[\"预测精准性增加\",{\"1\":{\"205\":1}}],[\"预测\",{\"1\":{\"203\":1,\"204\":1}}],[\"预测得越准确\",{\"1\":{\"81\":1}}],[\"预训练\",{\"1\":{\"137\":1}}],[\"预训练数据集重复的影响是什么\",{\"1\":{\"119\":1}}],[\"预备知识\",{\"0\":{\"78\":1}}],[\"压缩即智能\",{\"0\":{\"81\":1}}],[\"利用\",{\"0\":{\"80\":1}}],[\"利用这个性质可以实现\",{\"1\":{\"65\":1}}],[\"都能取得一定的改进效果\",{\"1\":{\"143\":1}}],[\"都检索更长的输入系列的前\",{\"1\":{\"140\":1}}],[\"都没有使用dropout\",{\"1\":{\"129\":1}}],[\"都采用\",{\"1\":{\"79\":1}}],[\"都可以拆解为\",{\"1\":{\"65\":1}}],[\"文章讨论了在重复的数据集上进行多次训练对大语言模型性能的影响\",{\"1\":{\"118\":1}}],[\"文字是由内在智能产生的\",{\"1\":{\"77\":1}}],[\"文学\",{\"1\":{\"17\":1}}],[\"另外一个关键是第\",{\"1\":{\"202\":1}}],[\"另外一种观点则认为\",{\"1\":{\"77\":1}}],[\"另外\",{\"1\":{\"83\":1,\"202\":1}}],[\"另一方面说明大模型在这方面表现确实比小模型要好\",{\"1\":{\"81\":1}}],[\"另一个优点是\",{\"1\":{\"41\":1}}],[\"请联系删除\",{\"1\":{\"77\":1,\"200\":1}}],[\"请访问我们的网站或查看我们的论文以了解更多详细信息\",{\"1\":{\"26\":1}}],[\"版权归属原作者\",{\"1\":{\"77\":1,\"200\":1}}],[\"版本并纠正或删除了一部分错误试题\",{\"1\":{\"27\":1}}],[\"一种让大语言模型\",{\"1\":{\"215\":1}}],[\"一种观点认为gpt\",{\"1\":{\"77\":1}}],[\"一般基于kl惩罚的ppo算法称为ppo1算法\",{\"1\":{\"197\":1}}],[\"一般得到的是随机性策略\",{\"1\":{\"96\":1}}],[\"一言以蔽之\",{\"1\":{\"185\":1}}],[\"一文中\",{\"1\":{\"137\":1}}],[\"一个示例\",{\"0\":{\"228\":1}}],[\"一个非重复实体\",{\"1\":{\"204\":1}}],[\"一个重复实体\",{\"1\":{\"204\":1}}],[\"一个是模型所需要的计算量\",{\"1\":{\"126\":1}}],[\"一个是模型参数\",{\"1\":{\"126\":1}}],[\"一个随机过程被称为具有马尔可夫性质\",{\"1\":{\"95\":1}}],[\"一个知识点会激发很多对它进行编码的\",{\"1\":{\"83\":1}}],[\"一个神经元编码一个知识\",{\"1\":{\"83\":1}}],[\"一起完成的\",{\"1\":{\"81\":1}}],[\"一\",{\"0\":{\"77\":1}}],[\"深度学习\",{\"2\":{\"76\":1,\"135\":1}}],[\"监督微调\",{\"0\":{\"74\":1}}],[\"⋯\",{\"1\":{\"73\":2}}],[\"usa\",{\"1\":{\"165\":1}}],[\"use\",{\"1\":{\"162\":1,\"171\":1}}],[\"ul2这种模型就不适合多epoch的训练\",{\"1\":{\"128\":1}}],[\"u−1\",{\"1\":{\"73\":1}}],[\"u−k\",{\"1\":{\"73\":1}}],[\"u1​\",{\"1\":{\"73\":1}}],[\"unsupervised\",{\"1\":{\"181\":1,\"219\":1}}],[\"unlimiform\",{\"1\":{\"137\":1}}],[\"unlimiformer原理图\",{\"1\":{\"140\":1}}],[\"unlimiformer编码\",{\"0\":{\"139\":1}}],[\"unlimiformer技术原理\",{\"0\":{\"138\":1}}],[\"unlimiformer\",{\"0\":{\"136\":1},\"1\":{\"137\":8,\"138\":2,\"142\":1}}],[\"unlimited\",{\"1\":{\"137\":1}}],[\"under\",{\"0\":{\"118\":1},\"1\":{\"118\":1}}],[\"understanding\",{\"0\":{\"70\":1}}],[\"understands\",{\"1\":{\"39\":1}}],[\"un​\",{\"1\":{\"73\":1}}],[\"unified\",{\"1\":{\"136\":1}}],[\"unifiedskg\",{\"1\":{\"8\":4}}],[\"uniformer\",{\"1\":{\"136\":2}}],[\"universally\",{\"1\":{\"39\":1}}],[\"unnatural\",{\"1\":{\"7\":2,\"8\":2}}],[\"比如我们想编码\",{\"1\":{\"228\":1}}],[\"比如写小说\",{\"1\":{\"217\":1}}],[\"比如如果训练数据里大量出现\",{\"1\":{\"205\":1}}],[\"比如第\",{\"1\":{\"202\":1}}],[\"比如图中\",{\"1\":{\"202\":1}}],[\"比如状态价值函数\",{\"1\":{\"194\":1}}],[\"比如\",{\"1\":{\"66\":1,\"232\":1}}],[\"比模型集成的成本小多了\",{\"1\":{\"44\":1}}],[\"字节对算法流程\",{\"1\":{\"227\":1}}],[\"字节对编码\",{\"1\":{\"226\":1}}],[\"字节\",{\"2\":{\"68\":1}}],[\"字节内部版本还支持了许多\",{\"1\":{\"66\":1}}],[\"字节跳动\",{\"1\":{\"61\":1,\"66\":1}}],[\"字节跳动aml团队先前提出的\",{\"1\":{\"61\":1}}],[\"除了一个在gpt2论文中提到的一个额外限制\",{\"1\":{\"230\":1}}],[\"除了生成ai生成内容\",{\"1\":{\"218\":1}}],[\"除了chatgpt是基于gpt3\",{\"1\":{\"163\":1}}],[\"除了多一个t的对角元素之外还多一个偏移向量\",{\"1\":{\"53\":1}}],[\"除此之外\",{\"1\":{\"66\":1}}],[\"目标策略与行为策略并不一致\",{\"1\":{\"114\":1}}],[\"目标策略与行为策略保持一致\",{\"1\":{\"112\":1}}],[\"目的\",{\"1\":{\"104\":1}}],[\"目前大语言模型的训练目标有很多\",{\"1\":{\"128\":1}}],[\"目前发现\",{\"1\":{\"83\":2}}],[\"目前规模够大的\",{\"1\":{\"79\":1}}],[\"目前\",{\"1\":{\"66\":1}}],[\"目录\",{\"0\":{\"4\":1}}],[\"支持\",{\"0\":{\"66\":1}}],[\"变成qkv\",{\"1\":{\"156\":1}}],[\"变成了n\",{\"1\":{\"40\":1}}],[\"变换网络\",{\"1\":{\"137\":1}}],[\"变种\",{\"0\":{\"66\":1},\"1\":{\"66\":1}}],[\"内容创作者只需先选择一个主题\",{\"1\":{\"217\":1}}],[\"内部是这样做的\",{\"1\":{\"83\":1}}],[\"内线程通信交换数据\",{\"1\":{\"65\":1}}],[\"内线程读取连续的\",{\"1\":{\"65\":1}}],[\"内存方面\",{\"1\":{\"140\":1}}],[\"内存中\",{\"1\":{\"137\":1}}],[\"内存\",{\"2\":{\"55\":1}}],[\"效率很低\",{\"1\":{\"65\":1}}],[\"效果追上了fine\",{\"1\":{\"45\":1}}],[\"依次读取所有的子问题\",{\"1\":{\"65\":1}}],[\"共同组成\",{\"1\":{\"203\":1}}],[\"共享子问题参数\",{\"1\":{\"65\":1}}],[\"共21个任务数据集\",{\"1\":{\"8\":1}}],[\"读取子问题参数会有不小的开销\",{\"1\":{\"65\":1}}],[\"通常输入超过\",{\"1\":{\"137\":1}}],[\"通常具有\",{\"1\":{\"137\":1}}],[\"通常较大\",{\"1\":{\"65\":1}}],[\"通过大模型来改善数学计算\",{\"1\":{\"208\":1}}],[\"通过探究\",{\"1\":{\"202\":1}}],[\"通过限定pθ\",{\"1\":{\"197\":1}}],[\"通过采样足够多的样本来用均值估算数学期望\",{\"1\":{\"192\":1}}],[\"通过attention层的第二个conv1d\",{\"1\":{\"187\":1}}],[\"通过attention层的第一个conv1d\",{\"1\":{\"187\":1}}],[\"通过将\",{\"1\":{\"138\":1}}],[\"通过moe扫描确定稠密模型的最佳超参数\",{\"0\":{\"132\":1}}],[\"通过混合专家模型\",{\"1\":{\"119\":1}}],[\"通过与环境的交互得到下一步的状态和奖励\",{\"1\":{\"96\":1}}],[\"通过组合多个\",{\"1\":{\"83\":1}}],[\"通过\",{\"1\":{\"82\":2,\"203\":1,\"215\":1}}],[\"通过每个\",{\"1\":{\"82\":1}}],[\"通过以下方面体现\",{\"1\":{\"74\":1}}],[\"通过共享使参数存储量从\",{\"1\":{\"65\":1}}],[\"通过手写\",{\"1\":{\"64\":1}}],[\"通过对模型输出答案打分来训练奖励模型\",{\"1\":{\"163\":1}}],[\"通过对输入的重排列\",{\"1\":{\"61\":1}}],[\"通过对权重矩阵进行重要性评分\",{\"1\":{\"41\":1}}],[\"通过操纵奇异值\",{\"1\":{\"41\":1}}],[\"时的退化\",{\"1\":{\"123\":1}}],[\"时\",{\"1\":{\"65\":1}}],[\"固定的上下文窗口可能会在注意力不那么关注的\",{\"1\":{\"138\":1}}],[\"固定分块大小\",{\"1\":{\"65\":1}}],[\"固定预训练模型\",{\"1\":{\"46\":1}}],[\"固定预训练参数\",{\"1\":{\"44\":1}}],[\"传递真实的\",{\"1\":{\"65\":1}}],[\"传统离散prompt直接将模板t的每个token映射为对应的embedding\",{\"1\":{\"46\":1}}],[\"传统上定义为将输入字符串映射到输出字符串\",{\"1\":{\"7\":1}}],[\"开发的\",{\"1\":{\"65\":1}}],[\"开源\",{\"1\":{\"63\":1}}],[\"开源了一系列工具\",{\"1\":{\"8\":1}}],[\"开源了其在自己产品线中使用的\",{\"1\":{\"7\":1}}],[\"到字符串的完整过程\",{\"1\":{\"232\":1}}],[\"到\",{\"1\":{\"64\":1,\"202\":1}}],[\"到多种非英语语言\",{\"1\":{\"8\":1}}],[\"分词器\",{\"2\":{\"235\":1}}],[\"分词算法\",{\"0\":{\"227\":1}}],[\"分布差异过大的另一种方法\",{\"1\":{\"197\":1}}],[\"分布的不相似度的值\",{\"1\":{\"196\":1}}],[\"分布的差异程度\",{\"1\":{\"196\":1,\"197\":1}}],[\"分成两个\",{\"1\":{\"64\":1}}],[\"分别是a\",{\"1\":{\"104\":1}}],[\"分别是\",{\"1\":{\"39\":1,\"41\":1}}],[\"分别是初中\",{\"1\":{\"27\":1}}],[\"保证高性能\",{\"1\":{\"64\":1}}],[\"矩阵乘通过调用\",{\"1\":{\"64\":1}}],[\"操作\",{\"1\":{\"64\":1,\"227\":1}}],[\"算术编码的编码补偿能力\",{\"1\":{\"81\":1}}],[\"算术编码\",{\"1\":{\"81\":1}}],[\"算子\",{\"1\":{\"64\":1}}],[\"算法也就结束了\",{\"1\":{\"228\":1}}],[\"算法核心思想\",{\"0\":{\"102\":1}}],[\"算法步骤如下\",{\"1\":{\"63\":1}}],[\"算法\",{\"0\":{\"63\":1},\"1\":{\"63\":1}}],[\"每一个它将自动生成第一段\",{\"1\":{\"217\":1}}],[\"每轮训练结束之后参数θ都要更新\",{\"1\":{\"192\":1}}],[\"每次产生新单词后\",{\"1\":{\"181\":1}}],[\"每次预测都需要结合之前的几个demonstration\",{\"1\":{\"164\":1}}],[\"每108个流式多核处理器各有192kb的片上sram\",{\"1\":{\"155\":1}}],[\"每组各自计算互不影响\",{\"1\":{\"64\":1}}],[\"每个gpt2block\",{\"1\":{\"188\":1}}],[\"每个gpt2mlp\",{\"1\":{\"188\":1}}],[\"每个gpt2mlp中的第二个conv1d\",{\"1\":{\"188\":1}}],[\"每个gpt2mlp中的第一个conv1d\",{\"1\":{\"188\":1}}],[\"每个gpt2attention\",{\"1\":{\"188\":1}}],[\"每个gpt2attention中的第二个conv1d\",{\"1\":{\"188\":1}}],[\"每个gpt2attention中的第一个conv1d\",{\"1\":{\"188\":1}}],[\"每个newgeluactivation\",{\"1\":{\"188\":1}}],[\"每个ln\",{\"1\":{\"188\":1}}],[\"每个dropout\",{\"1\":{\"188\":1}}],[\"每个示例由可变长度的符号序列\",{\"1\":{\"182\":1}}],[\"每个头只单独保留一份query参数\",{\"1\":{\"156\":1}}],[\"每个神经元会对输入中的多个不同知识点都有响应\",{\"1\":{\"83\":1}}],[\"每个线程的读取次数降低到\",{\"1\":{\"65\":1}}],[\"每个线程都需要遍历读取所有的子问题大小\",{\"1\":{\"65\":1}}],[\"每个\",{\"1\":{\"65\":1}}],[\"每个矩阵乘子问题根据问题大小和分块大小\",{\"1\":{\"65\":1}}],[\"每个矩阵乘子问题\",{\"1\":{\"65\":1}}],[\"每个输入产生多组\",{\"1\":{\"64\":1}}],[\"每个学科内两百到五百道不等的四个选项的单项选择题\",{\"1\":{\"27\":1}}],[\"旧版的多头注意力\",{\"1\":{\"64\":1}}],[\"融合的多头注意力\",{\"0\":{\"64\":1}}],[\"再根据字节高四位来唯一编码\",{\"1\":{\"232\":1}}],[\"再根据instructgpt发布后半年多才发布chatgpt\",{\"1\":{\"163\":1}}],[\"再用这个估算值对分布做梯度上升求式1\",{\"1\":{\"192\":1}}],[\"再用第二个linear层b\",{\"1\":{\"40\":1}}],[\"再然后合并\",{\"1\":{\"186\":1}}],[\"再把每一条轨迹的值加起来除以n取平均\",{\"1\":{\"103\":1}}],[\"再把这些块平均分配到每个\",{\"1\":{\"65\":1}}],[\"再参与后续的矩阵乘计算\",{\"1\":{\"63\":1}}],[\"重复遍历\",{\"1\":{\"227\":1}}],[\"重复训练的性能影响\",{\"1\":{\"124\":1}}],[\"重要性采样\",{\"0\":{\"193\":1}}],[\"重要性感知秩分配\",{\"1\":{\"41\":1}}],[\"重叠编码示意图\",{\"1\":{\"83\":1}}],[\"重排列为\",{\"1\":{\"63\":1}}],[\"把所有字符通过utf\",{\"1\":{\"231\":1}}],[\"把θ加上梯度∇rθ​\",{\"1\":{\"103\":1}}],[\"把\",{\"1\":{\"82\":1,\"203\":1}}],[\"把单词\",{\"1\":{\"82\":1}}],[\"把之前的修饰语\",{\"1\":{\"82\":1}}],[\"把llm看做函数\",{\"1\":{\"80\":1}}],[\"把问题大小传入到\",{\"1\":{\"65\":1}}],[\"把输入张量从\",{\"1\":{\"63\":1}}],[\"把预训练大模型freeze住\",{\"1\":{\"43\":1}}],[\"团队已经在\",{\"1\":{\"66\":1}}],[\"团队之前的工作\",{\"1\":{\"63\":1}}],[\"团队提出了\",{\"1\":{\"61\":1}}],[\"计算上和环境上的代价都不小\",{\"1\":{\"137\":1}}],[\"计算下一个token的在词表中的概率分布\",{\"1\":{\"80\":1}}],[\"计算\",{\"1\":{\"61\":1,\"63\":1}}],[\"部分的性能\",{\"1\":{\"64\":1}}],[\"部分仍然需要\",{\"1\":{\"61\":1}}],[\"部分缓解了\",{\"1\":{\"7\":1}}],[\"才能利用批处理加速transformer计算\",{\"1\":{\"61\":1}}],[\"要预测\",{\"1\":{\"203\":1}}],[\"要产生输出\",{\"1\":{\"82\":1}}],[\"要求输入序列长度相同\",{\"1\":{\"61\":1}}],[\"要高于模型的指标\",{\"1\":{\"51\":1}}],[\"介绍\",{\"0\":{\"61\":1,\"136\":1}}],[\"介绍页\",{\"0\":{\"2\":1}}],[\"成功避免了传统实现中的冗余运算\",{\"1\":{\"60\":1}}],[\"针对自然语言处理常见的可变长输入\",{\"1\":{\"60\":1}}],[\"机制是由多个\",{\"1\":{\"83\":1}}],[\"机器学习之强化学习中的价值学习\",{\"0\":{\"111\":1},\"2\":{\"117\":1}}],[\"机器学习之强化学习中的策略学习\",{\"0\":{\"100\":1},\"2\":{\"110\":1}}],[\"机器学习之强化学习概述\",{\"0\":{\"93\":1},\"2\":{\"99\":1}}],[\"机器学习\",{\"2\":{\"55\":1,\"135\":1,\"145\":1}}],[\"机构\",{\"1\":{\"15\":1,\"208\":1}}],[\"非对称量化\",{\"0\":{\"53\":1}}],[\"少存了w的一半大小\",{\"1\":{\"52\":1}}],[\"少样本的设置\",{\"1\":{\"19\":1}}],[\"少样本评估结果\",{\"1\":{\"18\":2}}],[\"网络相当于舍弃了w\",{\"1\":{\"52\":1}}],[\"量化之后\",{\"1\":{\"52\":1}}],[\"量化操作仅针对w\",{\"1\":{\"52\":1}}],[\"量化的目是为了减少计算时间和计算能耗\",{\"1\":{\"51\":1}}],[\"前有没有空格是不算作同一个token的\",{\"1\":{\"233\":1}}],[\"前者更容易出现过拟合\",{\"1\":{\"124\":1}}],[\"前向传播的计算公式变成了\",{\"1\":{\"52\":1}}],[\"前缀完全由自由参数组成\",{\"1\":{\"43\":1}}],[\"前缀微调只优化了前缀\",{\"1\":{\"43\":1}}],[\"前缀微调\",{\"1\":{\"43\":1,\"45\":1}}],[\"9个\",{\"1\":{\"163\":1}}],[\"907\",{\"1\":{\"152\":1}}],[\"960\",{\"1\":{\"152\":1}}],[\"960=134\",{\"1\":{\"152\":1}}],[\"936\",{\"1\":{\"152\":1}}],[\"936+16\",{\"1\":{\"152\":1}}],[\"99\",{\"1\":{\"140\":1}}],[\"9\",{\"0\":{\"130\":1},\"1\":{\"165\":1,\"185\":1,\"202\":1}}],[\"98\",{\"1\":{\"52\":1}}],[\"95\",{\"1\":{\"52\":1}}],[\"选择优先级最高的词对\",{\"1\":{\"231\":1}}],[\"选择了一部分prompt\",{\"1\":{\"163\":1}}],[\"选择价值最高的动作\",{\"1\":{\"111\":1}}],[\"选择不同的prompt对下游任务的性能影响较大\",{\"1\":{\"45\":1}}],[\"选定t保证w\",{\"1\":{\"52\":1}}],[\"相比现有的方法\",{\"1\":{\"217\":1}}],[\"相比结构\",{\"1\":{\"178\":1}}],[\"相比于当前领域的研究\",{\"1\":{\"217\":1}}],[\"相比于初代模型\",{\"1\":{\"146\":1}}],[\"相比于传统的微调\",{\"1\":{\"43\":1}}],[\"相对更高质量的数据集并不能降低重复训练带来的影响\",{\"1\":{\"125\":1}}],[\"相对大量数据\",{\"1\":{\"81\":1}}],[\"相关研究\",{\"0\":{\"219\":1}}],[\"相关\",{\"1\":{\"82\":1}}],[\"相关信息集成到\",{\"1\":{\"82\":1}}],[\"相同参数规模不同计算量的模型都会受到重复数据集训练的影响\",{\"1\":{\"127\":1}}],[\"相同\",{\"1\":{\"65\":1}}],[\"相当于w\",{\"1\":{\"52\":1}}],[\"令w=tw\",{\"1\":{\"52\":1}}],[\"wpe\",{\"1\":{\"183\":1,\"188\":1}}],[\"wp是position嵌入矩阵\",{\"1\":{\"73\":1}}],[\"wte+wpe+gpt2block\",{\"1\":{\"188\":1}}],[\"wte\",{\"1\":{\"183\":1,\"188\":1}}],[\"why\",{\"1\":{\"165\":1}}],[\"wang\",{\"1\":{\"206\":1,\"208\":1}}],[\"war\",{\"1\":{\"202\":1}}],[\"warp\",{\"1\":{\"65\":4}}],[\"wainwright\",{\"1\":{\"165\":1}}],[\"written\",{\"1\":{\"162\":1}}],[\"wei\",{\"1\":{\"165\":2,\"208\":1}}],[\"weight和bias有可训练参数\",{\"1\":{\"187\":1}}],[\"weight\",{\"1\":{\"129\":1,\"187\":2}}],[\"we\",{\"1\":{\"162\":2}}],[\"word\",{\"1\":{\"151\":2,\"152\":2}}],[\"workshop\",{\"1\":{\"8\":1}}],[\"wu\",{\"1\":{\"137\":1,\"165\":1}}],[\"wd\",{\"1\":{\"129\":1}}],[\"wild\",{\"1\":{\"204\":1,\"206\":1}}],[\"wikipedia\",{\"1\":{\"125\":1}}],[\"with\",{\"1\":{\"83\":1,\"137\":1,\"161\":1,\"165\":1,\"203\":1,\"219\":1}}],[\"wise前馈层\",{\"1\":{\"73\":1}}],[\"wmma\",{\"1\":{\"64\":1}}],[\"w\",{\"1\":{\"52\":3,\"187\":3}}],[\"基座模型的升级\",{\"0\":{\"147\":1}}],[\"基本概念\",{\"0\":{\"94\":1}}],[\"基准\",{\"1\":{\"52\":1}}],[\"基于当前的情节铺设\",{\"1\":{\"217\":1}}],[\"基于变换器\",{\"1\":{\"216\":1}}],[\"基于clip的ppo算法称为ppo2算法\",{\"1\":{\"197\":1}}],[\"基于chatglm初代模型的开发经验\",{\"1\":{\"146\":1}}],[\"基于encoder和decoder的三种架构\",{\"0\":{\"169\":1}}],[\"基于multi\",{\"1\":{\"146\":1}}],[\"基于flashattention技术\",{\"1\":{\"146\":1}}],[\"基于\",{\"1\":{\"143\":1}}],[\"基于价值的\",{\"1\":{\"100\":1,\"111\":1}}],[\"基于价值的强化学习方法会学习q\",{\"1\":{\"96\":1}}],[\"基于价值和基于策略的强化学习方法\",{\"1\":{\"96\":1}}],[\"基于策略的强化学习方法则对策略进行建模\",{\"1\":{\"96\":1}}],[\"基于模型的强化学习的特点是对环境进行建模\",{\"1\":{\"96\":1}}],[\"基于高性能的\",{\"1\":{\"64\":1}}],[\"基于svd参数化\",{\"1\":{\"41\":1}}],[\"基于svd的自适应\",{\"1\":{\"41\":1}}],[\"基于敏感性的重要性度量\",{\"1\":{\"41\":1}}],[\"基于奇异值的重要性度量\",{\"1\":{\"41\":1}}],[\"调研\",{\"1\":{\"48\":1}}],[\"什么是ntp任务\",{\"0\":{\"79\":1}}],[\"什么是\",{\"1\":{\"48\":1}}],[\"阅读笔记\",{\"1\":{\"48\":1}}],[\"阅读原文\",{\"1\":{\"8\":1}}],[\"参考\",{\"0\":{\"107\":1,\"165\":1,\"206\":1},\"1\":{\"83\":1}}],[\"参考文章\",{\"0\":{\"48\":1}}],[\"参数量计算\",{\"0\":{\"188\":1}}],[\"参数量\",{\"0\":{\"152\":1}}],[\"参数量就大大地降低了\",{\"1\":{\"40\":1}}],[\"参数为θ\",{\"1\":{\"102\":1}}],[\"参数为的θ策略接受状态s\",{\"1\":{\"102\":1}}],[\"参数高效微调\",{\"1\":{\"37\":1}}],[\"可参考图2\",{\"1\":{\"205\":1}}],[\"可解释性\",{\"1\":{\"204\":1}}],[\"可能是因为太慢了\",{\"1\":{\"129\":1}}],[\"可能要再乘一个矩阵来调整形状\",{\"1\":{\"64\":1}}],[\"可按图3\",{\"1\":{\"96\":1}}],[\"可找到办法\",{\"1\":{\"83\":1}}],[\"可学习的\",{\"1\":{\"46\":1}}],[\"可以进行循环计算\",{\"1\":{\"216\":1}}],[\"可以参考图1\",{\"1\":{\"204\":1}}],[\"可以探测出第\",{\"1\":{\"202\":1}}],[\"可以计算这些条件概率的模型的表达能力有了显著的提高\",{\"1\":{\"182\":1}}],[\"可以计算某一条轨迹τ发生的概率为轨迹τ来源于在特定的环境状态下采取特定动作的序列\",{\"1\":{\"103\":1}}],[\"可以应用于\",{\"1\":{\"137\":1}}],[\"可以应用于多个基础模型\",{\"1\":{\"137\":1}}],[\"可以直接在decoder的每一个layer内的self\",{\"1\":{\"174\":1}}],[\"可以直接应用于经过训练的模型\",{\"1\":{\"137\":1}}],[\"可以直观地理解lora的实现原理\",{\"1\":{\"40\":1}}],[\"可以被注入到任何现有的编码器\",{\"1\":{\"137\":1}}],[\"可以采样n条轨迹τ并计算每一条轨迹的值\",{\"1\":{\"103\":1}}],[\"可以使用策略π收集一批样本\",{\"1\":{\"102\":1}}],[\"可以看做隐性微调\",{\"1\":{\"164\":1}}],[\"可以看作layernorm在均值为0时的一个特例\",{\"1\":{\"153\":1}}],[\"可以看到\",{\"1\":{\"123\":1,\"124\":1,\"143\":1}}],[\"可以看到随着模型体积增大效果越来越好\",{\"1\":{\"44\":1}}],[\"可以看出\",{\"1\":{\"81\":1,\"202\":1}}],[\"可以就这个思路深入思考两个相关问题\",{\"1\":{\"81\":1}}],[\"可以举个例子来解释这种数据压缩能力\",{\"1\":{\"80\":1}}],[\"可以在所有输入\",{\"1\":{\"137\":1}}],[\"可以在这些任务上实现巨大收益\",{\"1\":{\"70\":1}}],[\"可以在一个\",{\"1\":{\"65\":1}}],[\"提高数据集的质量也无法挽救重复训练带来的过拟合\",{\"0\":{\"125\":1}}],[\"提取出来\",{\"1\":{\"82\":1}}],[\"提出要求\",{\"1\":{\"217\":1}}],[\"提出\",{\"1\":{\"46\":1,\"169\":1}}],[\"提升效果\",{\"1\":{\"46\":1}}],[\"提示\",{\"1\":{\"15\":1,\"41\":1,\"86\":1,\"202\":1,\"204\":1,\"208\":1}}],[\"提示微调\",{\"1\":{\"6\":1,\"8\":4}}],[\"提示技术\",{\"0\":{\"211\":1},\"1\":{\"4\":1},\"2\":{\"209\":1,\"212\":1,\"214\":1,\"220\":1}}],[\"映射为一个可训练的参数\",{\"1\":{\"46\":1}}],[\"方法降低相同参数模型的flops\",{\"1\":{\"126\":1}}],[\"方法输出的是动作的价值\",{\"1\":{\"111\":1}}],[\"方法直接输出下一步动作的概率\",{\"1\":{\"100\":1}}],[\"方法\",{\"1\":{\"46\":4}}],[\"方法能够将预训练的语言模型\",{\"1\":{\"37\":1}}],[\"特征\",{\"1\":{\"174\":2}}],[\"特征值的平方根\",{\"1\":{\"41\":1}}],[\"特殊之处在于它的attention\",{\"1\":{\"172\":1}}],[\"特定的动作又分别采样自智能体的动作概率分布pθ​\",{\"1\":{\"103\":1}}],[\"特点\",{\"1\":{\"46\":4}}],[\"各类提示微调对比\",{\"0\":{\"46\":1}}],[\"更新其长短时记忆\",{\"1\":{\"217\":1}}],[\"更新的幅度太小\",{\"1\":{\"196\":1}}],[\"更开放的协议\",{\"1\":{\"146\":1}}],[\"更长的上下文\",{\"1\":{\"146\":1}}],[\"更强大的性能\",{\"1\":{\"146\":1}}],[\"更便宜\",{\"1\":{\"140\":1}}],[\"更高效的推理\",{\"1\":{\"146\":1}}],[\"更高效\",{\"1\":{\"137\":1}}],[\"更大规模的数据集会缓解重复epoch对模型性能下降的影响\",{\"0\":{\"124\":1}}],[\"更加适用于小一点的模型\",{\"1\":{\"45\":1}}],[\"更重要的是功能上的区别\",{\"1\":{\"178\":1}}],[\"更重要的是\",{\"1\":{\"41\":1}}],[\"会接收上一个时间步生成的内容\",{\"1\":{\"217\":1}}],[\"会学会一个用于简单数学计算的任务回路\",{\"1\":{\"205\":1}}],[\"会发现是\",{\"1\":{\"202\":1}}],[\"会发现尽管\",{\"1\":{\"81\":1}}],[\"会影响模型的性能\",{\"1\":{\"133\":1}}],[\"会分配给不太重要的特征\",{\"1\":{\"83\":1}}],[\"会被分配给重要特征\",{\"1\":{\"83\":1}}],[\"会把输入上文中的重要信息通过\",{\"1\":{\"82\":1}}],[\"会差于微调\",{\"1\":{\"45\":1}}],[\"会预先给定模型同任务的若干示例\",{\"1\":{\"18\":1}}],[\"改进监督模型的泛化\",{\"1\":{\"74\":1}}],[\"改为一个\",{\"1\":{\"65\":1}}],[\"改变量偏小使得效果有时候不太稳定\",{\"1\":{\"45\":1}}],[\"改动较大\",{\"1\":{\"43\":1}}],[\"也有相对应的传递关系\",{\"1\":{\"202\":1}}],[\"也有把单词masked之后用来判断是什么单词的判别式目标\",{\"1\":{\"128\":1}}],[\"也是越\",{\"1\":{\"196\":1}}],[\"也是强化学习模型推断时使用的策略\",{\"1\":{\"114\":1}}],[\"也是最关键的词汇\",{\"1\":{\"82\":1}}],[\"也为避免方差过大\",{\"1\":{\"104\":1}}],[\"也就不需要实际执行动作收集这些数据\",{\"1\":{\"96\":1}}],[\"也就是针对单个样本的不同特征做操作\",{\"1\":{\"185\":1}}],[\"也就是针对不同样本的同一特征做操作\",{\"1\":{\"185\":1}}],[\"也就是只有接受encoder输出的cross\",{\"1\":{\"175\":1}}],[\"也就是通过价值选动作\",{\"1\":{\"111\":1}}],[\"也就是说\",{\"1\":{\"83\":1,\"182\":1}}],[\"也就是说只会被特定输入模式激活\",{\"1\":{\"83\":1}}],[\"也就是最后一个位置的\",{\"1\":{\"82\":1}}],[\"也就是\",{\"1\":{\"82\":1}}],[\"也就是先用一个linear层a\",{\"1\":{\"40\":1}}],[\"也就是在一个batch里同时训练同一个任务的不同prompt\",{\"1\":{\"44\":1}}],[\"也就是在\",{\"1\":{\"8\":1}}],[\"也会对多个输入知识点产生响应\",{\"1\":{\"83\":1}}],[\"也会存储某种知识\",{\"1\":{\"82\":1}}],[\"也相同\",{\"1\":{\"65\":1}}],[\"也在transformer上的embedding输入每一层进行微调\",{\"1\":{\"45\":1}}],[\"进一步提升效果\",{\"1\":{\"45\":1}}],[\"进行merge操作\",{\"1\":{\"231\":1}}],[\"进行长篇小说创作成为了可能\",{\"1\":{\"215\":1}}],[\"进行concate然后计算self\",{\"1\":{\"174\":1}}],[\"进行编码\",{\"1\":{\"140\":1}}],[\"进行无缝集成\",{\"1\":{\"136\":1}}],[\"进行优化\",{\"1\":{\"96\":1}}],[\"进行数据压缩\",{\"0\":{\"80\":1}}],[\"进行合并\",{\"1\":{\"8\":1}}],[\"进行了multi\",{\"1\":{\"187\":1}}],[\"进行了合并\",{\"1\":{\"7\":1}}],[\"进行了简要介绍\",{\"1\":{\"6\":1}}],[\"进行\",{\"1\":{\"7\":1}}],[\"进行改写\",{\"1\":{\"7\":1,\"8\":1}}],[\"替换为可训练的嵌入\",{\"1\":{\"45\":1}}],[\"拼接到数据上作为输入\",{\"1\":{\"44\":1}}],[\"之后相应地在地短期记忆库中去去除无用的信息并增添新的信息\",{\"1\":{\"217\":1}}],[\"之后单独询问\",{\"1\":{\"164\":1}}],[\"之后\",{\"1\":{\"82\":1}}],[\"之后拼接\",{\"1\":{\"44\":1}}],[\"之前内容的语义集成到\",{\"1\":{\"203\":1}}],[\"之前的工作也观察到了这种辅助目标的改进性能\",{\"1\":{\"74\":1}}],[\"之前加入prefix\",{\"1\":{\"46\":1}}],[\"之间\",{\"1\":{\"8\":1}}],[\"加速收敛\",{\"1\":{\"74\":1}}],[\"加了个更大的mlp\",{\"1\":{\"43\":1}}],[\"加入了结构化数据做辅助\",{\"1\":{\"8\":1}}],[\"毕竟prompt的出现就是要解决大模型少样本的适配\",{\"1\":{\"43\":1}}],[\"精调起来效率低\",{\"1\":{\"43\":1}}],[\"原始的多头注意力\",{\"1\":{\"156\":1}}],[\"原始实现中\",{\"1\":{\"65\":1}}],[\"原文链接\",{\"1\":{\"143\":1}}],[\"原来有一个参数θ\",{\"1\":{\"103\":1}}],[\"原理\",{\"1\":{\"65\":2}}],[\"原论文仅在以下任务中进行了比较\",{\"1\":{\"43\":1}}],[\"原因可能是\",{\"1\":{\"19\":1}}],[\"我们需要知道\",{\"1\":{\"229\":1}}],[\"我们用x来替代zy\",{\"1\":{\"228\":1}}],[\"我们这里只看两个字符的频率\",{\"1\":{\"228\":1}}],[\"我们会发现这里的aa出现的词频最高\",{\"1\":{\"228\":1}}],[\"我们会面临token训练完的危机\",{\"1\":{\"133\":1}}],[\"我们还展示了使用recurrentgpt创建个性化交互式小说的可能性\",{\"1\":{\"218\":1}}],[\"我们还展示了使用recurrentgpt作为与消费者直接交互的交互式小说的可能性\",{\"1\":{\"218\":1}}],[\"我们在此基础上可以重新看待任务回路的形成\",{\"1\":{\"205\":1}}],[\"我们现在既需要一个kl散度来约束θ和θ\",{\"1\":{\"196\":1}}],[\"我们希望这个值正负参半\",{\"1\":{\"194\":1}}],[\"我们希望将这三个动作的概率以及对数概率都拉高\",{\"1\":{\"104\":1}}],[\"我们要优化的rθ​函数的实际意义是奖励关于完整路径τ的数学期望\",{\"1\":{\"194\":1}}],[\"我们要确保当前的策略参数不会偏离旧策略参数太远\",{\"1\":{\"191\":1}}],[\"我们再来看看实际做attention时做的运算\",{\"1\":{\"155\":1}}],[\"我们再深入到底层gpu运算\",{\"1\":{\"155\":1}}],[\"我们观察到较大的模型在token危机条件下更容易过度拟合\",{\"1\":{\"123\":1}}],[\"我们随机选择了\",{\"1\":{\"123\":1}}],[\"我们的新优化目标和之前一样\",{\"1\":{\"196\":1}}],[\"我们的下一个调查围绕着使用重复数据训练\",{\"1\":{\"123\":1}}],[\"我们的模型将把全球所有数据集的token都训练完成\",{\"1\":{\"120\":1}}],[\"我们很可能陷入缺少token训练的地步\",{\"1\":{\"120\":1}}],[\"我们称这种生成模型的用法为\",{\"1\":{\"218\":1}}],[\"我们称q\",{\"1\":{\"114\":1}}],[\"我们称sarsa是on\",{\"1\":{\"114\":1}}],[\"我们引入两个重要的量\",{\"1\":{\"95\":1}}],[\"我们可以对θ\",{\"1\":{\"195\":1}}],[\"我们可以根据模型的压缩效率来评估模型的智能程度\",{\"1\":{\"81\":1}}],[\"我们可以只消耗θ这部分的资源\",{\"1\":{\"40\":1}}],[\"我们只需要存储一个大型transformer和已知任务特定前缀的副本\",{\"1\":{\"43\":1}}],[\"与此不同\",{\"1\":{\"156\":1}}],[\"与之相对的目标策略是我们优化的对象\",{\"1\":{\"114\":1}}],[\"与监督学习不同的是\",{\"1\":{\"93\":1}}],[\"与以前的方法相比\",{\"1\":{\"70\":1}}],[\"与真正的token不对应\",{\"1\":{\"43\":1}}],[\"与提示\",{\"1\":{\"43\":1}}],[\"只能通过词汇表上的字节或字节串来\",{\"1\":{\"233\":1}}],[\"只保留nlg生成任务\",{\"1\":{\"149\":1}}],[\"只保留每个\",{\"1\":{\"139\":1}}],[\"只是类似鹦鹉学舌的语言片段缝合怪而已\",{\"1\":{\"77\":1}}],[\"只是利用多层感知编码prefix\",{\"1\":{\"43\":1}}],[\"只有特征\",{\"1\":{\"176\":1,\"177\":1}}],[\"只有lora与adalora的效果接近全参数微调\",{\"1\":{\"47\":1}}],[\"只有prefix部分的参数进行更新\",{\"1\":{\"43\":1}}],[\"只对下游任务的输入添加额外的\",{\"1\":{\"46\":1}}],[\"只需要为每个任务存储前缀\",{\"1\":{\"43\":1}}],[\"注\",{\"1\":{\"227\":1}}],[\"注意力回路示意图\",{\"1\":{\"204\":1,\"205\":1}}],[\"注意力层面临的主要问题是中间结果p\",{\"1\":{\"155\":1}}],[\"注意多层感知机就是prefix的编码器\",{\"1\":{\"43\":1}}],[\"注册会计师考试\",{\"1\":{\"27\":1}}],[\"又发现ab出现的频率最高\",{\"1\":{\"228\":1}}],[\"又称\",{\"1\":{\"226\":1}}],[\"又称为\",{\"1\":{\"42\":2}}],[\"又包含了一个描述θ和θ\",{\"1\":{\"196\":1}}],[\"又不能像trpo算法那样将kl散度作为外在约束难以融入到梯度更新的操作中\",{\"1\":{\"196\":1}}],[\"又有特征\",{\"1\":{\"175\":1}}],[\"又因为对数概率是一个概率\",{\"1\":{\"104\":1}}],[\"又叫做软提示\",{\"1\":{\"42\":1}}],[\"又叫做硬提示\",{\"1\":{\"42\":1}}],[\"离散prompt是一个实际的文本字符串\",{\"1\":{\"42\":1}}],[\"q和v的关系\",{\"1\":{\"95\":1}}],[\"qπ​\",{\"1\":{\"95\":2}}],[\"qk\",{\"1\":{\"64\":1}}],[\"qkv\",{\"1\":{\"61\":1}}],[\"q\",{\"0\":{\"113\":1},\"1\":{\"64\":1,\"113\":3,\"156\":1,\"186\":1}}],[\"quoc\",{\"1\":{\"208\":1}}],[\"quantizedlinear\",{\"1\":{\"151\":4}}],[\"quality\",{\"1\":{\"7\":1}}],[\"query\",{\"0\":{\"156\":1},\"1\":{\"44\":1,\"146\":1,\"151\":2,\"152\":2,\"156\":5,\"203\":2}}],[\"q的等级\",{\"1\":{\"41\":1}}],[\"vdb\",{\"1\":{\"216\":1}}],[\"vector\",{\"1\":{\"216\":1}}],[\"v的三个线性变换\",{\"1\":{\"187\":1}}],[\"v加入self\",{\"1\":{\"187\":1}}],[\"via\",{\"1\":{\"219\":1}}],[\"view\",{\"1\":{\"187\":2}}],[\"vincent\",{\"1\":{\"165\":1}}],[\"v在最后一个维度平等的拆分\",{\"1\":{\"186\":1}}],[\"v是三个矩阵分别与输入x做矩阵乘法的结果\",{\"1\":{\"186\":1}}],[\"v分别复制头\",{\"1\":{\"156\":1}}],[\"v分别拆分成多头\",{\"1\":{\"156\":1}}],[\"vt\",{\"1\":{\"137\":1}}],[\"variengien\",{\"1\":{\"206\":2}}],[\"vanilla\",{\"1\":{\"137\":1}}],[\"values\",{\"1\":{\"164\":1,\"174\":1}}],[\"value\",{\"1\":{\"111\":1,\"151\":2,\"152\":2,\"156\":4,\"203\":1},\"2\":{\"116\":1}}],[\"valid\",{\"1\":{\"63\":1,\"65\":2}}],[\"vπ​\",{\"1\":{\"95\":2}}],[\"v1\",{\"1\":{\"45\":1}}],[\"v1将自然语言提示的token\",{\"1\":{\"45\":1}}],[\"v\",{\"1\":{\"41\":1,\"64\":1,\"156\":3,\"186\":2,\"208\":1}}],[\"v2因为每层插入了token\",{\"1\":{\"45\":1}}],[\"v2则不只是针对embedding层\",{\"1\":{\"45\":1}}],[\"v2简单来说其实是soft\",{\"1\":{\"45\":1}}],[\"v2将prefix\",{\"1\":{\"45\":1}}],[\"v2提升小模型上的prompt\",{\"1\":{\"45\":1}}],[\"v2\",{\"1\":{\"7\":1,\"39\":1,\"46\":2,\"48\":1}}],[\"右图为\",{\"1\":{\"44\":1}}],[\"右奇异向量\",{\"1\":{\"41\":1}}],[\"右侧看起来像是左侧原有矩阵w的分解\",{\"1\":{\"40\":1}}],[\"迭代地将svd应用于大量高维权重矩阵会变得非常昂贵\",{\"1\":{\"41\":1}}],[\"然而θ和θ\",{\"1\":{\"195\":1}}],[\"然而\",{\"1\":{\"41\":1,\"61\":1,\"118\":1,\"125\":1,\"138\":1}}],[\"然后依次用另一个字符替换频率最高的一对字符\",{\"1\":{\"226\":1}}],[\"然后依靠采样得到的数据更新策略\",{\"1\":{\"96\":1}}],[\"然后简单地描述一下要生成的内容的背景设定和大纲\",{\"1\":{\"217\":1}}],[\"然后马上会切分到三个768列的矩阵然后分别作为q\",{\"1\":{\"187\":1}}],[\"然后平行地经过self\",{\"1\":{\"186\":1}}],[\"然后将语料中所有该字符对融合\",{\"1\":{\"227\":1}}],[\"然后将这些元梯度应用于原始gpt以构建icl模型\",{\"1\":{\"164\":1}}],[\"然后将其送到添加的具有参数的线性输出层来以预测\",{\"1\":{\"74\":1}}],[\"然后是text\",{\"1\":{\"161\":1}}],[\"然后是position\",{\"1\":{\"73\":1}}],[\"然后介绍了chatgpt模型最重要的技术指令微调\",{\"1\":{\"160\":1}}],[\"然后根据这些块来计算注意力输出\",{\"1\":{\"155\":1}}],[\"然后看模型在不同规模数据集上重复训练的性能影响\",{\"1\":{\"124\":1}}],[\"然后只是用了其中一部分数据集\",{\"1\":{\"123\":1}}],[\"然后使用梯度下降算法学习这些样本\",{\"1\":{\"102\":1}}],[\"然后在每个特定任务上进行歧视性微调\",{\"1\":{\"70\":1}}],[\"然后在每个transformer块里注入可训练的层\",{\"1\":{\"40\":1}}],[\"然后通过\",{\"1\":{\"65\":1}}],[\"然后训练的时候只更新prefix部分的参数\",{\"1\":{\"43\":1}}],[\"然后截断最小的奇异值\",{\"1\":{\"41\":1}}],[\"然后\",{\"1\":{\"41\":1,\"137\":1,\"140\":1}}],[\"k和v矩阵划分成块\",{\"1\":{\"155\":1}}],[\"kelvin\",{\"1\":{\"165\":1}}],[\"keys\",{\"1\":{\"164\":1}}],[\"key\",{\"1\":{\"151\":2,\"152\":2,\"156\":7,\"174\":1,\"203\":3}}],[\"kernel\",{\"1\":{\"61\":1,\"64\":4,\"65\":2}}],[\"knn\",{\"1\":{\"138\":1,\"140\":1}}],[\"knowledge\",{\"1\":{\"8\":1}}],[\"kociskýet\",{\"1\":{\"137\":1}}],[\"koltchinskii等人\",{\"1\":{\"41\":1}}],[\"krys\",{\"1\":{\"137\":1}}],[\"klmax​\",{\"1\":{\"196\":1}}],[\"klmin​\",{\"1\":{\"196\":1}}],[\"kl惩罚的优势在于\",{\"1\":{\"196\":1}}],[\"kl惩罚\",{\"0\":{\"196\":1}}],[\"kl散度也越高\",{\"1\":{\"195\":1}}],[\"kl散度可以计算两个分布的不相似度\",{\"1\":{\"195\":1}}],[\"kl散度的外在约束\",{\"0\":{\"195\":1}}],[\"kl\",{\"1\":{\"105\":1,\"195\":2,\"196\":1}}],[\"k\",{\"1\":{\"64\":1,\"137\":1,\"138\":1,\"140\":4,\"156\":5,\"186\":3,\"187\":2,\"206\":1}}],[\"k个\",{\"1\":{\"46\":1}}],[\"根据这些内容生成一段内容\",{\"1\":{\"217\":1}}],[\"根据常见的上下文窗口长度绘制了几个流行的摘要和问答数据集的大小\",{\"1\":{\"137\":1}}],[\"根据前面的实验我们知道\",{\"1\":{\"133\":1}}],[\"根据状态图可以理解sarsa的更新规则\",{\"1\":{\"112\":1}}],[\"根据状态执行动作由模型决定\",{\"1\":{\"94\":1}}],[\"根据公式我们知道sarsa是通过预估下一步的收益来更新自身的q值\",{\"1\":{\"114\":1}}],[\"根据公式\",{\"1\":{\"104\":1}}],[\"根据按照蒙特卡洛方法近似求期望的原则\",{\"1\":{\"103\":1}}],[\"根据概率来选取动作\",{\"1\":{\"100\":1}}],[\"根据输出的下一个token的概率分布进行算术编码\",{\"1\":{\"80\":1}}],[\"根据已有的token\",{\"1\":{\"80\":1}}],[\"根据\",{\"1\":{\"63\":1}}],[\"根据结果可以看出\",{\"1\":{\"47\":1}}],[\"根据新的重要性指标\",{\"1\":{\"41\":1}}],[\"根据论文的研究结果分析\",{\"1\":{\"40\":1}}],[\"奇异值\",{\"1\":{\"41\":1}}],[\"它的词汇表由256个单字节符号+50000个merge词+1个<|endoftext|>组成\",{\"1\":{\"229\":1}}],[\"它的主要作用在于当\",{\"1\":{\"203\":1}}],[\"它的信息在顺着\",{\"1\":{\"82\":1}}],[\"它不使用kl散度来描述两种分布的不相似度\",{\"1\":{\"197\":1}}],[\"它不仅应该对输入进行调节\",{\"1\":{\"182\":1}}],[\"它告诉我们只要以奖励的期望式1\",{\"1\":{\"192\":1}}],[\"它对标准的策略梯度方法做了改进\",{\"1\":{\"191\":1}}],[\"它应该建模为p\",{\"1\":{\"182\":1}}],[\"它能够将卷积与自注意力的优点通过\",{\"1\":{\"136\":1}}],[\"它本身带有随机性\",{\"1\":{\"96\":1}}],[\"它本身没有随机性质\",{\"1\":{\"96\":1}}],[\"它是将状态空间s映射到动作空间a的函数\",{\"1\":{\"96\":1}}],[\"它们的kl散度值为0\",{\"1\":{\"195\":1}}],[\"它们都在2022年11月发布\",{\"1\":{\"161\":1}}],[\"它们各自只对输入里某个特殊的知识点产生响应\",{\"1\":{\"83\":1}}],[\"它们大多直接计算矩阵的奇异值分解\",{\"1\":{\"41\":1}}],[\"它证明了\",{\"1\":{\"82\":1}}],[\"它在低层已经集成了单词\",{\"1\":{\"82\":1}}],[\"它实现了变长输入的\",{\"1\":{\"61\":1}}],[\"它根据不同的模型结构定义了不同的prompt拼接方式\",{\"1\":{\"43\":1}}],[\"它根据我们新设计的重要性度量修剪冗余奇异值\",{\"1\":{\"41\":1}}],[\"它考虑了gi中每个条目对模型性能的贡献\",{\"1\":{\"41\":1}}],[\"它以奇异值分解的形式表示增量矩阵∆\",{\"1\":{\"41\":1}}],[\"它由\",{\"1\":{\"26\":1}}],[\"以模拟记忆的更新\",{\"1\":{\"217\":1}}],[\"以模拟svd\",{\"1\":{\"41\":1}}],[\"以此增加等号后数字的\",{\"1\":{\"205\":1}}],[\"以此方式来通过\",{\"1\":{\"201\":1}}],[\"以决定输出\",{\"1\":{\"205\":1}}],[\"以及对接下来生成内容的规划\",{\"1\":{\"217\":1}}],[\"以及一个对下一步生成内容的梗概\",{\"1\":{\"217\":1}}],[\"以及多步中间推理\",{\"1\":{\"208\":1}}],[\"以及p\",{\"1\":{\"182\":1}}],[\"以及反向移动这个瓶颈\",{\"1\":{\"155\":1}}],[\"以使模型具备人类倾向的回答问题能力\",{\"1\":{\"162\":1}}],[\"以上的注意力性能\",{\"1\":{\"140\":1}}],[\"以上描述的过程是对称量化\",{\"1\":{\"53\":1}}],[\"以确保编码过程前后都有足够的上下文\",{\"1\":{\"139\":1}}],[\"以进行进一步改进\",{\"1\":{\"137\":1}}],[\"以进一步提高性能\",{\"1\":{\"61\":1}}],[\"以在测试时接受无限长度的输入\",{\"1\":{\"137\":1}}],[\"以在目标token上产生输出分布\",{\"1\":{\"73\":1}}],[\"以取得最大化的预期利益\",{\"1\":{\"93\":1}}],[\"以获得最终transformer块的激活\",{\"1\":{\"74\":1}}],[\"以\",{\"1\":{\"64\":1}}],[\"以奇异值分解的形式对权重矩阵的增量更新进行参数化\",{\"1\":{\"41\":1}}],[\"以防止过度拟合并节省计算预算\",{\"1\":{\"41\":1}}],[\"以控制其预算\",{\"1\":{\"41\":1}}],[\"以提高参数高效微调的性能\",{\"1\":{\"41\":1}}],[\"具备类人智能\",{\"1\":{\"77\":1}}],[\"具有能接受encoder输出的cross\",{\"1\":{\"174\":1}}],[\"具有高度重要性的三元组会被保留\",{\"1\":{\"41\":1}}],[\"具有低重要性分数的三元组被授予低优先级\",{\"1\":{\"41\":1}}],[\"具有挑战性的人类的考试题中构建了\",{\"1\":{\"27\":1}}],[\"具体的\",{\"1\":{\"155\":1}}],[\"具体例子可以参照下图\",{\"1\":{\"82\":1}}],[\"具体提取动作是通过某个\",{\"1\":{\"82\":1}}],[\"具体是在计算时对注意力做一些变形\",{\"1\":{\"64\":1}}],[\"具体来说\",{\"1\":{\"41\":1}}],[\"具体而言就是已知p\",{\"1\":{\"96\":1}}],[\"具体而言\",{\"1\":{\"41\":1,\"74\":1,\"123\":1,\"202\":1}}],[\"自己对应\",{\"1\":{\"203\":1}}],[\"自回归的意思是指\",{\"1\":{\"181\":1}}],[\"自bos直到eos是另一部分\",{\"1\":{\"172\":1}}],[\"自开始直到gmask是一部分\",{\"1\":{\"172\":1}}],[\"自然就短得多\",{\"1\":{\"81\":1}}],[\"自然语言提示本身十分脆弱\",{\"1\":{\"45\":1}}],[\"自动化地寻找连续空间中的知识模板\",{\"1\":{\"46\":1}}],[\"自适应的低秩自适应\",{\"1\":{\"41\":1}}],[\"自主生成的\",{\"1\":{\"7\":1}}],[\"自主生成\",{\"1\":{\"7\":1}}],[\"甚至会损害模型性能\",{\"1\":{\"41\":1}}],[\"甚至超过了在\",{\"1\":{\"7\":1}}],[\"因而目前它是最流行的方法\",{\"1\":{\"226\":1}}],[\"因为bpe算法训练tokenizer的语料库以英文语料库为主\",{\"1\":{\"233\":1}}],[\"因为用户可以观察和编辑自然语言记忆\",{\"1\":{\"217\":1}}],[\"因为当前的gpt模型只能生成有限长度的文本\",{\"1\":{\"216\":1}}],[\"因为它把\",{\"1\":{\"195\":1}}],[\"因为这样就可以衡量策略是好还是坏\",{\"1\":{\"194\":1}}],[\"因为原生注意力机制具有平方级的复杂度\",{\"1\":{\"137\":1}}],[\"因为在它的最上层会给出\",{\"1\":{\"82\":1}}],[\"因为在前向计算的时候\",{\"1\":{\"40\":1}}],[\"因为从线程角度看\",{\"1\":{\"65\":1}}],[\"因为共享内存大小限制\",{\"1\":{\"64\":1}}],[\"因为可以把\",{\"1\":{\"64\":1}}],[\"因为大模型参数量大\",{\"1\":{\"43\":1}}],[\"因此官方制定了一条限制\",{\"1\":{\"230\":1}}],[\"因此recurrentgpt是可解释的\",{\"1\":{\"217\":2}}],[\"因此考虑将kl散度加入到优化目标式3\",{\"1\":{\"196\":1}}],[\"因此必须有一个约束\",{\"1\":{\"195\":1}}],[\"因此ln可以不受样本数的限制\",{\"1\":{\"185\":1}}],[\"因此通常将符号上的联合概率分解为条件概率的乘积\",{\"1\":{\"182\":1}}],[\"因此通常会结合ϵ贪心算法或向动作值中加入高斯噪声的方法来增加策略的随机性\",{\"1\":{\"96\":1}}],[\"因此偏向于decoder自然语言生成的功能\",{\"1\":{\"172\":1}}],[\"因此偏向于encoder自然语言理解的功能\",{\"1\":{\"172\":1}}],[\"因此每生成一个词元\",{\"1\":{\"156\":1}}],[\"因此不再包含mask\",{\"1\":{\"149\":1}}],[\"因此用moe去提前预估大模型的性能\",{\"1\":{\"132\":1}}],[\"因此非对称量化的w\",{\"1\":{\"53\":1}}],[\"因此对显存来说相当于多存了t的对角元素\",{\"1\":{\"52\":1}}],[\"因此t完全由w决定\",{\"1\":{\"52\":1}}],[\"因此作者在每层都加了prompt的参数\",{\"1\":{\"43\":1}}],[\"因此奇异值被清零\",{\"1\":{\"41\":1}}],[\"因此论文提出了以下问题\",{\"1\":{\"41\":1}}],[\"因此\",{\"1\":{\"17\":1,\"41\":1,\"43\":2,\"74\":1,\"123\":2,\"129\":1,\"130\":1,\"138\":1,\"155\":2,\"187\":1}}],[\"梯度计算量少了很多\",{\"1\":{\"40\":1}}],[\"多项研究证明这个回路的存在\",{\"1\":{\"203\":1}}],[\"多次更新θ\",{\"1\":{\"193\":1}}],[\"多个只有self\",{\"1\":{\"173\":1}}],[\"多样的训练目标可以减轻多epoch下降吗\",{\"0\":{\"128\":1}}],[\"多轮epoch的训练会降低模型性能\",{\"0\":{\"123\":1}}],[\"多的也都是个位数\",{\"1\":{\"120\":1}}],[\"多语义神经元和知识点之间的关系是多对多的映射\",{\"1\":{\"83\":1}}],[\"多语义神经元\",{\"1\":{\"83\":8}}],[\"多头注意力\",{\"1\":{\"64\":1}}],[\"多了δ\",{\"1\":{\"40\":1}}],[\"多主题的知识评估数据集\",{\"1\":{\"15\":1}}],[\"​pθ​​的范围来约束θ和θ\",{\"1\":{\"197\":1}}],[\"​a\",{\"1\":{\"194\":1,\"195\":1,\"196\":1,\"197\":1}}],[\"​aθ\",{\"1\":{\"105\":1}}],[\"​r\",{\"1\":{\"192\":1,\"193\":3}}],[\"​γ+β\",{\"1\":{\"185\":1}}],[\"​​\",{\"1\":{\"105\":1}}],[\"​=p\",{\"1\":{\"103\":1}}],[\"​\",{\"1\":{\"40\":1,\"103\":2,\"105\":2,\"192\":2,\"193\":8,\"194\":3,\"195\":3,\"196\":3,\"197\":3}}],[\"θ←θ+η∇rθ​\",{\"1\":{\"192\":1}}],[\"θ\",{\"1\":{\"40\":4,\"105\":3,\"195\":2,\"196\":2}}],[\"θmax​\",{\"1\":{\"40\":1}}],[\"表3\",{\"1\":{\"163\":1}}],[\"表现更好\",{\"1\":{\"137\":1}}],[\"表现更差\",{\"1\":{\"123\":1}}],[\"表现出了良好的效果\",{\"1\":{\"7\":1}}],[\"表明较小的模型已收到足够的token\",{\"1\":{\"122\":1}}],[\"表示\",{\"1\":{\"40\":1,\"95\":1}}],[\"φ部分还是需要参与计算的\",{\"1\":{\"40\":1}}],[\"φ\",{\"1\":{\"40\":4}}],[\"φmax​\",{\"1\":{\"40\":1}}],[\"y来代替ab\",{\"1\":{\"228\":1}}],[\"yy\",{\"1\":{\"202\":3}}],[\"year\",{\"1\":{\"202\":2}}],[\"you\",{\"1\":{\"169\":1}}],[\"yaru\",{\"1\":{\"165\":1}}],[\"yutao\",{\"1\":{\"165\":1}}],[\"yu\",{\"1\":{\"165\":1}}],[\"yun\",{\"1\":{\"41\":1}}],[\"y=ab\",{\"1\":{\"228\":2}}],[\"y=var\",{\"1\":{\"185\":1}}],[\"y=tw\",{\"1\":{\"52\":1}}],[\"y=wx+b\",{\"1\":{\"52\":1}}],[\"y为输出\",{\"1\":{\"45\":1}}],[\"y<t​\",{\"1\":{\"40\":2}}],[\"yt​∣x\",{\"1\":{\"40\":2}}],[\"y\",{\"1\":{\"40\":2,\"43\":4,\"45\":1,\"52\":2,\"165\":1,\"185\":2,\"187\":4}}],[\"yizhongw\",{\"1\":{\"7\":2,\"8\":1}}],[\"x42\",{\"1\":{\"232\":2}}],[\"x84\",{\"1\":{\"232\":2}}],[\"x93\",{\"1\":{\"232\":2}}],[\"xe6\",{\"1\":{\"232\":2}}],[\"x65\",{\"1\":{\"232\":2}}],[\"xa1\",{\"1\":{\"232\":2}}],[\"xc2高四位是c\",{\"1\":{\"232\":1}}],[\"xc2\",{\"1\":{\"232\":2}}],[\"x=zy\",{\"1\":{\"228\":1}}],[\"xdxac\",{\"1\":{\"228\":1}}],[\"xia\",{\"1\":{\"208\":1}}],[\"xiao\",{\"1\":{\"137\":1}}],[\"xx\",{\"1\":{\"202\":1}}],[\"xn​\",{\"1\":{\"182\":1}}],[\"x2​\",{\"1\":{\"182\":1}}],[\"x1​\",{\"1\":{\"182\":1}}],[\"xu等\",{\"1\":{\"219\":1}}],[\"xuezhi\",{\"1\":{\"208\":1}}],[\"xu\",{\"1\":{\"165\":1}}],[\"xsum\",{\"1\":{\"137\":1}}],[\"xl\",{\"1\":{\"123\":1}}],[\"x+b\",{\"1\":{\"52\":1}}],[\"x为输入\",{\"1\":{\"45\":1}}],[\"x\",{\"1\":{\"40\":2,\"43\":4,\"45\":3,\"52\":1,\"65\":3,\"151\":2,\"182\":2,\"185\":5,\"187\":10}}],[\"xmtf\",{\"1\":{\"8\":2}}],[\"从下面算法流程就能明白原因了\",{\"1\":{\"229\":1}}],[\"从\",{\"1\":{\"205\":1}}],[\"从上图例子可看出\",{\"1\":{\"204\":1}}],[\"从策略梯度算法到近端策略优化算法\",{\"0\":{\"191\":1}}],[\"从代码来看\",{\"1\":{\"187\":1}}],[\"从此它就开始不断进化\",{\"1\":{\"161\":1}}],[\"从名称我们可以看出其学习更新函数依赖的5个值\",{\"1\":{\"112\":1}}],[\"从第二个式子可以看到\",{\"1\":{\"40\":1}}],[\"从论文中的公式来看\",{\"1\":{\"40\":1}}],[\"从而保持短期记忆不会因为迭代的轮数增加而变得过长\",{\"1\":{\"217\":1}}],[\"从而实现任意长度文本的生成\",{\"1\":{\"217\":1}}],[\"从而实现生成任意长度的文本\",{\"1\":{\"217\":2}}],[\"从而形成了一个由低向上逐层激发\",{\"1\":{\"205\":1}}],[\"从而使gpt产生的答案更偏向于标注人员的喜好\",{\"1\":{\"163\":1}}],[\"从而大大减少key和value矩阵的参数量\",{\"1\":{\"156\":1}}],[\"从而大大降低了计算和存储成本\",{\"1\":{\"37\":1,\"38\":1}}],[\"从而达到自主决策的目的\",{\"1\":{\"94\":1}}],[\"从而将参数量从\",{\"1\":{\"40\":1}}],[\"而没有空格则代表了cat作为英文单词的中间片段或者后缀\",{\"1\":{\"233\":1}}],[\"而解码的过程则就是编码的反向过程\",{\"1\":{\"229\":1}}],[\"而不遗忘过去的信息\",{\"1\":{\"217\":1}}],[\"而不是在每一轮的迭代中\",{\"1\":{\"217\":1}}],[\"而不是比较谁更好\",{\"1\":{\"194\":1}}],[\"而不是截断关键字\",{\"1\":{\"140\":1}}],[\"而缺乏长文本生成的能力\",{\"1\":{\"216\":1}}],[\"而上文内容中出现的\",{\"1\":{\"203\":1}}],[\"而左图则展示了\",{\"1\":{\"202\":1}}],[\"而第\",{\"1\":{\"202\":1}}],[\"而第二部分只是单向特性\",{\"1\":{\"172\":1}}],[\"而后半部分\",{\"1\":{\"196\":1}}],[\"而multi\",{\"1\":{\"186\":1}}],[\"而右边的不会\",{\"1\":{\"172\":1}}],[\"而在于它的功能设计\",{\"1\":{\"172\":1}}],[\"而在调试的过程中gpt3\",{\"1\":{\"163\":1}}],[\"而采用正则技术虽然会影响模型训练效率\",{\"1\":{\"133\":1}}],[\"而前面说的galactica训练使用了\",{\"1\":{\"129\":1}}],[\"而多epoch的负面影响也都是过拟合\",{\"1\":{\"129\":1}}],[\"而229token的数据集上重复训练\",{\"1\":{\"124\":1}}],[\"而2023年全球的token估计只有9万亿\",{\"1\":{\"120\":1}}],[\"而taylor在训练galactica模型时候发现epoch次数达到4次也可以提升模型效果\",{\"1\":{\"120\":1}}],[\"而transformer中的其他部分参数固定\",{\"1\":{\"43\":1}}],[\"而q来自于自己的self\",{\"1\":{\"170\":1}}],[\"而q\",{\"1\":{\"114\":1}}],[\"而\",{\"1\":{\"103\":1,\"204\":1}}],[\"而特定的状态\",{\"1\":{\"103\":1}}],[\"而具有马尔可夫性质的随机过程便是马尔可夫过程\",{\"1\":{\"95\":1}}],[\"而一个\",{\"1\":{\"83\":1}}],[\"而模型若能给出越短的描述\",{\"1\":{\"81\":1}}],[\"而零填充会引入大量的额外计算开销\",{\"1\":{\"61\":1}}],[\"而非对称量化是把每一行的最大值变换到127\",{\"1\":{\"53\":1}}],[\"而保留了w\",{\"1\":{\"52\":1}}],[\"而p\",{\"1\":{\"46\":1}}],[\"而prefix则是可以学习的\",{\"1\":{\"43\":1}}],[\"而且\",{\"1\":{\"202\":1}}],[\"而且更加注重生成答案的无害性和对话性\",{\"1\":{\"163\":1}}],[\"而且本文还发现\",{\"1\":{\"137\":1}}],[\"而且下一步是按照行为策略选出的\",{\"1\":{\"114\":1}}],[\"而且这篇文章证明了\",{\"1\":{\"82\":1}}],[\"而且学到了人类语言甚至包括物理世界的内在运行规律\",{\"1\":{\"77\":1}}],[\"而且冻结模型所有参数去学习插入token\",{\"1\":{\"45\":1}}],[\"而且从优化角度无法达到最优\",{\"1\":{\"45\":1}}],[\"而自动化生成prompt又分为离散提示\",{\"1\":{\"42\":1}}],[\"而正交矩阵p和q表示∆的左\",{\"1\":{\"41\":1}}],[\"而是使用裁剪函数clip\",{\"1\":{\"197\":1}}],[\"而是openai自定义的一个卷积层\",{\"1\":{\"187\":1}}],[\"而是逐渐引入\",{\"1\":{\"130\":1}}],[\"而是连续向量\",{\"1\":{\"46\":1}}],[\"而是将连续型token插入每一层\",{\"1\":{\"45\":1}}],[\"而是将∆参数化为∆=p∧q\",{\"1\":{\"41\":1}}],[\"而是所有出现大矩阵的地方\",{\"1\":{\"40\":1}}],[\"而θ部分是凭空增加了的参数\",{\"1\":{\"40\":1}}],[\"而根据假设\",{\"1\":{\"40\":1}}],[\"而加入了lora之后\",{\"1\":{\"40\":1}}],[\"而lora保留了原来的矩阵w\",{\"1\":{\"40\":1}}],[\"而无需微调模型的所有参数\",{\"1\":{\"37\":1,\"38\":1}}],[\"其余的词汇直接丢弃\",{\"1\":{\"227\":1}}],[\"其次\",{\"1\":{\"204\":1,\"217\":1}}],[\"其原因就是模型学会了一个主要由\",{\"1\":{\"204\":1}}],[\"其源码参数nf\",{\"1\":{\"187\":1}}],[\"其作为tensor2tensor包的一部分\",{\"1\":{\"169\":1}}],[\"其监督微调\",{\"1\":{\"162\":1}}],[\"其主要思想是将输入的q\",{\"1\":{\"155\":1}}],[\"其下游任务的效果也是更差\",{\"1\":{\"123\":1}}],[\"其定义为状态下采取动作后未来累积奖励的期望\",{\"1\":{\"95\":1}}],[\"其定义为状态s未来累积奖励的期望\",{\"1\":{\"95\":1}}],[\"其实把它们通过utf\",{\"1\":{\"233\":1}}],[\"其实对decoder\",{\"1\":{\"178\":1}}],[\"其实现的逻辑在于\",{\"1\":{\"156\":1}}],[\"其实是\",{\"1\":{\"81\":1}}],[\"其实并未具备智能\",{\"1\":{\"77\":1}}],[\"其二是跨层参数共享\",{\"1\":{\"40\":1}}],[\"其一是embedding矩阵分解\",{\"1\":{\"40\":1}}],[\"其中有空格代表一个英文单词或者是一个英文单词前缀\",{\"1\":{\"233\":1}}],[\"其中nf\",{\"1\":{\"187\":1}}],[\"其中n是残差层的数量\",{\"1\":{\"183\":1}}],[\"其中q\",{\"1\":{\"186\":1}}],[\"其中是防止分母为0的超参数\",{\"1\":{\"185\":1}}],[\"其中第一部分具有双向特性\",{\"1\":{\"172\":1}}],[\"其中encoder单层包括self\",{\"1\":{\"170\":1}}],[\"其中包含大约\",{\"1\":{\"123\":1}}],[\"其中每个实例由一系列输入token以及标签\",{\"1\":{\"74\":1}}],[\"其中每个三元组gi包含第i个奇异值和相应的奇异向量\",{\"1\":{\"41\":1}}],[\"其中u=\",{\"1\":{\"73\":1}}],[\"其中k是上下文窗口的大小\",{\"1\":{\"73\":1}}],[\"其中t是一个对角矩阵\",{\"1\":{\"52\":1}}],[\"其中\",{\"1\":{\"39\":1,\"40\":2,\"65\":1,\"155\":1}}],[\"其中四大类分别是\",{\"1\":{\"27\":1}}],[\"该算法简单有效\",{\"1\":{\"226\":1}}],[\"该模型每个时间步生成一个段落\",{\"1\":{\"217\":1}}],[\"该模型在很大程度上遵循openai\",{\"1\":{\"183\":1}}],[\"该模型在外部数据存储中执行\",{\"1\":{\"138\":1}}],[\"该模型在输入上下文token上应用multi\",{\"1\":{\"73\":1}}],[\"该文提出了一种名为循环生成式预训练变换器\",{\"1\":{\"217\":1}}],[\"该文旨在解决gpt模型生成文本长度受限的问题\",{\"1\":{\"216\":1}}],[\"该文介绍了\",{\"1\":{\"86\":1,\"208\":1}}],[\"该项研究还发现不仅仅上述\",{\"1\":{\"202\":1}}],[\"该项目包含了\",{\"1\":{\"8\":1}}],[\"该层执行了multi\",{\"1\":{\"187\":1}}],[\"该初始化考虑了模型深度在残差路径上的累积\",{\"1\":{\"183\":1}}],[\"该方案目的的是为了保证模型效果的同时加快decoder生成token的速度\",{\"1\":{\"156\":1}}],[\"该方法允许从p\",{\"1\":{\"182\":1}}],[\"该方法冻结lm参数\",{\"1\":{\"43\":1}}],[\"该方法其实和构造prompt类似\",{\"1\":{\"43\":1}}],[\"该方法是在输入token之前构造一段任务相关的virtual\",{\"1\":{\"43\":1}}],[\"该方法只需要在保持奇异向量的同时删除不重要的奇异值\",{\"1\":{\"41\":1}}],[\"该方法在类似lora的微调过程中在权重矩阵之间动态分配参数预算\",{\"1\":{\"41\":1}}],[\"该思想与albert的思想有异曲同工之处\",{\"1\":{\"40\":1}}],[\"事实上\",{\"1\":{\"40\":1,\"172\":1}}],[\"<输入><gmask><bos><输出><eos>\",{\"1\":{\"172\":1}}],[\"<bound\",{\"1\":{\"151\":1,\"183\":1}}],[\"<δ​\",{\"1\":{\"195\":1}}],[\"<δ\",{\"1\":{\"105\":1}}],[\"<\",{\"1\":{\"40\":4,\"82\":1,\"203\":1}}],[\"+ϵ​x−e\",{\"1\":{\"185\":1}}],[\"+60\",{\"1\":{\"146\":1}}],[\"+571\",{\"1\":{\"146\":1}}],[\"+33\",{\"1\":{\"146\":1}}],[\"+23\",{\"1\":{\"146\":1}}],[\"+γσs\",{\"1\":{\"95\":1}}],[\"+\",{\"1\":{\"40\":1,\"81\":2,\"187\":1}}],[\"∗\",{\"1\":{\"40\":1}}],[\"旁支\",{\"1\":{\"40\":1}}],[\"实例化一个conv1d对象\",{\"1\":{\"187\":1}}],[\"实例化一个ln层\",{\"1\":{\"185\":1}}],[\"实例化一个bn层\",{\"1\":{\"185\":1}}],[\"实体\",{\"1\":{\"82\":1}}],[\"实现\",{\"1\":{\"63\":1,\"64\":1,\"65\":1,\"66\":1}}],[\"实现了高效的特征学习\",{\"1\":{\"136\":1}}],[\"实现了\",{\"1\":{\"61\":1}}],[\"实现了端到端的推理过程的大幅优化\",{\"1\":{\"60\":1}}],[\"实际上就是一种贪心算法\",{\"1\":{\"227\":1}}],[\"实际上是增加了右侧的\",{\"1\":{\"40\":1}}],[\"实际测试下来只作用在embedding层的话交互能力会变弱\",{\"1\":{\"45\":1}}],[\"实验结论\",{\"0\":{\"121\":1}}],[\"实验结果\",{\"0\":{\"47\":1,\"141\":1}}],[\"实验结果表明\",{\"1\":{\"7\":1}}],[\"实验证实只加到embedding上的效果不太好\",{\"1\":{\"43\":1}}],[\"直到某一轮扫描\",{\"1\":{\"231\":1}}],[\"直到词表中单词数达到设定量或下一个最高频数为\",{\"1\":{\"227\":1}}],[\"直到循环次数结束\",{\"1\":{\"226\":1}}],[\"直到\",{\"1\":{\"201\":1}}],[\"直接对π\",{\"1\":{\"96\":1}}],[\"直白的来说\",{\"1\":{\"40\":1}}],[\"直译为大语言模型的低阶适应\",{\"1\":{\"40\":1}}],[\"结合\",{\"1\":{\"208\":1}}],[\"结合gpt论文给出的模型架构\",{\"1\":{\"184\":1}}],[\"结合对text\",{\"1\":{\"164\":1}}],[\"结合了微调和提示两个范式的优点\",{\"1\":{\"162\":1}}],[\"结合上图\",{\"1\":{\"40\":1}}],[\"结果中输出\",{\"1\":{\"204\":1}}],[\"结果在\",{\"1\":{\"203\":1}}],[\"结果如图\",{\"1\":{\"123\":1}}],[\"结果展示\",{\"0\":{\"31\":1}}],[\"结构里的一种信息压缩编码机制\",{\"1\":{\"83\":1}}],[\"所谓\",{\"1\":{\"201\":1,\"204\":1}}],[\"所谓强化学习\",{\"1\":{\"94\":1}}],[\"所做的任务是nlu还是nlg\",{\"1\":{\"178\":1}}],[\"所示\",{\"1\":{\"123\":1}}],[\"所以最后\",{\"1\":{\"202\":1}}],[\"所以直接基于gpt3\",{\"1\":{\"163\":1}}],[\"所以制约了长度的扩展\",{\"1\":{\"155\":1}}],[\"所以它处理起来非常困难\",{\"1\":{\"195\":1}}],[\"所以它的目标策略与行为策略保持一致\",{\"1\":{\"114\":1}}],[\"所以它是被发现存在\",{\"1\":{\"83\":1}}],[\"所以权重大的\",{\"1\":{\"104\":1}}],[\"所以仅仅通过一个多语义神经元是无法探测当前是对谁在做出响应\",{\"1\":{\"83\":1}}],[\"所以随着信息往上层流动\",{\"1\":{\"82\":1}}],[\"所以就越聪明\",{\"1\":{\"81\":1}}],[\"所以就在低资源的情况下\",{\"1\":{\"40\":1}}],[\"所以在这种情况下量化是一个必然的选择\",{\"1\":{\"51\":1}}],[\"所以理论上\",{\"1\":{\"40\":1}}],[\"所以需要计算梯度的部分就只剩下旁支的a和b两个小矩阵\",{\"1\":{\"40\":1}}],[\"所以将embedding矩阵分解成两个相对较小的矩阵\",{\"1\":{\"40\":1}}],[\"所以\",{\"1\":{\"40\":1,\"77\":1,\"81\":1,\"83\":1}}],[\"所有2\",{\"1\":{\"231\":1}}],[\"所有生成内容中和当前时间步相关程度最高的几个段落\",{\"1\":{\"217\":1}}],[\"所有的一切表明\",{\"1\":{\"133\":1}}],[\"所有参数小于\",{\"1\":{\"19\":1}}],[\"所有题目均使用准确率计算得分\",{\"1\":{\"18\":1}}],[\"冻结预训练好的模型权重参数\",{\"1\":{\"40\":1}}],[\"训练的步骤与前面所提到的bpe原始步骤基本一致\",{\"1\":{\"230\":1}}],[\"训练的时间开销集中在了数据采样\",{\"1\":{\"192\":1}}],[\"训练\",{\"0\":{\"230\":1}}],[\"训练成本很高\",{\"1\":{\"127\":1}}],[\"训练效果下降的原因是什么\",{\"1\":{\"119\":1}}],[\"训练过程可以看成是对数据的无损压缩\",{\"1\":{\"81\":1}}],[\"训练框架\",{\"0\":{\"72\":1}}],[\"训练完只保存mlp变换后的参数就行了\",{\"1\":{\"43\":1}}],[\"训练出任务对应prompt的embedding向量\",{\"1\":{\"39\":1}}],[\"训练集\",{\"1\":{\"7\":1}}],[\"谷歌\",{\"1\":{\"39\":1}}],[\"北京智源\",{\"1\":{\"39\":1}}],[\"清华\",{\"1\":{\"39\":1}}],[\"清华keg\",{\"1\":{\"39\":1}}],[\"斯坦福大学\",{\"1\":{\"219\":1}}],[\"斯坦福\",{\"1\":{\"39\":1}}],[\"微软\",{\"1\":{\"39\":2}}],[\"微调大型\",{\"1\":{\"37\":1}}],[\"微调技术\",{\"0\":{\"56\":1},\"1\":{\"4\":1},\"2\":{\"49\":1,\"54\":1,\"57\":1,\"59\":1}}],[\"同样的\",{\"1\":{\"228\":1}}],[\"同样使用该算法去除对\",{\"1\":{\"63\":1}}],[\"同样是使用\",{\"1\":{\"7\":1}}],[\"同时还提供了在线演示\",{\"1\":{\"218\":1}}],[\"同时在对短期记忆修改时作者们指示大语言模型首先分析短期记忆中哪些内容对于后续创作不再重要以及新生成的内容中哪些会对后续生成有所影响\",{\"1\":{\"217\":1}}],[\"同时有一些小的改动\",{\"1\":{\"183\":1}}],[\"同时通常还能保留\",{\"1\":{\"140\":1}}],[\"同时也无需对非最优解的精确地纠正\",{\"1\":{\"93\":1}}],[\"同时对模型架构所需的更改最小\",{\"1\":{\"70\":1}}],[\"同时利用lstm进行reparamerization加速训练\",{\"1\":{\"45\":1}}],[\"同时\",{\"1\":{\"44\":1}}],[\"同时freeze预训练模型进行训练\",{\"1\":{\"44\":1}}],[\"同时是hugging\",{\"1\":{\"38\":1}}],[\"技术\",{\"1\":{\"38\":1}}],[\"额外添加一个或多个\",{\"1\":{\"44\":1}}],[\"额外\",{\"1\":{\"37\":1,\"38\":1}}],[\"最高层\",{\"1\":{\"201\":1}}],[\"最近生成内容的摘要\",{\"1\":{\"217\":1}}],[\"最近邻\",{\"1\":{\"138\":1}}],[\"最近的peft技术实现了与完全微调相当的性能\",{\"1\":{\"38\":1}}],[\"最长的输入比\",{\"1\":{\"137\":1}}],[\"最后要求\",{\"1\":{\"217\":1}}],[\"最后经过一层线性层输出\",{\"1\":{\"186\":1}}],[\"最后介绍了上下文学习\",{\"1\":{\"160\":1}}],[\"最后\",{\"1\":{\"139\":1,\"204\":1,\"228\":1}}],[\"最后一个结论其实与epoch关系不大\",{\"1\":{\"132\":1}}],[\"最后一个单词对应的\",{\"1\":{\"82\":1}}],[\"最后来明确下on\",{\"1\":{\"114\":1}}],[\"最后发现的奖励是正的\",{\"1\":{\"103\":1}}],[\"最后把输出拼接在一起作为总输出\",{\"1\":{\"64\":1}}],[\"最后再将左右两部分的结果相加融合\",{\"1\":{\"40\":1}}],[\"最小值变换到−128\",{\"1\":{\"53\":1}}],[\"最小化结果矩阵和原始矩阵之间的差异\",{\"1\":{\"41\":1}}],[\"最开始应用在nlg任务上\",{\"1\":{\"45\":1}}],[\"最关键的就是引入prefix\",{\"1\":{\"45\":1}}],[\"最终我们的优化目标确定了\",{\"1\":{\"194\":1}}],[\"最终参与多头计算的q\",{\"1\":{\"156\":1}}],[\"最终发现\",{\"1\":{\"130\":1}}],[\"最终追上了精调的效果\",{\"1\":{\"44\":1}}],[\"最终得到了\",{\"1\":{\"7\":1,\"8\":1}}],[\"最先进的参数高效微调方法\",{\"0\":{\"37\":1}}],[\"评测结果显示\",{\"1\":{\"146\":1}}],[\"评测集\",{\"1\":{\"27\":1}}],[\"评论家算法\",{\"0\":{\"104\":1}}],[\"评价标准\",{\"0\":{\"103\":1}}],[\"评估模型\",{\"1\":{\"26\":1}}],[\"评估\",{\"2\":{\"21\":1,\"36\":1}}],[\"评估结果分析\",{\"0\":{\"19\":1}}],[\"评估结果\",{\"0\":{\"18\":1}}],[\"评估方法\",{\"0\":{\"22\":1},\"1\":{\"4\":1},\"2\":{\"20\":1,\"23\":1,\"25\":1,\"35\":1}}],[\"组成\",{\"1\":{\"182\":1}}],[\"组成了一个独立的\",{\"1\":{\"27\":1}}],[\"组织\",{\"1\":{\"8\":1}}],[\"组织之一\",{\"1\":{\"8\":1}}],[\"物理和化学等\",{\"1\":{\"27\":1}}],[\"消防工程师考试等\",{\"1\":{\"27\":1}}],[\"公式解析\",{\"0\":{\"52\":1}}],[\"公务员考试\",{\"1\":{\"27\":1}}],[\"公司旗下的\",{\"1\":{\"7\":1}}],[\"社会科学与其他\",{\"1\":{\"27\":1}}],[\"社会科学和自然科学三大类进行构建\",{\"1\":{\"17\":1}}],[\"种不同的学科\",{\"1\":{\"27\":1}}],[\"测试程序如下所示\",{\"1\":{\"187\":1}}],[\"测试结果\",{\"0\":{\"157\":1}}],[\"测试数据\",{\"0\":{\"27\":1}}],[\"测试集\",{\"1\":{\"7\":1}}],[\"duplicate\",{\"1\":{\"204\":1}}],[\"dog\",{\"1\":{\"230\":2}}],[\"does\",{\"1\":{\"202\":1,\"206\":1}}],[\"dong\",{\"1\":{\"165\":1}}],[\"dk​​qkt​\",{\"1\":{\"186\":1}}],[\"digram\",{\"1\":{\"226\":1}}],[\"diogo\",{\"1\":{\"165\":1}}],[\"directly\",{\"1\":{\"164\":1}}],[\"dissecting\",{\"1\":{\"82\":1}}],[\"discrete\",{\"1\":{\"42\":1}}],[\"discipline\",{\"1\":{\"26\":1}}],[\"dm\",{\"1\":{\"137\":1}}],[\"drop\",{\"1\":{\"183\":1}}],[\"droppath\",{\"1\":{\"129\":1}}],[\"dropout不能有效降低多epoch带来的坏处\",{\"1\":{\"131\":1}}],[\"dropout对不同规模模型的影响不同\",{\"0\":{\"131\":1}}],[\"dropout对模型性能的影响\",{\"1\":{\"129\":1}}],[\"dropout\",{\"1\":{\"129\":1,\"151\":2,\"152\":1,\"183\":7,\"187\":4}}],[\"dropout是一个被大语言模型忽视的正则技术\",{\"0\":{\"129\":1}}],[\"d\",{\"1\":{\"83\":3,\"232\":1}}],[\"dtype=torch\",{\"1\":{\"52\":4}}],[\"denny\",{\"1\":{\"208\":1}}],[\"dense\",{\"1\":{\"151\":6,\"152\":6}}],[\"def\",{\"1\":{\"187\":2}}],[\"descent\",{\"1\":{\"165\":1}}],[\"december\",{\"1\":{\"165\":1}}],[\"decay\",{\"1\":{\"129\":1}}],[\"decoder最标志性的cross\",{\"1\":{\"178\":1}}],[\"decoder计算过程中\",{\"1\":{\"174\":1}}],[\"decoder之分\",{\"1\":{\"172\":1}}],[\"decoder单层包括self\",{\"1\":{\"170\":1}}],[\"decoder\",{\"0\":{\"170\":1,\"174\":1},\"1\":{\"171\":11,\"174\":1}}],[\"decoder变成decoder\",{\"1\":{\"148\":1}}],[\"decoder用于语言模型\",{\"1\":{\"73\":1}}],[\"decoder模型上采用\",{\"1\":{\"43\":1}}],[\"decoder的bart\",{\"1\":{\"43\":1}}],[\"deberta\",{\"1\":{\"66\":1}}],[\"device=\",{\"1\":{\"52\":8}}],[\"deepcopy\",{\"1\":{\"171\":2}}],[\"deep\",{\"1\":{\"46\":1}}],[\"d维降到r\",{\"1\":{\"40\":1}}],[\"dathathri等\",{\"1\":{\"219\":1}}],[\"database\",{\"1\":{\"216\":1}}],[\"dataset\",{\"2\":{\"13\":1}}],[\"datasets\",{\"1\":{\"7\":2,\"8\":2}}],[\"dale\",{\"1\":{\"208\":1}}],[\"dai\",{\"1\":{\"165\":1}}],[\"damai\",{\"1\":{\"165\":1}}],[\"davinci\",{\"1\":{\"7\":1,\"161\":3,\"162\":1,\"163\":1,\"164\":1}}],[\"如对话生成式预训练变换器\",{\"1\":{\"217\":1}}],[\"如图1\",{\"1\":{\"202\":2,\"203\":1,\"204\":1}}],[\"如图3\",{\"1\":{\"123\":1,\"124\":1}}],[\"如何从中找到正确答案\",{\"1\":{\"204\":1}}],[\"如何计算大于\",{\"1\":{\"202\":1}}],[\"如何根据模块的重要性自适应地分配参数预算\",{\"1\":{\"41\":1}}],[\"如式3\",{\"1\":{\"194\":1}}],[\"如式2\",{\"1\":{\"193\":1}}],[\"如\",{\"1\":{\"137\":1,\"215\":1}}],[\"如书籍摘要\",{\"1\":{\"137\":1}}],[\"如gpt\",{\"1\":{\"129\":1}}],[\"如随机丢弃\",{\"1\":{\"129\":1}}],[\"如果已经达到设定量\",{\"1\":{\"227\":1}}],[\"如果变换\",{\"1\":{\"202\":1}}],[\"如果再深入探究\",{\"1\":{\"202\":1}}],[\"如果a\",{\"1\":{\"194\":2}}],[\"如果能够把计算量降下去\",{\"1\":{\"155\":1}}],[\"如果您发现官方的开源模型对您的业务有用\",{\"1\":{\"146\":1}}],[\"如果在token数量一定的数据集上做多epoch的模型训练\",{\"1\":{\"133\":1}}],[\"如果在st​执行at​会导致的奖励变成负的\",{\"1\":{\"103\":1}}],[\"如果前期训练不用dropout\",{\"1\":{\"130\":1}}],[\"如果语言模型的训练目标多样化\",{\"1\":{\"128\":1}}],[\"如果token数量不够\",{\"1\":{\"123\":1}}],[\"如果有对环境的建模\",{\"1\":{\"96\":1}}],[\"如果我们更严谨地来看\",{\"1\":{\"81\":1}}],[\"如果不理解这个概念\",{\"1\":{\"81\":1}}],[\"如果\",{\"1\":{\"81\":1,\"205\":1}}],[\"如果大语言模型具备越强的数据压缩能力\",{\"1\":{\"80\":1}}],[\"如果提供了任务说明\",{\"1\":{\"7\":1}}],[\"如此简单的操作\",{\"1\":{\"79\":1}}],[\"如涉侵权\",{\"1\":{\"77\":1,\"200\":1}}],[\"如tensorflow\",{\"1\":{\"61\":1}}],[\"如上图所示\",{\"1\":{\"45\":1,\"83\":1,\"155\":1}}],[\"如下图b所示\",{\"1\":{\"45\":1}}],[\"如下图所示是作者研究的模型参数规模增长和目前互联网是可用的数据集token数量增长情况\",{\"1\":{\"120\":1}}],[\"如下图所示\",{\"1\":{\"45\":2}}],[\"如下图中的红色块所示\",{\"1\":{\"43\":1}}],[\"如下所示\",{\"1\":{\"26\":1}}],[\"涵盖四个难度级别\",{\"1\":{\"27\":1}}],[\"涵盖\",{\"1\":{\"26\":1}}],[\"道多项选择题组成\",{\"1\":{\"26\":1}}],[\"e\",{\"1\":{\"232\":1}}],[\"ed\",{\"1\":{\"208\":1}}],[\"elicits\",{\"1\":{\"208\":1}}],[\"elementwise\",{\"1\":{\"151\":3,\"183\":3}}],[\"empty\",{\"1\":{\"187\":1}}],[\"emb\",{\"1\":{\"151\":2,\"152\":2}}],[\"embeddings\",{\"1\":{\"151\":2,\"152\":2}}],[\"embedding\",{\"1\":{\"44\":2,\"82\":2,\"151\":4,\"152\":2,\"183\":2}}],[\"embedded\",{\"1\":{\"2\":1}}],[\"eps=1e\",{\"1\":{\"151\":3,\"183\":3}}],[\"epoch提升性能的最重要的原因\",{\"1\":{\"129\":1}}],[\"epoch能提高训练效果可能是因为他的数据集质量更好\",{\"1\":{\"125\":1}}],[\"epoch\",{\"1\":{\"119\":1}}],[\"et\",{\"1\":{\"137\":7,\"138\":1,\"139\":4,\"165\":3,\"206\":1}}],[\"experts\",{\"1\":{\"119\":1}}],[\"effect\",{\"1\":{\"164\":1}}],[\"effective\",{\"1\":{\"61\":1,\"63\":1}}],[\"efficient\",{\"1\":{\"38\":1,\"39\":2}}],[\"encoder架构\",{\"1\":{\"172\":1}}],[\"encoder的输出流向\",{\"1\":{\"170\":1}}],[\"encoder\",{\"0\":{\"170\":1,\"173\":1},\"1\":{\"148\":1,\"151\":1,\"152\":14,\"171\":8,\"226\":1}}],[\"ensembling\",{\"1\":{\"44\":1}}],[\"engineering\",{\"1\":{\"27\":1}}],[\"evaluation\",{\"1\":{\"26\":1}}],[\"eval是一个针对基础模型的综合中文评估套件\",{\"1\":{\"26\":1}}],[\"eval\",{\"0\":{\"26\":1,\"34\":1},\"1\":{\"26\":1,\"27\":4,\"34\":1},\"2\":{\"24\":1}}],[\"0xxxxxxx\",{\"1\":{\"232\":1}}],[\"064=4\",{\"1\":{\"188\":1}}],[\"064\",{\"1\":{\"188\":1}}],[\"087\",{\"1\":{\"188\":1}}],[\"0866\",{\"1\":{\"187\":1}}],[\"0817\",{\"1\":{\"187\":1}}],[\"0829\",{\"1\":{\"187\":1}}],[\"0720\",{\"1\":{\"187\":1}}],[\"0434\",{\"1\":{\"187\":1}}],[\"0990\",{\"1\":{\"187\":1}}],[\"096=67\",{\"1\":{\"152\":1}}],[\"096=616\",{\"1\":{\"152\":1}}],[\"096=16\",{\"1\":{\"152\":1}}],[\"096+4\",{\"1\":{\"152\":2}}],[\"096\",{\"1\":{\"152\":7}}],[\"02\",{\"1\":{\"187\":1}}],[\"0tb\",{\"1\":{\"155\":1}}],[\"0501\",{\"1\":{\"187\":1}}],[\"05\",{\"1\":{\"151\":3,\"183\":3}}],[\"01625v1\",{\"1\":{\"137\":1}}],[\"03052\",{\"1\":{\"61\":1}}],[\"0310\",{\"1\":{\"52\":1}}],[\"00593\",{\"1\":{\"206\":1}}],[\"00586\",{\"1\":{\"206\":1}}],[\"0000\",{\"1\":{\"185\":12}}],[\"001\",{\"1\":{\"162\":1}}],[\"003的测试\",{\"1\":{\"164\":1}}],[\"003相同的训练方法\",{\"1\":{\"163\":1}}],[\"003和\",{\"1\":{\"161\":1}}],[\"0038\",{\"1\":{\"52\":1}}],[\"0037\",{\"1\":{\"52\":1}}],[\"002的有监督指令微调\",{\"1\":{\"161\":1}}],[\"002是一个基于code\",{\"1\":{\"161\":1}}],[\"002\",{\"1\":{\"7\":1}}],[\"0\",{\"1\":{\"19\":1,\"52\":24,\"151\":3,\"152\":25,\"183\":1,\"185\":6,\"187\":18,\"188\":2,\"232\":1}}],[\"经典的强化学习模型可以总结为图1\",{\"1\":{\"94\":1}}],[\"经历了明显的三阶段过程\",{\"1\":{\"82\":1}}],[\"经过qkv的线性层\",{\"1\":{\"156\":1}}],[\"经过了\",{\"1\":{\"146\":1}}],[\"经过微调后\",{\"1\":{\"137\":1}}],[\"经过测试发现\",{\"1\":{\"126\":1}}],[\"经过研究\",{\"1\":{\"82\":1}}],[\"经过两百万指令微调的\",{\"1\":{\"19\":1}}],[\"经改写后得到240k条instruction数据\",{\"1\":{\"8\":1}}],[\"有三个后缀字节\",{\"1\":{\"232\":1}}],[\"有两个后缀字节\",{\"1\":{\"232\":1}}],[\"有两个关键部分\",{\"1\":{\"202\":1}}],[\"有一个后缀字节\",{\"1\":{\"232\":1}}],[\"有一些控制矩阵秩的方法\",{\"1\":{\"41\":1}}],[\"有点类似哈夫曼树\",{\"1\":{\"227\":1}}],[\"有更强的可解释性\",{\"1\":{\"217\":1}}],[\"有关llm在完成任务过程中\",{\"1\":{\"200\":1}}],[\"有大有小\",{\"1\":{\"104\":1}}],[\"有时会简称为\",{\"1\":{\"79\":1}}],[\"有助于学习\",{\"1\":{\"74\":1}}],[\"有助于模型性能的提升\",{\"1\":{\"19\":1}}],[\"有效地分配参数预算\",{\"1\":{\"41\":1}}],[\"有效地适应各种下游应用程序\",{\"1\":{\"37\":1,\"38\":1}}],[\"有监督微调指令的数量也是一个重要的因素\",{\"1\":{\"19\":1}}],[\"有\",{\"1\":{\"19\":1}}],[\"但也有很多光看词汇表看不出来是哪国语言的奇异符号\",{\"1\":{\"233\":1}}],[\"但自己没有产生encoder输出的能力\",{\"1\":{\"175\":1}}],[\"但当前版本的chatglm2\",{\"1\":{\"146\":1}}],[\"但仍需要大量的计算资源\",{\"1\":{\"137\":1}}],[\"但仍然与\",{\"1\":{\"19\":1}}],[\"但在访问\",{\"1\":{\"123\":1}}],[\"但不一定概率最高就会选择该动作\",{\"1\":{\"100\":1}}],[\"但\",{\"1\":{\"61\":1,\"137\":1}}],[\"但是它对语言建模而言是次优的\",{\"1\":{\"230\":1}}],[\"但是它们前面的权重不一样\",{\"1\":{\"104\":1}}],[\"但是gpt2tokenizer还是需要一个merges\",{\"1\":{\"229\":1}}],[\"但是却要求使用者拥有并可以修改模型的结构和参数\",{\"1\":{\"216\":1}}],[\"但是也发现一些以\",{\"1\":{\"203\":1}}],[\"但是体现数字比较关系\",{\"1\":{\"202\":1}}],[\"但是trpo算法也存在问题\",{\"1\":{\"195\":1}}],[\"但是策略梯度算法存在问题\",{\"1\":{\"192\":1}}],[\"但是特征\",{\"1\":{\"175\":1}}],[\"但是会降低这种影响\",{\"1\":{\"133\":1}}],[\"但是在不同规模模型下是不同的\",{\"1\":{\"131\":1}}],[\"但是dropout会降低模型的性能\",{\"1\":{\"130\":1}}],[\"但是可以降低多epoch的影响\",{\"0\":{\"129\":1}}],[\"但是更低的计算量来预估模型的表现将十分有价值\",{\"1\":{\"127\":1}}],[\"但是二者在模型性能表现的趋势上类似\",{\"1\":{\"127\":1}}],[\"但是依然无法有效降低重复训练带来的模型损失\",{\"1\":{\"126\":1}}],[\"但是这种方式对于下游任务的影响也没有人探测过\",{\"1\":{\"123\":1}}],[\"但是这个问题很重要\",{\"1\":{\"120\":1}}],[\"但是如果有多个对某个知识点都有响应的\",{\"1\":{\"83\":1}}],[\"但是能够达成\",{\"1\":{\"81\":1}}],[\"但是p\",{\"1\":{\"45\":1}}],[\"但是相应地\",{\"1\":{\"40\":1}}],[\"但是从前面的max的目标来看\",{\"1\":{\"40\":1}}],[\"但是不让w参与训练\",{\"1\":{\"40\":1}}],[\"但是与albert不同的是\",{\"1\":{\"40\":1}}],[\"但是\",{\"1\":{\"19\":1,\"120\":1,\"123\":1,\"133\":1}}],[\"取得了中文大模型中最好的成绩\",{\"1\":{\"19\":1}}],[\"bpe\",{\"1\":{\"226\":2,\"227\":1}}],[\"bpe分词器\",{\"0\":{\"226\":1}}],[\"bn\",{\"1\":{\"185\":2}}],[\"bn是对batch的维度去做归一化\",{\"1\":{\"185\":1}}],[\"bloom\",{\"0\":{\"176\":1}}],[\"block\",{\"1\":{\"82\":2,\"155\":1}}],[\"brian\",{\"1\":{\"165\":1,\"208\":1}}],[\"broad\",{\"1\":{\"162\":1}}],[\"bosma\",{\"1\":{\"165\":1,\"208\":1}}],[\"bbh\",{\"1\":{\"146\":1}}],[\"by\",{\"0\":{\"70\":1},\"1\":{\"82\":2}}],[\"byte\",{\"1\":{\"226\":1}}],[\"bytedance\",{\"1\":{\"61\":1}}],[\"bytetransformer\",{\"0\":{\"60\":1},\"1\":{\"60\":1,\"61\":2,\"63\":1,\"64\":1,\"65\":1,\"66\":1},\"2\":{\"69\":1}}],[\"bias=false\",{\"1\":{\"151\":5,\"183\":1}}],[\"bias=true\",{\"1\":{\"151\":5,\"156\":1}}],[\"bias\",{\"1\":{\"64\":1,\"187\":2}}],[\"bigscience\",{\"1\":{\"8\":12}}],[\"bad\",{\"1\":{\"203\":7}}],[\"based\",{\"1\":{\"100\":1,\"111\":1},\"2\":{\"109\":1,\"116\":1}}],[\"batchnorm1d\",{\"1\":{\"185\":1}}],[\"batch\",{\"1\":{\"63\":1,\"65\":4}}],[\"bartbase\",{\"1\":{\"143\":1}}],[\"bart\",{\"1\":{\"43\":1,\"137\":2}}],[\"b\",{\"1\":{\"44\":1,\"52\":2,\"104\":1,\"232\":1}}],[\"b2\",{\"1\":{\"44\":1}}],[\"b1\",{\"1\":{\"44\":1}}],[\"behavior\",{\"1\":{\"203\":1}}],[\"beta\",{\"1\":{\"163\":1}}],[\"beta和text\",{\"1\":{\"162\":1}}],[\"beltagy\",{\"1\":{\"137\":1}}],[\"belle\",{\"1\":{\"19\":3}}],[\"beat\",{\"1\":{\"82\":2}}],[\"beats\",{\"1\":{\"82\":1}}],[\"bert\",{\"1\":{\"66\":1,\"137\":1}}],[\"be\",{\"1\":{\"39\":1}}],[\"budget\",{\"1\":{\"39\":1}}],[\"这也就是bpe分词器解决oov问题的一种思路\",{\"1\":{\"233\":1}}],[\"这使得用户可以更清晰地理解这个框架是如何工作的\",{\"1\":{\"217\":1}}],[\"这导致\",{\"1\":{\"203\":1}}],[\"这项研究探测了\",{\"1\":{\"203\":1}}],[\"这层只用这\",{\"1\":{\"202\":1}}],[\"这说明这条回路可能专门用于对数字进行关系比较\",{\"1\":{\"202\":1}}],[\"这说明它在预训练中学会了数字间的比较关系\",{\"1\":{\"202\":1}}],[\"这说明较低教育阶段中的知识仍然是当前中文大模型的短板之一\",{\"1\":{\"19\":1}}],[\"这在目前闭源模型遥遥领先的大语言模型时代中是不符合实际的\",{\"1\":{\"216\":1}}],[\"这在多任务和元学习环境中已被各种形式化\",{\"1\":{\"182\":1}}],[\"这在预训练和下游任务都会产生影响\",{\"1\":{\"133\":1}}],[\"这构成了一个新的监督训练数据集\",{\"1\":{\"163\":1}}],[\"这一步得到的模型是davinci\",{\"1\":{\"163\":1}}],[\"这一方面说明大模型确实可以学习一些抽象概念\",{\"1\":{\"81\":1}}],[\"这对于目前许多文本摘要数据集\",{\"1\":{\"137\":1}}],[\"这时候多epoch显然不是好的方向\",{\"1\":{\"133\":1}}],[\"这意味着我们应该寻找新的大语言模型的方向\",{\"1\":{\"133\":1}}],[\"这意味着我们可以利用较低计算量的模型预估大模型的训练结果\",{\"1\":{\"127\":1}}],[\"这意味如果你要充分训练一个大型语言模型\",{\"1\":{\"122\":1}}],[\"这里才是决定一个模型所采用的架构的关键所在\",{\"1\":{\"178\":1}}],[\"这里的wikipedia数据集质量相对c4更好一点\",{\"1\":{\"125\":1}}],[\"这里需要注意一下\",{\"1\":{\"82\":1}}],[\"这被作者称为token危机\",{\"1\":{\"120\":1}}],[\"这类神经元被称为\",{\"1\":{\"83\":1}}],[\"这类\",{\"1\":{\"83\":1}}],[\"这点对应该是个新知识\",{\"1\":{\"82\":1}}],[\"这个新的长文本生成范式将带给所有内容创作者和读者一种全新的体验\",{\"1\":{\"217\":1}}],[\"这个流程能显著提高内容创作者的效率\",{\"1\":{\"217\":1}}],[\"这个好处是让\",{\"1\":{\"217\":1}}],[\"这个工作发现了\",{\"1\":{\"204\":1}}],[\"这个工作主要探讨\",{\"1\":{\"202\":1}}],[\"这个\",{\"1\":{\"202\":1,\"203\":1}}],[\"这个值越小越好\",{\"1\":{\"196\":1}}],[\"这个值越大越好\",{\"1\":{\"196\":1}}],[\"这个式子前半部分的数学期望\",{\"1\":{\"196\":1}}],[\"这个过程被称为\",{\"1\":{\"155\":1}}],[\"这个文章指出了\",{\"1\":{\"83\":1}}],[\"这个信息在高层\",{\"1\":{\"82\":1}}],[\"这个操作也发生在\",{\"1\":{\"82\":1}}],[\"这个单词对应层数的\",{\"1\":{\"82\":1}}],[\"这个算法源自字节跳动\",{\"1\":{\"63\":1}}],[\"这是传统aigc的下一形式\",{\"1\":{\"218\":1}}],[\"这是一个新问题\",{\"1\":{\"216\":1}}],[\"这是一个在cuda编程层面提高模型训练速度的技术\",{\"1\":{\"155\":1}}],[\"这是一个有趣的发现\",{\"1\":{\"127\":1}}],[\"这是一种基于检索的方法\",{\"1\":{\"137\":1}}],[\"这是galactica能够训练4\",{\"1\":{\"129\":1}}],[\"这是第一个步骤\",{\"1\":{\"82\":1}}],[\"这是目前\",{\"1\":{\"81\":1}}],[\"这是transformer的变体\",{\"1\":{\"73\":1}}],[\"这种循环机制使得recurrentgpt能够生成任意长度的文本而不会遗忘\",{\"1\":{\"217\":1}}],[\"这种加减乘除的例子\",{\"1\":{\"205\":1}}],[\"这种方法增强了预训练的语言模型\",{\"1\":{\"137\":1}}],[\"这种方法可以有效地提高模型性能和参数效率\",{\"1\":{\"41\":1}}],[\"这种编码机制被称为\",{\"1\":{\"83\":1}}],[\"这种\",{\"1\":{\"77\":1}}],[\"这两种prompt的含义如下\",{\"1\":{\"42\":1}}],[\"这保留了未来恢复的可能性\",{\"1\":{\"41\":1}}],[\"这样我们可以对θ\",{\"1\":{\"193\":1}}],[\"这样就能从整个输入序列中检索关键字\",{\"1\":{\"140\":1}}],[\"这样相当于训练了不同\",{\"1\":{\"44\":1}}],[\"这样的循环计算机制打破了常规transformer\",{\"1\":{\"217\":1}}],[\"这样的方法虽然在长文本建模上展现了一定的优势\",{\"1\":{\"216\":1}}],[\"这样的参数化避免了svd的密集计算\",{\"1\":{\"41\":1}}],[\"这样的操作可以显式地操纵秩\",{\"1\":{\"41\":1}}],[\"这样它们可以捕获更细粒度和特定于任务的信息\",{\"1\":{\"41\":1}}],[\"这样一来就可以在单卡低显存的情况下训练大模型了\",{\"1\":{\"40\":1}}],[\"这样一来需要训练的参数量就减少了很多\",{\"1\":{\"40\":1}}],[\"这就使得训练过程中\",{\"1\":{\"40\":1}}],[\"这些小说直接与读者交互而不是与作家交互\",{\"1\":{\"218\":1}}],[\"这些层的\",{\"1\":{\"202\":1}}],[\"这些\",{\"1\":{\"202\":1}}],[\"这些资源随着上下文窗口大小的增加而增加\",{\"1\":{\"137\":1}}],[\"这些样本不能继续被使用\",{\"1\":{\"102\":1}}],[\"这些算法在保证运算正确性的前提下\",{\"1\":{\"60\":1}}],[\"这些问题基本需要大学及以上的水平才能进行解决\",{\"1\":{\"27\":1}}],[\"这些考试可以被分为四大类共\",{\"1\":{\"27\":1}}],[\"这些自然语言指令清楚而完整地描述了一项任务\",{\"1\":{\"7\":1}}],[\"即短期记忆\",{\"1\":{\"217\":2}}],[\"即后面这一项效果太弱了\",{\"1\":{\"196\":1}}],[\"即后面这一项效果太强了\",{\"1\":{\"196\":1}}],[\"即采样的次数足够多的情况下式1\",{\"1\":{\"195\":1}}],[\"即使对于相同的输入\",{\"1\":{\"182\":1}}],[\"即使在小学阶段的测试中\",{\"1\":{\"19\":1}}],[\"即语义理解是双向性的还是单向性的\",{\"1\":{\"178\":1}}],[\"即用prompt格式的训练数据进行finetune\",{\"1\":{\"162\":1}}],[\"即只需保留大小为\",{\"1\":{\"156\":1}}],[\"即如果预训练模型在重复数据上进行\",{\"1\":{\"123\":1}}],[\"即在采样到的数据里面\",{\"1\":{\"103\":1}}],[\"即\",{\"1\":{\"103\":1}}],[\"即p\",{\"1\":{\"95\":1}}],[\"即可得期望值\",{\"1\":{\"103\":1}}],[\"即可\",{\"1\":{\"65\":1}}],[\"即自适应预算分配以实现参数有效的微调\",{\"1\":{\"41\":1}}],[\"即参数高效微调\",{\"1\":{\"38\":1}}],[\"即小学\",{\"1\":{\"17\":1}}],[\"水平\",{\"1\":{\"19\":1}}],[\"大概过程就是token返回到字节\",{\"1\":{\"232\":1}}],[\"大概率\",{\"1\":{\"205\":1}}],[\"大部分知识回路应由\",{\"1\":{\"203\":1}}],[\"大部分经过微调后的中文大模型仅达到随机结果\",{\"1\":{\"19\":1}}],[\"大于\",{\"1\":{\"202\":3}}],[\"大\",{\"1\":{\"196\":1}}],[\"大大节省了训练中采样数据的时间开销\",{\"1\":{\"193\":1}}],[\"大大减少了下游任务的可训练参数数量\",{\"1\":{\"40\":1}}],[\"大大减少了需要训练的计算量\",{\"1\":{\"40\":1}}],[\"大语言模型的训练epoch通常都是1\",{\"1\":{\"120\":1}}],[\"大模型扩展定律都认为模型的规模与训练数据的规模必须同时扩大才能让模型产生更好的性能\",{\"1\":{\"120\":1}}],[\"大家发现大语言模型的规模和训练数据集中词元\",{\"1\":{\"120\":1}}],[\"大幅优化推理速度\",{\"0\":{\"60\":1},\"2\":{\"69\":1}}],[\"大学与专业\",{\"1\":{\"27\":1}}],[\"大学等主要教育阶段\",{\"1\":{\"17\":1}}],[\"大学\",{\"1\":{\"16\":1}}],[\"722\",{\"1\":{\"188\":2}}],[\"776+590\",{\"1\":{\"188\":1}}],[\"776\",{\"1\":{\"188\":1}}],[\"771\",{\"1\":{\"188\":2}}],[\"7071\",{\"1\":{\"185\":4}}],[\"768+768=590\",{\"1\":{\"188\":1}}],[\"768+768\",{\"1\":{\"188\":2}}],[\"768=786\",{\"1\":{\"188\":1}}],[\"768=38\",{\"1\":{\"188\":1}}],[\"768列的矩阵向768列的矩阵进行了一个线性变换\",{\"1\":{\"187\":1}}],[\"768列的矩阵会被扩增为为列的矩阵\",{\"1\":{\"187\":1}}],[\"768\",{\"1\":{\"183\":5,\"188\":7}}],[\"781\",{\"1\":{\"152\":2}}],[\"73\",{\"1\":{\"52\":1}}],[\"7b\",{\"1\":{\"19\":3}}],[\"7\",{\"0\":{\"46\":1,\"128\":1},\"1\":{\"19\":2,\"48\":1,\"185\":1,\"232\":2}}],[\"710\",{\"1\":{\"152\":1}}],[\"71\",{\"1\":{\"16\":1}}],[\"未经过微调\",{\"1\":{\"19\":1}}],[\"五个教育水平下各模型的零样本和少样本平均准确率\",{\"1\":{\"18\":1}}],[\"四个学科分类下各模型的零样本和少样本平均准确率\",{\"1\":{\"18\":1}}],[\"不相似度越高\",{\"1\":{\"195\":1}}],[\"不能跨符号类别进行merge操作\",{\"1\":{\"230\":1}}],[\"不能满足无害性等要求\",{\"1\":{\"163\":1}}],[\"不能在一个手写\",{\"1\":{\"64\":1}}],[\"不确定的环境中最大化它能获得的奖励\",{\"1\":{\"94\":1}}],[\"不断往\",{\"1\":{\"82\":1}}],[\"不仅在这些数据集上比\",{\"1\":{\"137\":1}}],[\"不仅学会了语言元素间的表面统计关系\",{\"1\":{\"77\":1}}],[\"不仅只在embedding上进行微调\",{\"1\":{\"45\":1}}],[\"不针对b\",{\"1\":{\"52\":1}}],[\"不过当策略π的参数更新后\",{\"1\":{\"102\":1}}],[\"不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新\",{\"1\":{\"43\":1}}],[\"不过使用了更\",{\"1\":{\"7\":1}}],[\"不再像prompt是人为构造的\",{\"1\":{\"43\":1}}],[\"不太重要的增量矩阵被修剪为具有较低的秩\",{\"1\":{\"41\":1}}],[\"不同\",{\"1\":{\"65\":1}}],[\"不同的注意力头可能会关注不同类型的信息\",{\"1\":{\"138\":1}}],[\"不同的信息可能是相关的\",{\"1\":{\"138\":1}}],[\"不同的是\",{\"1\":{\"43\":1}}],[\"不同的prefix同时加在编码器和解码器的开头\",{\"1\":{\"43\":1}}],[\"不同教育阶段下的模型零样本\",{\"1\":{\"18\":1}}],[\"不同学科类别下的模型零样本\",{\"1\":{\"18\":1}}],[\"引入自适应kl惩罚\",{\"1\":{\"196\":1}}],[\"引入了重要性采样\",{\"1\":{\"193\":1}}],[\"引入了状态值函数v\",{\"1\":{\"95\":1}}],[\"引入状态动作值函数q\",{\"1\":{\"95\":1}}],[\"引入lora部分的参数\",{\"1\":{\"40\":1}}],[\"引入task\",{\"1\":{\"8\":1}}],[\"引导模型进行情景学习\",{\"1\":{\"18\":1}}],[\"图4\",{\"1\":{\"157\":1,\"164\":1}}],[\"图\",{\"1\":{\"137\":1,\"140\":1,\"143\":1}}],[\"图3显示了长文本\",{\"1\":{\"142\":1}}],[\"图3\",{\"1\":{\"18\":2,\"83\":2,\"96\":2,\"122\":1,\"123\":1,\"124\":1,\"125\":1,\"126\":1,\"129\":1,\"142\":2,\"143\":1,\"163\":1,\"184\":1,\"186\":2,\"218\":1}}],[\"图2\",{\"1\":{\"17\":2,\"29\":1,\"30\":1,\"32\":1,\"33\":1,\"34\":1,\"63\":1,\"65\":2,\"82\":1,\"95\":1,\"104\":1,\"113\":2,\"120\":1,\"140\":1,\"155\":1,\"205\":1,\"217\":2}}],[\"图1\",{\"1\":{\"16\":1,\"61\":1,\"71\":1,\"81\":1,\"94\":1,\"103\":2,\"112\":2,\"137\":1,\"161\":1,\"170\":3,\"202\":2,\"203\":1,\"204\":2,\"227\":1}}],[\"宗教以及计算机等级考试等任务\",{\"1\":{\"17\":1}}],[\"法学\",{\"1\":{\"17\":1}}],[\"法律\",{\"1\":{\"16\":1}}],[\"理论上都可以用到这样的分解\",{\"1\":{\"40\":1}}],[\"理学\",{\"1\":{\"17\":1}}],[\"理解\",{\"1\":{\"7\":1}}],[\"高质量数据集的token数将很快用完\",{\"1\":{\"133\":1}}],[\"高层\",{\"1\":{\"82\":1}}],[\"高考\",{\"1\":{\"17\":1}}],[\"高中\",{\"1\":{\"16\":1,\"17\":1,\"27\":1}}],[\"为解决这一问题\",{\"1\":{\"216\":1}}],[\"为主\",{\"1\":{\"204\":1}}],[\"为主的知识回路\",{\"1\":{\"203\":1}}],[\"为何\",{\"1\":{\"202\":1}}],[\"为避免奖励总为正增加基线\",{\"1\":{\"104\":1}}],[\"为回答问题之后的状态\",{\"1\":{\"102\":1}}],[\"为新的推理可能性打开了大门\",{\"1\":{\"86\":1}}],[\"为界划分为两种实现方式\",{\"1\":{\"64\":1}}],[\"为消除这一影响\",{\"1\":{\"45\":1}}],[\"为每一个任务\",{\"1\":{\"44\":1}}],[\"为了更精准预测某种特殊类型数据的\",{\"1\":{\"205\":1}}],[\"为了更好地进行\",{\"1\":{\"204\":1}}],[\"为了避免重复计算\",{\"1\":{\"156\":1}}],[\"为了将比模型的上下文窗口长度更长的输入序列进行编码\",{\"1\":{\"139\":1}}],[\"为了探索这一点\",{\"1\":{\"123\":1}}],[\"为了解决采样时间开销大的问题\",{\"1\":{\"193\":1}}],[\"为了解决奖励总是正的的问题\",{\"1\":{\"104\":1}}],[\"为了解决这个问题\",{\"1\":{\"61\":1,\"65\":1}}],[\"为了最大化奖励\",{\"1\":{\"103\":1}}],[\"为了评估某个状态的整体上的好坏\",{\"1\":{\"95\":1}}],[\"为了后续推导的方便\",{\"1\":{\"95\":1}}],[\"为了增加模型参数的利用效率\",{\"1\":{\"83\":1}}],[\"为了优化\",{\"1\":{\"64\":1}}],[\"为了量化三元组的重要性\",{\"1\":{\"41\":1}}],[\"为了正则化p和q的正交性\",{\"1\":{\"41\":1}}],[\"为了回答这个问题\",{\"1\":{\"41\":1}}],[\"为了提高数据集中学科知识点的覆盖范围\",{\"1\":{\"17\":1}}],[\"为进一步拓展数据集的丰富度\",{\"1\":{\"17\":1}}],[\"为提高数据集的学科覆盖率\",{\"1\":{\"17\":1}}],[\"为方便读者阅读\",{\"1\":{\"8\":1}}],[\"例如transformer的self\",{\"1\":{\"182\":1}}],[\"例如回答关于维基百科上所有健在作者的文章的聚合属性的问题\",{\"1\":{\"137\":1}}],[\"例如预测下一个单词是神什么的生成式目标\",{\"1\":{\"128\":1}}],[\"例如britain\",{\"1\":{\"45\":1}}],[\"例如\",{\"1\":{\"17\":1,\"120\":1,\"128\":1,\"137\":2,\"155\":1}}],[\"研究了大语言模型的epoch次数设置问题\",{\"1\":{\"118\":1}}],[\"研究生入学考试和中国公务员考试等真题题目\",{\"1\":{\"17\":1}}],[\"研究生入学考试题目\",{\"1\":{\"16\":1}}],[\"研究人员补充了中医\",{\"1\":{\"17\":1}}],[\"研究人员基于人文艺术\",{\"1\":{\"17\":1}}],[\"研究人员选择了中国升学考试中的统考试题\",{\"1\":{\"17\":1}}],[\"研究人员模仿中国学生的教育经历\",{\"1\":{\"17\":1}}],[\"契合中国教育体系\",{\"1\":{\"17\":1}}],[\"艺术等学科\",{\"1\":{\"16\":1,\"17\":1}}],[\"工程技术\",{\"1\":{\"16\":1,\"17\":1}}],[\"科学\",{\"1\":{\"16\":1,\"17\":1}}],[\"心理学\",{\"1\":{\"16\":1,\"17\":1}}],[\"教育学\",{\"1\":{\"17\":1}}],[\"教育\",{\"1\":{\"16\":1}}],[\"政治\",{\"1\":{\"16\":1,\"17\":1}}],[\"历史生成内容中和当前时间步最相关的内容\",{\"1\":{\"217\":1}}],[\"历史\",{\"1\":{\"16\":1,\"17\":1}}],[\"初中\",{\"1\":{\"16\":1,\"17\":1}}],[\"包括\",{\"1\":{\"17\":1}}],[\"包括小升初\",{\"1\":{\"17\":1}}],[\"包括小学\",{\"1\":{\"16\":1}}],[\"包含超过\",{\"1\":{\"137\":1}}],[\"包含医学\",{\"1\":{\"27\":1}}],[\"包含\",{\"1\":{\"16\":1}}],[\"包含13个nlp任务\",{\"1\":{\"8\":1}}],[\"包含270个nlp任务的2000多个prompt模版\",{\"1\":{\"8\":1}}],[\"包含61个nlp任务\",{\"1\":{\"8\":1}}],[\"包含的语种个数不定\",{\"1\":{\"8\":1}}],[\"包含中文\",{\"1\":{\"8\":1}}],[\"覆盖多学科领域\",{\"1\":{\"17\":1}}],[\"覆盖多教育阶段\",{\"1\":{\"17\":1}}],[\"覆盖\",{\"1\":{\"16\":1}}],[\"收集了\",{\"1\":{\"16\":1}}],[\"天津大学与华为诺亚方实验室\",{\"1\":{\"15\":1}}],[\"旨在提升大型语言模型对长篇文档的理解能力\",{\"1\":{\"86\":1}}],[\"旨在评估中文大模型在不同教育阶段下的表现\",{\"1\":{\"17\":1}}],[\"旨在衡量中文大型语言模型在零样本和少样本设置中获取知识的能力\",{\"1\":{\"15\":1}}],[\"旨在训练helpful\",{\"1\":{\"8\":1}}],[\"m\",{\"1\":{\"206\":1}}],[\"mha\",{\"1\":{\"156\":1}}],[\"mt\",{\"1\":{\"137\":1}}],[\"merge\",{\"1\":{\"227\":3}}],[\"memory\",{\"1\":{\"216\":1,\"217\":1},\"2\":{\"221\":1}}],[\"memorizing\",{\"1\":{\"137\":1}}],[\"method\",{\"1\":{\"151\":1,\"183\":1}}],[\"meta\",{\"1\":{\"120\":1,\"165\":1}}],[\"mlm这种模型受到的影响反而更小\",{\"1\":{\"128\":1}}],[\"mlp\",{\"1\":{\"61\":1,\"151\":3,\"152\":6,\"183\":1,\"202\":7,\"203\":1}}],[\"mit\",{\"1\":{\"165\":1}}],[\"mishkin\",{\"1\":{\"165\":1}}],[\"mixture\",{\"1\":{\"119\":1}}],[\"michael\",{\"1\":{\"107\":1}}],[\"mover\",{\"1\":{\"204\":1}}],[\"modulelist\",{\"1\":{\"151\":2,\"183\":1}}],[\"modules\",{\"1\":{\"151\":1,\"183\":2}}],[\"module\",{\"1\":{\"151\":1,\"183\":1,\"187\":1}}],[\"model\",{\"1\":{\"122\":1,\"163\":1,\"183\":2,\"202\":1,\"203\":1,\"206\":1,\"217\":1}}],[\"models\",{\"1\":{\"26\":1,\"39\":1,\"40\":1,\"82\":1,\"162\":1,\"165\":3,\"181\":1,\"208\":1,\"219\":1}}],[\"moe\",{\"1\":{\"119\":1}}],[\"moritz\",{\"1\":{\"107\":1}}],[\"music\",{\"1\":{\"82\":6}}],[\"multitask\",{\"1\":{\"181\":1,\"219\":1}}],[\"multi\",{\"0\":{\"156\":1,\"186\":1},\"1\":{\"26\":2,\"64\":2,\"156\":3,\"186\":1}}],[\"mary\",{\"1\":{\"204\":1}}],[\"mathematical\",{\"1\":{\"202\":1,\"206\":1}}],[\"mathematics\",{\"1\":{\"27\":1}}],[\"ma\",{\"1\":{\"165\":1}}],[\"maarten\",{\"1\":{\"165\":1,\"208\":1}}],[\"machine\",{\"1\":{\"107\":1}}],[\"mask是对称阵还是上三角矩阵\",{\"1\":{\"178\":1}}],[\"mask\",{\"1\":{\"45\":2,\"63\":1,\"172\":1}}],[\"main\",{\"1\":{\"7\":2}}],[\"m3ke数据与其他评估数据集对比\",{\"1\":{\"17\":1}}],[\"m3ke数据集中任务领域和难度的分布\",{\"1\":{\"17\":1}}],[\"m3ke数据集中任务分布\",{\"1\":{\"16\":1}}],[\"m3ke数据集是一种针对大语言模型的多层次\",{\"1\":{\"15\":1}}],[\"m3ke\",{\"1\":{\"15\":1,\"16\":1,\"17\":1,\"18\":1}}],[\"m3ke评估数据集分享\",{\"0\":{\"15\":1}}],[\"将新字符加入词表\",{\"1\":{\"227\":1}}],[\"将新单词加到原输入句后面\",{\"1\":{\"181\":1}}],[\"将语料库中所有单词拆分为单个字符\",{\"1\":{\"227\":1}}],[\"将语言建模作为微调的辅助目标\",{\"1\":{\"74\":1}}],[\"将\",{\"1\":{\"203\":1}}],[\"将要产生后续\",{\"1\":{\"203\":1}}],[\"将式1\",{\"1\":{\"193\":1}}],[\"将它们加载到sram中\",{\"1\":{\"155\":1}}],[\"将这些块从hbm加载至sram中\",{\"1\":{\"155\":1}}],[\"将数据从r变回d维\",{\"1\":{\"40\":1}}],[\"将数据从\",{\"1\":{\"40\":1}}],[\"将结构化数据序列化并嵌入到prompt中\",{\"1\":{\"8\":1}}],[\"将flan\",{\"1\":{\"8\":1}}],[\"对所有token\",{\"1\":{\"232\":1}}],[\"对所有τ出现的概率与对应的奖励进行加权最后求和\",{\"1\":{\"103\":1}}],[\"对这个经过merge操作的新串\",{\"1\":{\"231\":1}}],[\"对某个选项进行修改或者自己编辑一个新的选项\",{\"1\":{\"217\":1}}],[\"对我们来说\",{\"1\":{\"196\":2}}],[\"对他进行了一下测试\",{\"1\":{\"187\":1}}],[\"对向量用以下函数进行了标准化\",{\"1\":{\"185\":1}}],[\"对其它无关输入保持沉默\",{\"1\":{\"83\":1}}],[\"对应位置\",{\"1\":{\"82\":1}}],[\"对应的属性\",{\"1\":{\"82\":1}}],[\"对应的\",{\"1\":{\"8\":1,\"82\":1,\"203\":1}}],[\"对同一个输入\",{\"1\":{\"65\":1}}],[\"对\",{\"1\":{\"65\":1,\"123\":1}}],[\"对称量化把每一行的绝对值的最大值变换到127\",{\"1\":{\"53\":1}}],[\"对每个额外任务产生非常小的开销\",{\"1\":{\"43\":1}}],[\"对角矩阵∧包含奇异值\",{\"1\":{\"41\":1}}],[\"对于每个交叉注意头\",{\"1\":{\"140\":1}}],[\"对于规模较大的模型\",{\"1\":{\"131\":1}}],[\"对于长\",{\"1\":{\"64\":1}}],[\"对于短\",{\"1\":{\"64\":1}}],[\"对于\",{\"1\":{\"64\":1}}],[\"对于单向语言模型采用\",{\"1\":{\"45\":1}}],[\"对于bert类双向语言模型采用模版\",{\"1\":{\"45\":1}}],[\"对于encoder\",{\"1\":{\"43\":1}}],[\"对于decoder\",{\"1\":{\"43\":1}}],[\"对于微调大型模型\",{\"1\":{\"41\":1}}],[\"对于左右两个部分\",{\"1\":{\"40\":1}}],[\"对话指令\",{\"1\":{\"8\":1}}],[\"类似于预激活残差网络\",{\"1\":{\"183\":1}}],[\"类似于下图所示\",{\"1\":{\"164\":1}}],[\"类似步长\",{\"1\":{\"103\":1}}],[\"类别\",{\"1\":{\"8\":1}}],[\"类型数据的\",{\"1\":{\"7\":1}}],[\"作为后缀字节\",{\"1\":{\"232\":1}}],[\"作为新的输入句\",{\"1\":{\"181\":1}}],[\"作为\",{\"1\":{\"63\":1}}],[\"作者在结尾再次精心设计了\",{\"1\":{\"217\":1}}],[\"作者还将上下文大小从512个token增加到1024个token\",{\"1\":{\"183\":1}}],[\"作者还发现\",{\"1\":{\"74\":1}}],[\"作者方法的核心是语言建模\",{\"1\":{\"182\":1}}],[\"作者强调的是moe的模型表现与大模型真正的训练有类似的趋势\",{\"1\":{\"132\":1}}],[\"作者考虑不在全部训练中使用dropout\",{\"1\":{\"130\":1}}],[\"作者考虑到词表的维度很大\",{\"1\":{\"40\":1}}],[\"作者已经发现dropout可以降低多epoch的影响\",{\"1\":{\"130\":1}}],[\"作者研究了这些正则技术是否可以降低多epoch的影响\",{\"1\":{\"129\":1}}],[\"作者对比了moe架构\",{\"1\":{\"126\":1}}],[\"作者用相同的重复策略在c4数据集和维基\",{\"1\":{\"125\":1}}],[\"作者也继续做了这方面的研究\",{\"1\":{\"123\":1}}],[\"作者也设计了few\",{\"1\":{\"27\":1}}],[\"作者分别使用c4数据集的子集\",{\"1\":{\"123\":1}}],[\"作者比较了在各种计算预算下掩码标记预测的验证准确性\",{\"1\":{\"122\":1}}],[\"作者采用t5模型和c4数据集进行实验\",{\"1\":{\"119\":1}}],[\"作者提出了一系列问题\",{\"1\":{\"119\":1}}],[\"作者指出\",{\"1\":{\"118\":1}}],[\"作者优化了以下目标\",{\"1\":{\"74\":1}}],[\"作者将初始化时残差层的权重按n​1​的因子进行缩放\",{\"1\":{\"183\":1}}],[\"作者将重复的次数固定\",{\"1\":{\"124\":1}}],[\"作者将参数调整为受监督的目标任务\",{\"1\":{\"74\":1}}],[\"作者将多层transformer\",{\"1\":{\"73\":1}}],[\"作者使用标准语言建模目标来最大化以下概率\",{\"1\":{\"73\":1}}],[\"作者证明了通过在大量未标注文本上对语言模型进行生成式预训练\",{\"1\":{\"70\":1}}],[\"作者发现直接优化prompt参数不太稳定\",{\"1\":{\"43\":1}}],[\"作者通过两个策略降低了训练的参数量\",{\"1\":{\"40\":1}}],[\"作者团队从\",{\"1\":{\"27\":1}}],[\"作者人工将试题数据做了统一\",{\"1\":{\"27\":1}}],[\"作者\",{\"1\":{\"8\":1,\"208\":1}}],[\"作用巨大\",{\"1\":{\"6\":1}}],[\"上一步生成的段落\",{\"1\":{\"217\":1}}],[\"上\",{\"1\":{\"202\":1}}],[\"上下文学习\",{\"0\":{\"164\":1},\"1\":{\"164\":1}}],[\"上浪费精力\",{\"1\":{\"138\":1}}],[\"上海人工智能实验室联合商汤科技共同提出一种新的\",{\"1\":{\"136\":1}}],[\"上升多的才会上升\",{\"1\":{\"104\":1}}],[\"上升的少的就是下降的\",{\"1\":{\"104\":1}}],[\"上升的少一些\",{\"1\":{\"104\":1}}],[\"上升的多一点\",{\"1\":{\"104\":1}}],[\"上图中蓝色部分\",{\"1\":{\"83\":1}}],[\"上面讲述内容是以数据压缩的视角来看待\",{\"1\":{\"81\":1}}],[\"上开源了\",{\"1\":{\"66\":1}}],[\"上进行指令微调的尝试\",{\"1\":{\"8\":1}}],[\"上述数据集可以总结概括为以下表格\",{\"1\":{\"8\":1}}],[\"上你可以找到\",{\"1\":{\"8\":1}}],[\"使得训练更加稳定\",{\"1\":{\"191\":1}}],[\"使得这些可以在同一个unifiedskg\",{\"1\":{\"8\":1}}],[\"使其可以忽略不计\",{\"1\":{\"65\":1}}],[\"使其已经具备较好的零样本学习能力\",{\"1\":{\"19\":1}}],[\"使前缀调优模块化并节省空间\",{\"1\":{\"43\":1}}],[\"使用utf\",{\"1\":{\"232\":1}}],[\"使用键值互换的反向词汇表映射到一个字节串列表\",{\"1\":{\"232\":1}}],[\"使用词汇表映射到token\",{\"1\":{\"231\":1}}],[\"使用自然语言模拟长短期记忆\",{\"1\":{\"217\":1}}],[\"使用修正的初始化\",{\"1\":{\"183\":1}}],[\"使用rm来更新ppo策略\",{\"1\":{\"163\":1}}],[\"使用人工标注prompt数据集的答案用来finetune模型\",{\"1\":{\"163\":1}}],[\"使用长范围训练方法的试验结果\",{\"1\":{\"142\":1}}],[\"使用编码器对完整输入进行块编码\",{\"1\":{\"140\":1}}],[\"使用编码后的数据进行数据传输\",{\"1\":{\"80\":1}}],[\"使用重复标记训练\",{\"1\":{\"123\":1}}],[\"使用kl散度解决两个分布相差大或步长难以确定的问题\",{\"1\":{\"105\":1}}],[\"使用随机梯度下降训练这些参数\",{\"1\":{\"73\":1}}],[\"使用peft微调llms\",{\"1\":{\"48\":1}}],[\"使用bilstm对pi序列进行表征\",{\"1\":{\"46\":1}}],[\"使用gpt3生成64k的instruction\",{\"1\":{\"8\":1}}],[\"使用gpt3将\",{\"1\":{\"8\":1}}],[\"使用llms生成prompt进行instruct\",{\"1\":{\"8\":1}}],[\"使用\",{\"1\":{\"7\":2,\"65\":1}}],[\"o\",{\"1\":{\"206\":1}}],[\"object\",{\"1\":{\"204\":4,\"206\":1}}],[\"ouyang\",{\"1\":{\"165\":1}}],[\"out\",{\"1\":{\"151\":10,\"156\":1,\"183\":1,\"187\":2}}],[\"output∣input\",{\"1\":{\"182\":2}}],[\"output\",{\"1\":{\"8\":1,\"151\":1,\"152\":1}}],[\"o=pv这三个反复执行的操作\",{\"1\":{\"155\":1}}],[\"online\",{\"1\":{\"165\":1}}],[\"only这两种\",{\"1\":{\"178\":1}}],[\"only和encoder\",{\"1\":{\"178\":1}}],[\"only架构相区别的特征\",{\"1\":{\"174\":1}}],[\"only架构有两大与encoder\",{\"1\":{\"174\":1}}],[\"only的gpt\",{\"1\":{\"43\":1}}],[\"only\",{\"0\":{\"29\":1,\"173\":1,\"174\":1},\"1\":{\"148\":1,\"164\":1}}],[\"on\",{\"0\":{\"114\":1},\"1\":{\"107\":1,\"162\":1,\"164\":1,\"165\":1}}],[\"own\",{\"1\":{\"82\":2}}],[\"openreview\",{\"1\":{\"165\":1}}],[\"openai官网的chatgpt的训练流程和instructgpt基本一致\",{\"1\":{\"163\":1}}],[\"openai发布了模型索引为的davinci的初代gpt\",{\"1\":{\"161\":1}}],[\"openai\",{\"1\":{\"81\":1,\"162\":1,\"219\":1},\"2\":{\"98\":1,\"109\":1,\"116\":1,\"167\":1}}],[\"operations\",{\"1\":{\"126\":1}}],[\"optimizers\",{\"1\":{\"165\":1}}],[\"optimization\",{\"1\":{\"107\":1,\"191\":1}}],[\"optimizing\",{\"1\":{\"39\":1,\"48\":1}}],[\"orleans\",{\"1\":{\"165\":1}}],[\"or\",{\"0\":{\"118\":1},\"1\":{\"118\":1}}],[\"org\",{\"1\":{\"61\":1,\"137\":1}}],[\"orhonovich\",{\"1\":{\"7\":1,\"8\":2}}],[\"offsets\",{\"1\":{\"63\":2}}],[\"of\",{\"0\":{\"208\":1,\"215\":1},\"1\":{\"39\":2,\"40\":1,\"82\":1,\"107\":1,\"119\":1,\"151\":1,\"162\":1,\"165\":1,\"183\":1,\"208\":2,\"219\":1}}],[\"并统计每个字符的频率\",{\"1\":{\"227\":1}}],[\"并提供几个可能的选项供创作者继续写故事\",{\"1\":{\"217\":1}}],[\"并重申了当前小说写作的情景\",{\"1\":{\"217\":1}}],[\"并说明在输入部分会给出的内容\",{\"1\":{\"217\":1}}],[\"并最后生成几个对下一个时间步中生成内容的规划\",{\"1\":{\"217\":1}}],[\"并最终关联到输出位置\",{\"1\":{\"205\":1}}],[\"并最终减少对hbm的访问\",{\"1\":{\"155\":1}}],[\"并分别更新存储在硬盘和提示中的基于语言的长短期记忆\",{\"1\":{\"217\":1}}],[\"并拷贝到后续\",{\"1\":{\"203\":1}}],[\"并使用自然语言来模拟lstm中的长短期记忆机制\",{\"1\":{\"217\":1}}],[\"并使用更大的batch\",{\"1\":{\"183\":1}}],[\"并使用同样的模型将\",{\"1\":{\"7\":1}}],[\"并没有encoder\",{\"1\":{\"172\":1}}],[\"并非是由于结构的原因\",{\"1\":{\"172\":1}}],[\"并注释该论文\",{\"1\":{\"169\":1}}],[\"并在注意力运算之间再次读取\",{\"1\":{\"155\":1}}],[\"并在对话阶段使用\",{\"1\":{\"146\":1}}],[\"并只关注这前\",{\"1\":{\"140\":1}}],[\"并只训练这些\",{\"1\":{\"44\":1}}],[\"并仅对输入序列中的前\",{\"1\":{\"140\":1}}],[\"并关注前\",{\"1\":{\"137\":1}}],[\"并采用参数共享\",{\"1\":{\"126\":1}}],[\"并采用了\",{\"1\":{\"8\":1}}],[\"并通过设置多次epoch来让模型总的训练过的token差不多水平\",{\"1\":{\"123\":1}}],[\"并贪婪的选择q值最大的动作\",{\"1\":{\"96\":1}}],[\"并不是生成所需上下文长度的上限\",{\"1\":{\"137\":1}}],[\"并不单单靠\",{\"1\":{\"81\":1}}],[\"并不会在推理阶段加速\",{\"1\":{\"40\":1}}],[\"并把上文中跟在\",{\"1\":{\"203\":1}}],[\"并把\",{\"1\":{\"64\":1}}],[\"并加入锚字符\",{\"1\":{\"46\":1}}],[\"并引入少量自然语言提示的锚字符\",{\"1\":{\"45\":1}}],[\"并进行微调\",{\"1\":{\"41\":1}}],[\"并进行微调训练\",{\"1\":{\"39\":1}}],[\"并稳定了训练\",{\"1\":{\"41\":1}}],[\"并且是可解释的\",{\"1\":{\"217\":1}}],[\"并且将其存储在硬盘和提示中\",{\"1\":{\"217\":1}}],[\"并且探索以自然语言模拟循环机制的可能性\",{\"1\":{\"216\":1}}],[\"并且在最终的self\",{\"1\":{\"183\":1}}],[\"并且可以实现长文本的交互式生成\",{\"1\":{\"217\":1}}],[\"并且可以进行交互式生成长文本\",{\"1\":{\"217\":1}}],[\"并且可以被注入到任何预训练的\",{\"1\":{\"140\":1}}],[\"并且可以在没有任何进一步训练的情况下改进现有的\",{\"1\":{\"137\":1}}],[\"并且上述各种优化手段也可以方便地应用到变种\",{\"1\":{\"66\":1}}],[\"并且实现了全面的\",{\"1\":{\"61\":1}}],[\"并且lora与全参数微调的差距不超过0\",{\"1\":{\"47\":1}}],[\"并且只优化prefix\",{\"1\":{\"43\":1}}],[\"并且无法更新参数\",{\"1\":{\"43\":1}}],[\"并且它不再局限于embedding层\",{\"1\":{\"40\":1}}],[\"并且思维与推理过程颇有难度\",{\"1\":{\"27\":1}}],[\"并将当前时间步的输出作为下一个时间步的输入\",{\"1\":{\"217\":1}}],[\"并将其作为一个外在约束\",{\"1\":{\"195\":1}}],[\"并将其存储在数据存储中\",{\"1\":{\"140\":1}}],[\"并将注意力计算的输出写回至hbm\",{\"1\":{\"155\":1}}],[\"并将它们加载到sram中\",{\"1\":{\"155\":1}}],[\"并将可训练的秩分解矩阵注入到transformer层的每个权重中\",{\"1\":{\"40\":1}}],[\"并将题目中涉及的公式都转化为了标准的\",{\"1\":{\"27\":1}}],[\"并针对拥有自由参数的prefix部分进行微调训练\",{\"1\":{\"39\":1}}],[\"扩展其\",{\"1\":{\"8\":1}}],[\"英文全称low\",{\"1\":{\"40\":1}}],[\"英文\",{\"1\":{\"8\":1}}],[\"模块的相关性聚合在浅层与深层分别配备了局部全局token\",{\"1\":{\"136\":1}}],[\"模型通过\",{\"1\":{\"205\":1}}],[\"模型可以做到输出的\",{\"1\":{\"202\":1}}],[\"模型能够通过预训练获得数学能力\",{\"1\":{\"202\":1}}],[\"模型中的conv1d层并非pytorch预设的卷积层torch\",{\"1\":{\"187\":1}}],[\"模型由多层单向\",{\"1\":{\"181\":1}}],[\"模型并不能正确回答\",{\"1\":{\"164\":1}}],[\"模型训练步骤\",{\"1\":{\"163\":1}}],[\"模型训练的优化表示为\",{\"1\":{\"40\":1}}],[\"模型结构\",{\"0\":{\"150\":1}}],[\"模型规模的增长其实表现在2个方面\",{\"1\":{\"126\":1}}],[\"模型时候认为他之所以用4\",{\"1\":{\"125\":1}}],[\"模型性能下降不明显\",{\"1\":{\"124\":1}}],[\"模型性能受到很大的影响\",{\"1\":{\"118\":1}}],[\"模型在生成长篇文本方面的限制\",{\"1\":{\"217\":1}}],[\"模型在预训练过程中\",{\"1\":{\"204\":1}}],[\"模型在\",{\"1\":{\"82\":1}}],[\"模型在对话任务上的表现强于在超大规模任务集上的结果\",{\"1\":{\"7\":1}}],[\"模型对知识的提取过程\",{\"0\":{\"82\":1}}],[\"模型压缩效率越高\",{\"1\":{\"81\":1}}],[\"模型智能程度越高\",{\"1\":{\"81\":1}}],[\"模型仅仅学会了语言中的单词共现等浅层的表面统计关系\",{\"1\":{\"77\":1}}],[\"模型架构解析\",{\"0\":{\"184\":1}}],[\"模型架构\",{\"0\":{\"71\":1,\"183\":1}}],[\"模型发展调研\",{\"1\":{\"48\":1}}],[\"模型\",{\"1\":{\"44\":1,\"46\":4,\"79\":1,\"161\":1},\"2\":{\"76\":1,\"135\":1,\"199\":1}}],[\"模型原有的参数是φ\",{\"1\":{\"40\":1}}],[\"模型的训练方法和数据集\",{\"0\":{\"163\":1}}],[\"模型的两种不同变体\",{\"1\":{\"161\":1}}],[\"模型的性能越差\",{\"1\":{\"123\":1}}],[\"模型的有损数据压缩能力\",{\"1\":{\"81\":1}}],[\"模型的优化表示为\",{\"1\":{\"40\":1}}],[\"模型的参数用\",{\"1\":{\"40\":1}}],[\"模型参数量与flops对模型性能的影响\",{\"1\":{\"126\":1}}],[\"模型参数相同的情况下\",{\"1\":{\"126\":1}}],[\"模型参数与训练所需token关系\",{\"1\":{\"122\":1}}],[\"模型参数规模越大\",{\"1\":{\"123\":1}}],[\"模型参数规模与token数量需要匹配\",{\"0\":{\"122\":1}}],[\"模型参数规模增长和目前互联网是可用的数据集token数量增长情况\",{\"1\":{\"120\":1}}],[\"模型参数规模的增长可能会导致token不足的情况\",{\"1\":{\"118\":1}}],[\"模型参数\",{\"1\":{\"37\":1,\"38\":1}}],[\"模型要求直接回答问题\",{\"1\":{\"18\":1}}],[\"模版\",{\"1\":{\"8\":1}}],[\"个神经元就能大致完成\",{\"1\":{\"202\":1}}],[\"个神经元\",{\"1\":{\"202\":1}}],[\"个隐状态\",{\"1\":{\"140\":1}}],[\"个输入\",{\"1\":{\"137\":1}}],[\"个token\",{\"1\":{\"123\":1}}],[\"个标记\",{\"1\":{\"123\":1}}],[\"个特征\",{\"1\":{\"83\":1}}],[\"个子问题参数\",{\"1\":{\"65\":1}}],[\"个独立的矩阵乘子问题\",{\"1\":{\"65\":1}}],[\"个学科的问题\",{\"1\":{\"27\":1}}],[\"个不同学科和四个难度级别\",{\"1\":{\"26\":1}}],[\"个不同的语言的版本\",{\"1\":{\"8\":1}}],[\"个任务\",{\"1\":{\"16\":1}}],[\"个候选答案\",{\"1\":{\"16\":1}}],[\"个真人标准化考试题目\",{\"1\":{\"16\":1}}],[\"个\",{\"1\":{\"8\":3,\"137\":1,\"140\":3,\"202\":1}}],[\"项目贡献者\",{\"1\":{\"15\":1}}],[\"项目地址\",{\"1\":{\"15\":1}}],[\"项目名称\",{\"1\":{\"8\":1}}],[\"项目主页\",{\"1\":{\"8\":1}}],[\"项目包含了\",{\"1\":{\"8\":1}}],[\"项目链接\",{\"1\":{\"7\":4,\"8\":3}}],[\"截止目前\",{\"1\":{\"8\":1}}],[\"帮助研究者基于现有nlp\",{\"1\":{\"8\":1}}],[\"id列表\",{\"1\":{\"232\":1}}],[\"id\",{\"1\":{\"231\":1}}],[\"id的字典\",{\"1\":{\"229\":1}}],[\"identification\",{\"1\":{\"204\":4,\"206\":1}}],[\"idris\",{\"1\":{\"8\":1}}],[\"is\",{\"1\":{\"169\":1,\"171\":4}}],[\"import\",{\"1\":{\"183\":1,\"185\":2}}],[\"implicitly\",{\"1\":{\"165\":1}}],[\"improving\",{\"0\":{\"70\":1}}],[\"ichter\",{\"1\":{\"208\":1}}],[\"iclr\",{\"1\":{\"165\":1}}],[\"icl只对attention有影响\",{\"1\":{\"164\":1}}],[\"icl只存在一次前向传播中\",{\"1\":{\"164\":1}}],[\"icl是一个元优化的过程\",{\"1\":{\"164\":1}}],[\"icl和微调的区别\",{\"1\":{\"164\":1}}],[\"icl\",{\"1\":{\"164\":2}}],[\"icml\",{\"1\":{\"107\":1}}],[\"ivgi\",{\"1\":{\"137\":1,\"139\":2}}],[\"inhibition\",{\"1\":{\"204\":1}}],[\"indirect\",{\"1\":{\"204\":4,\"206\":1}}],[\"induction\",{\"0\":{\"203\":1},\"1\":{\"203\":3,\"204\":1}}],[\"init\",{\"1\":{\"187\":3}}],[\"information\",{\"1\":{\"165\":1,\"219\":1}}],[\"inplace=false\",{\"1\":{\"151\":1,\"183\":4,\"187\":2}}],[\"input\",{\"1\":{\"137\":1,\"151\":2,\"152\":2}}],[\"insights\",{\"0\":{\"118\":1},\"1\":{\"118\":1}}],[\"instructgpt的训练数据构成\",{\"1\":{\"163\":1}}],[\"instructdial\",{\"1\":{\"7\":3,\"8\":1}}],[\"instructions\",{\"1\":{\"162\":1,\"165\":1}}],[\"instruction等\",{\"1\":{\"8\":1}}],[\"instruction\",{\"1\":{\"7\":8,\"8\":7,\"161\":2,\"162\":2}}],[\"instruct\",{\"0\":{\"6\":1,\"7\":1},\"1\":{\"6\":1,\"7\":6,\"8\":1,\"162\":1,\"163\":1},\"2\":{\"10\":1,\"167\":1}}],[\"in\",{\"1\":{\"18\":1,\"82\":1,\"83\":2,\"107\":1,\"151\":10,\"156\":1,\"164\":1,\"165\":4,\"183\":1,\"202\":1,\"204\":2,\"206\":3,\"208\":2},\"2\":{\"167\":1}}],[\"int4量化下\",{\"1\":{\"146\":1}}],[\"interactive\",{\"0\":{\"215\":1}}],[\"interpretability\",{\"1\":{\"204\":1,\"206\":1}}],[\"interpreting\",{\"1\":{\"202\":1,\"206\":1}}],[\"international\",{\"1\":{\"107\":1,\"165\":1}}],[\"intelligence\",{\"1\":{\"2\":1}}],[\"int8\",{\"1\":{\"52\":1}}],[\"int8量化技术原理讲解\",{\"0\":{\"51\":1}}],[\"int量化技术是一种节约大模型推理或训练的过程中占用的显存的技术\",{\"1\":{\"51\":1}}],[\"intrauct\",{\"1\":{\"7\":1}}],[\"由知识点相互激发形成的固定通路\",{\"1\":{\"205\":1}}],[\"由上可看出\",{\"1\":{\"204\":1}}],[\"由此可以证明icl只存在于一次前向传播\",{\"1\":{\"164\":1}}],[\"由此可以推导出策略梯度定理\",{\"1\":{\"103\":1}}],[\"由此推测icl并不会被模型记住\",{\"1\":{\"164\":1}}],[\"由sft模型随机生成多个答案\",{\"1\":{\"163\":1}}],[\"由gelu变成swiglu\",{\"1\":{\"154\":1}}],[\"由layernorm变成rmsnorm\",{\"1\":{\"153\":1}}],[\"由chatglm\",{\"1\":{\"146\":1}}],[\"由于dog有很多变体\",{\"1\":{\"230\":1}}],[\"由于人类用户可以轻松观察和编辑自然语言记忆\",{\"1\":{\"217\":2}}],[\"由于一般的系统应该能够执行许多不同的任务\",{\"1\":{\"182\":1}}],[\"由于语言具有自然的顺序性\",{\"1\":{\"182\":1}}],[\"由于self\",{\"1\":{\"155\":1}}],[\"由于抛弃了nlu任务\",{\"1\":{\"149\":1}}],[\"由于编码器上下文窗口的大小是固定的\",{\"1\":{\"138\":1}}],[\"由于每一个轨迹τ都有其对应的发生概率\",{\"1\":{\"103\":1}}],[\"由于每个教育阶段需要掌握的知识点不同\",{\"1\":{\"17\":1}}],[\"由于子问题的数量\",{\"1\":{\"65\":1}}],[\"由于变成了int8整型\",{\"1\":{\"52\":1}}],[\"由于在不太重要的权重矩阵添加更多的参数会产生很少的收益\",{\"1\":{\"41\":1}}],[\"由于不需要对模型的权重参数重新计算梯度\",{\"1\":{\"40\":1}}],[\"由于爬虫得到的试题格式不统一\",{\"1\":{\"27\":1}}],[\"由\",{\"1\":{\"8\":1,\"45\":1}}],[\"6月发布的text\",{\"1\":{\"161\":1}}],[\"6243584000\",{\"1\":{\"152\":2}}],[\"67\",{\"1\":{\"152\":2}}],[\"635\",{\"1\":{\"152\":1}}],[\"638\",{\"1\":{\"152\":1}}],[\"632795115\",{\"1\":{\"77\":1,\"200\":1}}],[\"688\",{\"1\":{\"152\":2}}],[\"65024\",{\"1\":{\"151\":1,\"152\":1}}],[\"65b模型用了1\",{\"1\":{\"120\":1}}],[\"6g显存支持的对话长度由1k提升到了8k\",{\"1\":{\"146\":1}}],[\"6b的参数量如下所示\",{\"1\":{\"152\":2}}],[\"6b的总体架构如下所示\",{\"1\":{\"151\":2}}],[\"6b的2k扩展到了32k\",{\"1\":{\"146\":1}}],[\"6b权重对学术研究完全开放\",{\"1\":{\"146\":1}}],[\"6b有更高效的推理速度和更低的显存占用\",{\"1\":{\"146\":1}}],[\"6b对单轮超长文档的理解能力有限\",{\"1\":{\"146\":1}}],[\"6b\",{\"1\":{\"146\":1}}],[\"6b在mmlu\",{\"1\":{\"146\":1}}],[\"6b使用了glm的混合目标函数\",{\"1\":{\"146\":1}}],[\"69\",{\"1\":{\"52\":1}}],[\"6\",{\"0\":{\"45\":1,\"127\":1,\"197\":1},\"1\":{\"7\":1,\"19\":2,\"48\":1,\"107\":1,\"129\":1,\"152\":2,\"185\":1,\"187\":2,\"197\":1,\"232\":1}}],[\"64k\",{\"1\":{\"7\":2,\"8\":1}}],[\"radford等\",{\"1\":{\"219\":1}}],[\"range\",{\"1\":{\"137\":1}}],[\"rank\",{\"1\":{\"39\":1,\"40\":1}}],[\"rnn\",{\"1\":{\"215\":1,\"217\":1}}],[\"rθ​≈n1​τ∑​min\",{\"1\":{\"197\":1}}],[\"rθ​=eτ∼pθ\",{\"1\":{\"193\":1,\"194\":1,\"195\":1,\"196\":1}}],[\"rθ​=eτ∼pθ​\",{\"1\":{\"192\":1}}],[\"rθ​=τ∑​r\",{\"1\":{\"103\":1}}],[\"rm就是基于第一步生成的sft6b版本\",{\"1\":{\"163\":1}}],[\"rm\",{\"1\":{\"163\":1}}],[\"rmsnorm是对layernorm的一个改进\",{\"1\":{\"153\":1}}],[\"rmsnorm\",{\"1\":{\"151\":3}}],[\"rmsprop等方法调整\",{\"1\":{\"103\":1}}],[\"rotaryembedding\",{\"1\":{\"151\":2}}],[\"rotary\",{\"1\":{\"151\":2,\"152\":2}}],[\"roformer\",{\"1\":{\"66\":1}}],[\"rl\",{\"1\":{\"93\":1}}],[\"rlhf\",{\"1\":{\"7\":4,\"8\":1,\"162\":1}}],[\"recurrent\",{\"1\":{\"215\":1,\"217\":2}}],[\"recurrentgpt展示了从认知科学和深度学习中流行的模型设计中借鉴思想对llms进行提示的效用\",{\"1\":{\"218\":1}}],[\"recurrentgpt架构图\",{\"1\":{\"217\":1}}],[\"recurrentgpt通过自然语言模拟了循环神经网络\",{\"1\":{\"217\":1}}],[\"recurrentgpt生成一个段落的文本\",{\"1\":{\"217\":1}}],[\"recurrentgpt的语言模型是在大型语言模型\",{\"1\":{\"217\":1}}],[\"recurrentgpt原理\",{\"0\":{\"217\":1}}],[\"recurrentgpt\",{\"0\":{\"215\":1},\"1\":{\"215\":1,\"217\":6}}],[\"recall\",{\"1\":{\"82\":1}}],[\"reasoning\",{\"1\":{\"208\":1}}],[\"return\",{\"1\":{\"187\":1}}],[\"resid\",{\"1\":{\"183\":1,\"187\":1}}],[\"research\",{\"1\":{\"7\":1}}],[\"representations\",{\"1\":{\"165\":1}}],[\"repeat\",{\"0\":{\"118\":2},\"1\":{\"118\":2}}],[\"re\",{\"1\":{\"153\":1}}],[\"reward\",{\"1\":{\"112\":1,\"163\":1}}],[\"region\",{\"1\":{\"107\":1}}],[\"regressive\",{\"1\":{\"82\":1}}],[\"reinforcement\",{\"1\":{\"93\":1,\"161\":1,\"162\":1},\"2\":{\"98\":1,\"109\":1,\"116\":1}}],[\"remove\",{\"0\":{\"63\":1},\"1\":{\"63\":1}}],[\"requires\",{\"1\":{\"52\":2}}],[\"r\",{\"1\":{\"40\":3,\"103\":1,\"112\":1,\"192\":5}}],[\"fn=<viewbackward0>\",{\"1\":{\"187\":1}}],[\"fn=<nativebatchnormbackward0>\",{\"1\":{\"185\":1}}],[\"fn=<nativelayernormbackward0>\",{\"1\":{\"185\":1}}],[\"fn=<addbackward0>\",{\"1\":{\"52\":1}}],[\"f\",{\"1\":{\"183\":1,\"232\":1}}],[\"fc\",{\"1\":{\"183\":1}}],[\"fei\",{\"1\":{\"208\":1}}],[\"few\",{\"1\":{\"208\":1}}],[\"feedback\",{\"1\":{\"161\":1,\"162\":1,\"165\":1}}],[\"features=50257\",{\"1\":{\"183\":1}}],[\"features=768\",{\"1\":{\"183\":1}}],[\"features=65024\",{\"1\":{\"151\":1}}],[\"features=27392\",{\"1\":{\"151\":1}}],[\"features=4608\",{\"1\":{\"151\":1,\"156\":1}}],[\"features=4096\",{\"1\":{\"151\":12,\"156\":1}}],[\"features=13696\",{\"1\":{\"151\":1}}],[\"features=150528\",{\"1\":{\"151\":1}}],[\"features=16384\",{\"1\":{\"151\":2}}],[\"features=12288\",{\"1\":{\"151\":1}}],[\"ffn\",{\"1\":{\"82\":1}}],[\"fused\",{\"1\":{\"64\":2}}],[\"fusion\",{\"1\":{\"61\":1}}],[\"false\",{\"1\":{\"171\":4}}],[\"faiss\",{\"1\":{\"139\":1}}],[\"factual\",{\"1\":{\"82\":1}}],[\"face开源的peft库目前支持5种方法\",{\"1\":{\"39\":1}}],[\"face开源的一个高效微调大模型的库\",{\"1\":{\"38\":1}}],[\"face\",{\"1\":{\"8\":2},\"2\":{\"50\":1}}],[\"fastertransformer\",{\"1\":{\"63\":1}}],[\"from\",{\"0\":{\"118\":1},\"1\":{\"118\":1,\"161\":1,\"162\":1,\"183\":2,\"185\":1,\"202\":1}}],[\"france\",{\"1\":{\"107\":1}}],[\"framework下进行学习并在这些任务上取得不错的结果\",{\"1\":{\"8\":1}}],[\"free\",{\"1\":{\"61\":2,\"63\":1,\"65\":1}}],[\"flashattentio算法\",{\"1\":{\"155\":1}}],[\"flashattention循环遍历q矩阵的块\",{\"1\":{\"155\":1}}],[\"flashattention循环遍历k和v矩阵的块\",{\"1\":{\"155\":1}}],[\"flashattention原理示意图\",{\"1\":{\"155\":1}}],[\"flashattention主要是为了做训练提速的\",{\"1\":{\"155\":1}}],[\"flashattention\",{\"0\":{\"155\":1}}],[\"flan\",{\"1\":{\"7\":4,\"8\":1}}],[\"flops较大的模型性能会更好一点\",{\"1\":{\"126\":1}}],[\"flops\",{\"1\":{\"126\":1}}],[\"floating\",{\"1\":{\"126\":1}}],[\"float16\",{\"1\":{\"52\":3}}],[\"follow\",{\"1\":{\"162\":1,\"165\":1}}],[\"focus\",{\"1\":{\"162\":1}}],[\"foundation\",{\"1\":{\"26\":1}}],[\"forward\",{\"1\":{\"187\":1}}],[\"for\",{\"1\":{\"26\":1,\"39\":3,\"48\":1,\"204\":1,\"206\":1}}],[\"final\",{\"1\":{\"151\":2,\"152\":2}}],[\"finding\",{\"1\":{\"83\":1}}],[\"finetuned\",{\"1\":{\"165\":1}}],[\"finetune\",{\"2\":{\"58\":1}}],[\"finetuning更新所有参数的方式不同\",{\"1\":{\"43\":1}}],[\"fine\",{\"1\":{\"7\":1,\"38\":1,\"39\":2,\"162\":2}}],[\"filtering等概念\",{\"1\":{\"8\":1}}],[\"filtering\",{\"1\":{\"7\":1}}],[\"5所示\",{\"1\":{\"204\":1}}],[\"592=2\",{\"1\":{\"188\":1}}],[\"592\",{\"1\":{\"188\":1}}],[\"597\",{\"1\":{\"188\":2}}],[\"536\",{\"1\":{\"188\":1}}],[\"5系列进行训练\",{\"1\":{\"163\":1}}],[\"5系列已经训练完成\",{\"1\":{\"163\":1}}],[\"5系列的\",{\"1\":{\"163\":1}}],[\"56098816\",{\"1\":{\"152\":1}}],[\"562\",{\"1\":{\"152\":2}}],[\"5710903296\",{\"1\":{\"152\":1}}],[\"50257=38\",{\"1\":{\"188\":1}}],[\"50257\",{\"1\":{\"183\":1,\"188\":1}}],[\"50\",{\"1\":{\"137\":1,\"152\":1}}],[\"512\",{\"1\":{\"137\":1,\"183\":1}}],[\"5参数数量和flops在重复训练上的影响\",{\"0\":{\"126\":1}}],[\"520\",{\"1\":{\"152\":1}}],[\"528=616\",{\"1\":{\"152\":1}}],[\"528\",{\"1\":{\"152\":1}}],[\"52\",{\"1\":{\"26\":1,\"27\":1}}],[\"5\",{\"0\":{\"44\":1,\"165\":1,\"196\":1},\"1\":{\"7\":1,\"19\":2,\"34\":1,\"39\":1,\"48\":1,\"81\":2,\"86\":1,\"126\":1,\"143\":1,\"152\":1,\"155\":1,\"185\":3,\"187\":1,\"196\":1,\"202\":2,\"204\":1,\"232\":2}}],[\"55k\",{\"1\":{\"7\":1}}],[\"问题提出\",{\"0\":{\"119\":1,\"137\":1,\"216\":1}}],[\"问题是为何模型压缩能力越强\",{\"1\":{\"81\":1}}],[\"问题\",{\"1\":{\"7\":1}}],[\"name\",{\"1\":{\"204\":1}}],[\"natural\",{\"1\":{\"7\":2,\"8\":5}}],[\"nx均为768\",{\"1\":{\"187\":1}}],[\"nx+nf个\",{\"1\":{\"187\":1}}],[\"nx是构造参数\",{\"1\":{\"187\":1}}],[\"nx\",{\"1\":{\"187\":2}}],[\"nf\",{\"1\":{\"187\":6}}],[\"nn\",{\"1\":{\"185\":3,\"187\":5}}],[\"n注意力矩阵\",{\"1\":{\"155\":1}}],[\"normal\",{\"1\":{\"187\":1}}],[\"november\",{\"1\":{\"165\":1}}],[\"not\",{\"0\":{\"118\":1},\"1\":{\"118\":1}}],[\"noise\",{\"1\":{\"7\":1}}],[\"need\",{\"1\":{\"169\":1}}],[\"newgeluactivation\",{\"1\":{\"183\":1}}],[\"new\",{\"1\":{\"165\":1}}],[\"network\",{\"1\":{\"217\":1}}],[\"networks\",{\"1\":{\"83\":1}}],[\"net\",{\"1\":{\"165\":1}}],[\"neurips\",{\"1\":{\"165\":1}}],[\"neural\",{\"1\":{\"83\":1,\"165\":1,\"217\":1}}],[\"neurons\",{\"1\":{\"83\":1}}],[\"next\",{\"1\":{\"79\":2,\"82\":1,\"201\":1,\"202\":1,\"203\":4,\"204\":3,\"205\":3}}],[\"ntp\",{\"1\":{\"79\":2,\"81\":1,\"82\":1,\"201\":1,\"203\":1,\"205\":2}}],[\"n是层数\",{\"1\":{\"73\":1}}],[\"num\",{\"1\":{\"65\":3,\"171\":2}}],[\"nvidia\",{\"1\":{\"63\":1,\"65\":1}}],[\"nlg\",{\"1\":{\"43\":1}}],[\"nlp\",{\"1\":{\"8\":2}}],[\"n\",{\"1\":{\"40\":4,\"83\":2,\"155\":2,\"156\":2}}],[\"和上层\",{\"1\":{\"202\":1}}],[\"和gpt2模型的源码\",{\"1\":{\"184\":1}}],[\"和人类的决策相似\",{\"1\":{\"164\":1}}],[\"和值\",{\"1\":{\"156\":1}}],[\"和记忆变换网络\",{\"1\":{\"137\":1}}],[\"和sarsa的区别在于直接用下一步的最大q值作为估计来更新\",{\"1\":{\"113\":1}}],[\"和奖励r\",{\"1\":{\"96\":1}}],[\"和r\",{\"1\":{\"96\":1}}],[\"和t\",{\"1\":{\"52\":1}}],[\"和连续提示\",{\"1\":{\"42\":1}}],[\"和规模在100m\",{\"1\":{\"8\":1}}],[\"和1600个nlp任务\",{\"1\":{\"8\":1}}],[\"和法国\",{\"1\":{\"8\":1}}],[\"和\",{\"1\":{\"7\":1,\"48\":1,\"61\":1,\"83\":1,\"123\":2,\"143\":1,\"156\":1,\"203\":1,\"227\":1}}],[\"生成的内容更具备像小说那样的细节\",{\"1\":{\"217\":1}}],[\"生成式问答中的开放域任务可以从更大的输入中综合信息\",{\"1\":{\"137\":1}}],[\"生成式摘要任务\",{\"1\":{\"43\":1}}],[\"生成几组就是几个头\",{\"1\":{\"64\":1}}],[\"生成任务\",{\"1\":{\"43\":1,\"46\":1}}],[\"生成\",{\"1\":{\"7\":1}}],[\"生成了\",{\"1\":{\"7\":1}}],[\"small\",{\"1\":{\"204\":2,\"206\":1}}],[\"so\",{\"1\":{\"203\":8}}],[\"softmax\",{\"1\":{\"64\":2}}],[\"soft\",{\"1\":{\"42\":1,\"45\":2,\"46\":2,\"48\":2}}],[\"sn−k−1​\",{\"1\":{\"182\":1}}],[\"sn−k​\",{\"1\":{\"182\":1}}],[\"sn−1​\",{\"1\":{\"182\":1}}],[\"sn​∣s1​\",{\"1\":{\"182\":2}}],[\"sn​\",{\"1\":{\"182\":1}}],[\"s2​\",{\"1\":{\"182\":1}}],[\"s2​∣s1​\",{\"1\":{\"103\":2}}],[\"short\",{\"1\":{\"217\":1}}],[\"shot\",{\"1\":{\"86\":1,\"165\":1,\"208\":1}}],[\"shot测试数据进行测试\",{\"1\":{\"27\":1}}],[\"shared\",{\"1\":{\"171\":2}}],[\"shuming\",{\"1\":{\"165\":1}}],[\"systems\",{\"1\":{\"165\":1}}],[\"sft阶段\",{\"1\":{\"163\":1}}],[\"sft\",{\"1\":{\"162\":1}}],[\"s和o从hbm移动到sram\",{\"1\":{\"155\":1}}],[\"s和o的大小\",{\"1\":{\"155\":1}}],[\"sram容量小却有着较高的访问速度\",{\"1\":{\"155\":1}}],[\"sled\",{\"1\":{\"137\":1}}],[\"sarsa的目标策略是优化q值\",{\"1\":{\"114\":1}}],[\"sarsa策略更新\",{\"1\":{\"112\":1}}],[\"sarsa是on\",{\"1\":{\"112\":1}}],[\"sarsa伪代码\",{\"1\":{\"112\":1}}],[\"sarsa\",{\"0\":{\"112\":1},\"1\":{\"112\":1}}],[\"s1​\",{\"1\":{\"103\":2,\"182\":1}}],[\"s为回答问题之前的状态\",{\"1\":{\"102\":1}}],[\"s0​\",{\"1\":{\"95\":1}}],[\"s\",{\"1\":{\"95\":8,\"96\":7,\"102\":1,\"112\":2,\"155\":3,\"204\":1}}],[\"specifically\",{\"1\":{\"162\":1}}],[\"specific向量添加到input前面\",{\"1\":{\"43\":1}}],[\"sparse\",{\"1\":{\"83\":1}}],[\"seq2seq\",{\"1\":{\"137\":2,\"140\":2}}],[\"sequence\",{\"1\":{\"137\":2}}],[\"seqlen\",{\"1\":{\"63\":2,\"64\":3,\"65\":3}}],[\"sergey\",{\"1\":{\"107\":1}}],[\"selfattention\",{\"1\":{\"151\":2}}],[\"self\",{\"0\":{\"186\":1},\"1\":{\"7\":4,\"8\":1,\"61\":1,\"73\":1,\"151\":1,\"152\":5,\"171\":4,\"186\":3,\"187\":10}}],[\"size\",{\"1\":{\"63\":3,\"65\":6,\"156\":14,\"183\":1,\"187\":4}}],[\"svd\",{\"1\":{\"41\":1}}],[\"std=0\",{\"1\":{\"187\":1}}],[\"stn​分别代表第n条轨迹里时刻t的动作\",{\"1\":{\"103\":1}}],[\"st​\",{\"1\":{\"95\":1,\"103\":1,\"105\":2}}],[\"st+1​∣s1​\",{\"1\":{\"95\":1}}],[\"st+1​∣st​\",{\"1\":{\"95\":1,\"103\":2}}],[\"studies\",{\"1\":{\"83\":1}}],[\"state\",{\"1\":{\"40\":1,\"112\":2,\"216\":1}}],[\"stem\",{\"1\":{\"27\":1}}],[\"schuurmans\",{\"1\":{\"208\":1}}],[\"schulman\",{\"1\":{\"107\":1}}],[\"scaling\",{\"0\":{\"118\":1},\"1\":{\"118\":1}}],[\"scale\",{\"1\":{\"39\":1}}],[\"scales\",{\"1\":{\"39\":1}}],[\"science\",{\"1\":{\"27\":1}}],[\"sumanth\",{\"1\":{\"219\":1}}],[\"sui\",{\"1\":{\"165\":1}}],[\"suite\",{\"1\":{\"26\":1}}],[\"sun\",{\"1\":{\"165\":1}}],[\"supervised\",{\"1\":{\"161\":1,\"162\":1}}],[\"superposition\",{\"1\":{\"83\":4}}],[\"super\",{\"1\":{\"7\":2,\"8\":3,\"187\":1}}],[\"4给出的例子\",{\"1\":{\"204\":1}}],[\"4+768=2\",{\"1\":{\"188\":1}}],[\"4=2\",{\"1\":{\"188\":1}}],[\"4399\",{\"1\":{\"232\":2}}],[\"439\",{\"1\":{\"188\":1}}],[\"432+1536\",{\"1\":{\"188\":1}}],[\"432\",{\"1\":{\"188\":2}}],[\"4314\",{\"1\":{\"52\":2}}],[\"4142\",{\"1\":{\"185\":4}}],[\"400\",{\"1\":{\"152\":2}}],[\"4096=266\",{\"1\":{\"152\":1}}],[\"4096\",{\"1\":{\"151\":5,\"152\":3,\"156\":1}}],[\"4h\",{\"1\":{\"151\":4,\"152\":4}}],[\"42\",{\"1\":{\"146\":1}}],[\"4t\",{\"1\":{\"146\":1}}],[\"4k\",{\"1\":{\"142\":2}}],[\"4x\",{\"1\":{\"123\":1}}],[\"4万亿token\",{\"1\":{\"120\":1}}],[\"4高\",{\"1\":{\"86\":1}}],[\"4820\",{\"1\":{\"52\":1}}],[\"4548\",{\"1\":{\"52\":1}}],[\"4753\",{\"1\":{\"52\":1}}],[\"477\",{\"1\":{\"16\":1}}],[\"4608\",{\"1\":{\"156\":2}}],[\"46种语言的多语言prompt数据\",{\"1\":{\"8\":1}}],[\"46\",{\"1\":{\"8\":1,\"165\":1}}],[\"4\",{\"0\":{\"19\":1,\"43\":1,\"48\":1,\"106\":1,\"125\":1,\"133\":1,\"154\":1,\"157\":1,\"164\":1,\"178\":1,\"188\":1,\"195\":1,\"219\":1,\"233\":1},\"1\":{\"7\":1,\"8\":1,\"16\":1,\"19\":2,\"33\":1,\"39\":1,\"48\":1,\"52\":2,\"77\":2,\"125\":1,\"142\":1,\"146\":1,\"152\":10,\"163\":1,\"185\":1,\"187\":1,\"188\":1,\"195\":1,\"204\":1,\"227\":1,\"231\":1,\"232\":2}}],[\"等大语言模型广泛应用于长内容生成的关键障碍\",{\"1\":{\"216\":1}}],[\"等方式缓解\",{\"1\":{\"216\":1}}],[\"等于r\",{\"1\":{\"194\":1}}],[\"等数据集上的性能取得了大幅度的提升\",{\"1\":{\"146\":1}}],[\"等数据上进行微调的\",{\"1\":{\"7\":1}}],[\"等库对数据存储中的编码输入进行索引\",{\"1\":{\"139\":1}}],[\"等强长程\",{\"1\":{\"137\":1}}],[\"等都是常用的防止过拟合的技术\",{\"1\":{\"129\":1}}],[\"等等\",{\"1\":{\"66\":1}}],[\"等操作\",{\"1\":{\"64\":1}}],[\"等联合组织\",{\"1\":{\"8\":1}}],[\"等\",{\"1\":{\"7\":1,\"215\":1,\"219\":1}}],[\"等概念被引入\",{\"1\":{\"7\":1}}],[\"等模型\",{\"1\":{\"7\":1}}],[\"pth\",{\"1\":{\"203\":1}}],[\"p=softmax\",{\"1\":{\"155\":1}}],[\"p=0\",{\"1\":{\"151\":1,\"183\":4,\"187\":2}}],[\"pdf\",{\"1\":{\"137\":2}}],[\"philipp\",{\"1\":{\"107\":1}}],[\"pieter\",{\"1\":{\"107\":1}}],[\"ppo裁剪实现的功能和kl惩罚一样\",{\"1\":{\"197\":1}}],[\"ppo裁剪\",{\"0\":{\"197\":1}}],[\"ppo的主要思想是\",{\"1\":{\"191\":1}}],[\"ppo阶段\",{\"1\":{\"163\":1}}],[\"ppo\",{\"0\":{\"106\":1,\"191\":1},\"1\":{\"191\":1}}],[\"pθ\",{\"1\":{\"105\":1,\"193\":3,\"194\":1,\"195\":1,\"196\":1,\"197\":2}}],[\"pθ​\",{\"1\":{\"103\":4,\"105\":1,\"192\":2,\"193\":3,\"194\":1,\"195\":1,\"196\":1,\"197\":2}}],[\"penalty\",{\"1\":{\"196\":1}}],[\"perform\",{\"1\":{\"165\":1}}],[\"pearl\",{\"0\":{\"86\":1},\"1\":{\"86\":2}}],[\"peft分类\",{\"0\":{\"39\":1}}],[\"peft能够将预训练的语言模型\",{\"1\":{\"38\":1}}],[\"peft定义\",{\"0\":{\"38\":1}}],[\"peft方法仅微调少量\",{\"1\":{\"37\":1,\"38\":1}}],[\"peft\",{\"0\":{\"37\":1},\"1\":{\"37\":2,\"38\":1},\"2\":{\"50\":1}}],[\"pair\",{\"1\":{\"226\":1}}],[\"patching\",{\"1\":{\"203\":1}}],[\"path\",{\"1\":{\"203\":1}}],[\"past\",{\"1\":{\"174\":1}}],[\"pamela\",{\"1\":{\"165\":1}}],[\"palm\",{\"1\":{\"129\":1}}],[\"paramshare\",{\"1\":{\"126\":1}}],[\"parameter\",{\"1\":{\"38\":1,\"39\":2,\"187\":2}}],[\"padding\",{\"0\":{\"63\":1},\"1\":{\"61\":3,\"63\":2,\"65\":1}}],[\"pytorch\",{\"1\":{\"61\":1}}],[\"psedo\",{\"1\":{\"46\":1}}],[\"p2\",{\"1\":{\"45\":2}}],[\"p1\",{\"1\":{\"45\":2}}],[\"pφ0​+δφ\",{\"1\":{\"40\":1}}],[\"pφ​\",{\"1\":{\"40\":1}}],[\"pos\",{\"1\":{\"151\":1,\"152\":1}}],[\"post\",{\"1\":{\"151\":2,\"152\":2}}],[\"point\",{\"1\":{\"126\":1}}],[\"policy算法\",{\"1\":{\"114\":2}}],[\"policy的概念\",{\"1\":{\"114\":1}}],[\"policy的强化学习方法\",{\"1\":{\"112\":1}}],[\"policy和off\",{\"0\":{\"114\":1},\"1\":{\"114\":1}}],[\"policy\",{\"0\":{\"114\":1},\"1\":{\"100\":1,\"107\":1,\"191\":1},\"2\":{\"109\":1}}],[\"polysemanticity\",{\"1\":{\"83\":1}}],[\"power\",{\"1\":{\"39\":1}}],[\"pool和quality\",{\"1\":{\"8\":1}}],[\"pool\",{\"1\":{\"7\":1}}],[\"p\",{\"0\":{\"45\":1},\"1\":{\"39\":4,\"45\":7,\"46\":3,\"48\":3,\"77\":1,\"103\":3,\"182\":1,\"200\":1},\"2\":{\"50\":1}}],[\"plm时\",{\"1\":{\"38\":1}}],[\"plm\",{\"1\":{\"37\":2,\"38\":1}}],[\"print\",{\"1\":{\"183\":1,\"185\":2,\"187\":1}}],[\"primera\",{\"1\":{\"137\":1,\"143\":1}}],[\"preprint\",{\"1\":{\"206\":2}}],[\"previous\",{\"1\":{\"203\":1}}],[\"pretrained\",{\"1\":{\"183\":1}}],[\"press\",{\"1\":{\"165\":1}}],[\"prediction\",{\"1\":{\"79\":2,\"204\":1}}],[\"pre\",{\"0\":{\"70\":1},\"1\":{\"202\":1,\"206\":1,\"217\":2}}],[\"prefetch\",{\"1\":{\"65\":2}}],[\"prefix不是真实的\",{\"1\":{\"46\":1}}],[\"prefix参数进行微调\",{\"1\":{\"45\":1}}],[\"prefix为前缀\",{\"1\":{\"45\":1}}],[\"prefix只加在句首\",{\"1\":{\"43\":1}}],[\"prefix\",{\"0\":{\"43\":1},\"1\":{\"39\":4,\"43\":12,\"45\":3,\"46\":4,\"48\":3},\"2\":{\"50\":1}}],[\"proximal\",{\"1\":{\"191\":1}}],[\"proj\",{\"1\":{\"183\":2,\"187\":1}}],[\"projection\",{\"1\":{\"61\":1}}],[\"processing\",{\"1\":{\"165\":1}}],[\"proceedings\",{\"1\":{\"107\":1,\"165\":1}}],[\"probing\",{\"1\":{\"83\":1}}],[\"problem\",{\"1\":{\"65\":2}}],[\"prompting\",{\"1\":{\"208\":2,\"215\":1}}],[\"prompting最初由人工设计prompt\",{\"1\":{\"45\":1}}],[\"prompt范式第二阶段｜prefix\",{\"1\":{\"48\":1}}],[\"prompt综述\",{\"1\":{\"48\":1}}],[\"prompt比较依靠模型参数量\",{\"1\":{\"45\":1}}],[\"prompt是只作用在embedding层中\",{\"1\":{\"45\":1}}],[\"prompt的一种改进\",{\"1\":{\"45\":1}}],[\"prompt的制作分为手工创建prompt和自动化生成prompt\",{\"1\":{\"42\":1}}],[\"prompt两种\",{\"1\":{\"42\":1}}],[\"prompt与soft\",{\"1\":{\"42\":1}}],[\"prompt分为hard\",{\"1\":{\"42\":1}}],[\"prompt分类\",{\"0\":{\"42\":1}}],[\"prompts\",{\"1\":{\"39\":1,\"42\":1,\"48\":1}}],[\"promptsource\",{\"1\":{\"8\":5}}],[\"prompt数据\",{\"1\":{\"8\":1}}],[\"prompt\",{\"0\":{\"8\":1,\"44\":1},\"1\":{\"7\":4,\"8\":7,\"39\":5,\"42\":3,\"43\":1,\"44\":4,\"46\":7,\"48\":6,\"201\":1,\"202\":3,\"217\":3},\"2\":{\"10\":1,\"50\":1,\"213\":1}}],[\"prakharguptaz\",{\"1\":{\"7\":2,\"8\":1}}],[\"p3\",{\"1\":{\"7\":2,\"8\":8,\"45\":1}}],[\"中给\",{\"1\":{\"217\":1}}],[\"中存在以\",{\"1\":{\"204\":1}}],[\"中存在很多单个的神经元\",{\"1\":{\"83\":1}}],[\"中影响最大的\",{\"1\":{\"202\":1}}],[\"中间层的\",{\"1\":{\"202\":1}}],[\"中英标识符的预训练与人类偏好对齐训练\",{\"1\":{\"146\":1}}],[\"中读取子问题参数进行了性能优化\",{\"1\":{\"65\":1}}],[\"中进行计算\",{\"1\":{\"65\":1}}],[\"中每个注意力头都会从全部输入中选择一个单独的上下文窗口\",{\"1\":{\"138\":1}}],[\"中每个\",{\"1\":{\"65\":1}}],[\"中的间接对象识别回路\",{\"1\":{\"204\":1}}],[\"中的一些关键神经元完成数学运算的\",{\"1\":{\"202\":1}}],[\"中的神经元被称为\",{\"1\":{\"83\":1}}],[\"中的分布\",{\"0\":{\"83\":1}}],[\"中的两次矩阵乘操作\",{\"1\":{\"65\":1}}],[\"中的\",{\"1\":{\"65\":1,\"203\":1}}],[\"中完成多个独立矩阵乘问题的计算\",{\"1\":{\"65\":1}}],[\"中完成所有操作\",{\"1\":{\"64\":1}}],[\"中实现了融合的多头注意力\",{\"1\":{\"64\":1}}],[\"中也有集成\",{\"1\":{\"63\":1}}],[\"中选择了具有挑战性的数学\",{\"1\":{\"27\":1}}],[\"中考\",{\"1\":{\"17\":1}}],[\"中\",{\"1\":{\"7\":1,\"18\":1,\"64\":1,\"66\":1,\"137\":1,\"140\":1,\"155\":1}}],[\"在merges\",{\"1\":{\"231\":1}}],[\"在线演示界面\",{\"1\":{\"218\":1}}],[\"在线演示\",{\"0\":{\"218\":1}}],[\"在实际使用中\",{\"1\":{\"217\":1}}],[\"在实际场景中\",{\"1\":{\"61\":1}}],[\"在提出要求后\",{\"1\":{\"217\":1}}],[\"在提取这条知识的时候\",{\"1\":{\"82\":1}}],[\"在模型内部建立起两类知识体系\",{\"1\":{\"205\":1}}],[\"在模型学习过程中\",{\"1\":{\"83\":1}}],[\"在输出\",{\"1\":{\"204\":1}}],[\"在传播过程中不断进行信息传递或知识加工\",{\"1\":{\"201\":1}}],[\"在网络中存在一些完成这个任务的关键路径\",{\"1\":{\"201\":1}}],[\"在理想情况\",{\"1\":{\"195\":1}}],[\"在加入这个限制的bpe算法下gpt2tokenizer诞生了\",{\"1\":{\"230\":1}}],[\"在加入重要性采样之后\",{\"1\":{\"195\":1}}],[\"在加入lora之前\",{\"1\":{\"40\":1}}],[\"在进行生成任务时\",{\"1\":{\"174\":1}}],[\"在进入交叉注意力模块之前\",{\"1\":{\"138\":1}}],[\"在定义时就给出了\",{\"1\":{\"171\":1}}],[\"在一次调用中教会它数学题\",{\"1\":{\"164\":1}}],[\"在一些场景下对能耗和时间的要求\",{\"1\":{\"51\":1}}],[\"在外部循环\",{\"1\":{\"155\":1}}],[\"在获得官方的书面许可后\",{\"1\":{\"146\":1}}],[\"在官方的模型实现下\",{\"1\":{\"146\":1}}],[\"在同尺寸开源模型中具有较强的竞争力\",{\"1\":{\"146\":1}}],[\"在图\",{\"1\":{\"142\":1}}],[\"在计算和\",{\"1\":{\"140\":1}}],[\"在标准的交叉注意力机制中\",{\"1\":{\"140\":1}}],[\"在每一个时间步中\",{\"1\":{\"217\":1}}],[\"在每个时间步骤\",{\"1\":{\"217\":1}}],[\"在每个更新步骤中\",{\"1\":{\"191\":1}}],[\"在每个注意力头都有单独的线性层用于k和v矩阵\",{\"1\":{\"156\":1}}],[\"在每个块中\",{\"1\":{\"155\":1}}],[\"在每个解码器层中的每个注意力头中选一组\",{\"1\":{\"138\":1}}],[\"在每个解码步骤中\",{\"1\":{\"138\":1}}],[\"在每层transformer\",{\"1\":{\"46\":1}}],[\"在解码过程中\",{\"1\":{\"138\":1}}],[\"在各种长程\",{\"1\":{\"137\":1}}],[\"在不久的将来\",{\"1\":{\"133\":1}}],[\"在不同教育阶段会包含相同的学科\",{\"1\":{\"17\":1}}],[\"在后续的迭代中使用dropout也是有效的\",{\"1\":{\"130\":1}}],[\"在前面的讨论中\",{\"1\":{\"130\":1}}],[\"在目前超过100亿参数规模的大语言模型中\",{\"1\":{\"129\":1}}],[\"在大语言模型的训练中\",{\"1\":{\"127\":1}}],[\"在c4数据集和wikipedia数据集上分别训练模型的结果\",{\"1\":{\"125\":1}}],[\"在chatgpt中参数为θ的神经网络对应rl微调的sft模型\",{\"1\":{\"102\":1}}],[\"在没有足够大的数据集的情况下进行训练时\",{\"1\":{\"123\":1}}],[\"在没有加额外层的情况下\",{\"1\":{\"44\":1}}],[\"在重复数据集上训练多次对模型的影响目前还没有一个相对完善的研究\",{\"1\":{\"120\":1}}],[\"在2023年\",{\"1\":{\"120\":1}}],[\"在此前的研究中\",{\"1\":{\"120\":1}}],[\"在动作概率分布中采样动作\",{\"1\":{\"102\":1}}],[\"在它们的响应之上做个线性组合\",{\"1\":{\"83\":1}}],[\"在推理过程中\",{\"1\":{\"82\":1,\"156\":1}}],[\"在训练过程中逐渐使用dropout是有效的策略\",{\"0\":{\"130\":1}}],[\"在训练基座模型的时候\",{\"1\":{\"79\":1}}],[\"在训练损失中增加了额外的惩罚\",{\"1\":{\"41\":1}}],[\"在预训练语言模型中解释数学能力\",{\"1\":{\"202\":1}}],[\"在预训练之后\",{\"1\":{\"74\":1}}],[\"在预训练阶段已经使用了部分指令数据\",{\"1\":{\"19\":1}}],[\"在作者的实验中\",{\"1\":{\"73\":1}}],[\"在只训练1个epoch的情况下\",{\"1\":{\"47\":1}}],[\"在参数量超过10b的模型上\",{\"1\":{\"45\":1}}],[\"在transformer的结构上已经近乎没有什么区别\",{\"1\":{\"178\":1}}],[\"在transformer\",{\"1\":{\"46\":1}}],[\"在t5类的encoder\",{\"1\":{\"43\":1}}],[\"在text\",{\"1\":{\"8\":1}}],[\"在gpt类的自回归模型上采用\",{\"1\":{\"43\":1}}],[\"在下游微调时\",{\"1\":{\"43\":1}}],[\"在现有的矩阵近似文献中\",{\"1\":{\"41\":1}}],[\"在增量矩阵之间动态地分配参数预算\",{\"1\":{\"41\":1}}],[\"在attention层的两个conv1d之间\",{\"1\":{\"187\":1}}],[\"在adalora中\",{\"1\":{\"41\":1}}],[\"在albert中\",{\"1\":{\"40\":2}}],[\"在微调大型\",{\"1\":{\"38\":1}}],[\"在这些超长输入的情况下\",{\"1\":{\"137\":1}}],[\"在这个基础上\",{\"1\":{\"196\":1}}],[\"在这个实验中\",{\"1\":{\"124\":1}}],[\"在这个项目中将自己的\",{\"1\":{\"7\":1}}],[\"在这样的步骤下\",{\"1\":{\"102\":1}}],[\"在这方面\",{\"1\":{\"37\":1}}],[\"在零样本评估的结果好于少样本评估结果\",{\"1\":{\"19\":1}}],[\"在零样本评估中\",{\"1\":{\"19\":2}}],[\"在零样本设置条件下\",{\"1\":{\"18\":1}}],[\"在少样本设置条件下\",{\"1\":{\"18\":1}}],[\"在语文学科中\",{\"1\":{\"17\":1}}],[\"在特定的一种任务类型\",{\"1\":{\"8\":1}}],[\"在英语\",{\"1\":{\"8\":1}}],[\"在promptsource基础上\",{\"1\":{\"8\":1}}],[\"在对话指令数据上微调后\",{\"1\":{\"7\":1}}],[\"在\",{\"1\":{\"7\":1,\"8\":3,\"18\":1,\"40\":1,\"63\":1,\"82\":1,\"86\":1,\"137\":1,\"162\":1}}],[\"论文题目\",{\"1\":{\"208\":1}}],[\"论文名称\",{\"1\":{\"181\":1}}],[\"论文相关的tensorflow的代码可以从github获取\",{\"1\":{\"169\":1}}],[\"论文中icl的测试数据\",{\"1\":{\"164\":1}}],[\"论文中显示\",{\"1\":{\"7\":1}}],[\"论文通过实验证明\",{\"1\":{\"153\":1}}],[\"论文链接\",{\"1\":{\"137\":1}}],[\"论文地址\",{\"1\":{\"61\":1}}],[\"论文信息\",{\"1\":{\"61\":1}}],[\"论文没有精确计算svd\",{\"1\":{\"41\":1}}],[\"论文提出了一套优化算法\",{\"1\":{\"60\":1}}],[\"论文提出了一种新的方法\",{\"1\":{\"41\":1}}],[\"论文提出了字节跳动的gpu\",{\"1\":{\"60\":1}}],[\"论文提出了两种重要性度量的方式\",{\"1\":{\"41\":1}}],[\"论文作者团队从中国真实的\",{\"1\":{\"27\":1}}],[\"论文\",{\"1\":{\"26\":1,\"82\":1,\"202\":1,\"204\":1}}],[\"论文分享\",{\"0\":{\"3\":1},\"2\":{\"5\":1}}],[\"条件概率p使用具有参数θ的神经网络来建模\",{\"1\":{\"73\":1}}],[\"条\",{\"1\":{\"7\":1,\"8\":1}}],[\"的循环计算机制\",{\"1\":{\"217\":1}}],[\"的模型\",{\"1\":{\"217\":1}}],[\"的模型对应专门采样的另一个sft模型\",{\"1\":{\"102\":1}}],[\"的大语言模型最明显的限制之一就是输入和输出的长度限制\",{\"1\":{\"216\":1}}],[\"的降低\",{\"1\":{\"205\":1}}],[\"的注意力中删除或者抑制重复出现的名字\",{\"1\":{\"204\":1}}],[\"的知识回路\",{\"1\":{\"204\":1}}],[\"的目的\",{\"1\":{\"203\":1}}],[\"的位置发生作用\",{\"1\":{\"204\":1}}],[\"的位置\",{\"1\":{\"203\":1}}],[\"的内在工作机制\",{\"1\":{\"203\":1}}],[\"的时候\",{\"1\":{\"203\":3}}],[\"的年份数字\",{\"1\":{\"202\":1}}],[\"的不相似程度\",{\"1\":{\"196\":1}}],[\"的kl散度\",{\"1\":{\"196\":1}}],[\"的分布有差异会带来估算结果差异很大的问题\",{\"1\":{\"195\":1}}],[\"的无监督分布估计\",{\"1\":{\"182\":1}}],[\"的解码器部分构成\",{\"1\":{\"181\":1}}],[\"的解码器关注编码器的最终隐状态\",{\"1\":{\"140\":1}}],[\"的使用需要用户从一开始传入encoder层的结果\",{\"1\":{\"175\":1}}],[\"的部分对应了davinci\",{\"1\":{\"162\":1}}],[\"的论文\",{\"1\":{\"162\":1}}],[\"的提出来自于google的一篇论文\",{\"1\":{\"162\":1}}],[\"的提示\",{\"1\":{\"43\":2}}],[\"的两个矩阵\",{\"1\":{\"156\":1}}],[\"的上下文长度训练\",{\"1\":{\"146\":1}}],[\"的上下文窗口长\",{\"1\":{\"137\":1}}],[\"的基座模型\",{\"1\":{\"146\":1}}],[\"的基础上构建的\",{\"1\":{\"217\":1}}],[\"的基础上\",{\"1\":{\"8\":1}}],[\"的训练方法中\",{\"1\":{\"142\":1}}],[\"的输出\",{\"1\":{\"217\":1}}],[\"的输出的中间一半\",{\"1\":{\"139\":1}}],[\"的输入层开始\",{\"1\":{\"205\":1}}],[\"的输入\",{\"1\":{\"137\":1}}],[\"的方法对输入的重叠块进行编码\",{\"1\":{\"139\":1}}],[\"的方式实现\",{\"1\":{\"64\":1}}],[\"的最大输入长度受到限制\",{\"1\":{\"138\":1}}],[\"的最后一个位置\",{\"1\":{\"82\":1}}],[\"的隐藏状态上构建一个数据存储\",{\"1\":{\"137\":1}}],[\"的个上下文窗口\",{\"1\":{\"137\":1}}],[\"的数量对模型的性能有很大的影响\",{\"1\":{\"120\":1}}],[\"的数据规模在\",{\"1\":{\"8\":1}}],[\"的取值\",{\"1\":{\"96\":1}}],[\"的关键技术之一\",{\"1\":{\"93\":1}}],[\"的信息来源则比较多\",{\"1\":{\"202\":1}}],[\"的信息\",{\"1\":{\"82\":1}}],[\"的信息集成到最后位置\",{\"1\":{\"82\":1}}],[\"的低层\",{\"1\":{\"82\":2}}],[\"的效果\",{\"1\":{\"81\":1}}],[\"的智能水准\",{\"1\":{\"81\":1}}],[\"的标准\",{\"1\":{\"66\":1}}],[\"的前缀和\",{\"1\":{\"63\":1}}],[\"的每一行四舍五入到整型之后最大值为127或者最小值为−127即可\",{\"1\":{\"52\":1}}],[\"的每一层之前都加入了soft\",{\"1\":{\"46\":1}}],[\"的每行乘以一个系数\",{\"1\":{\"52\":1}}],[\"的轻量微调\",{\"1\":{\"43\":1}}],[\"的情况下\",{\"1\":{\"40\":1}}],[\"的成本通常高得令人望而却步\",{\"1\":{\"37\":1}}],[\"的差距过大\",{\"1\":{\"196\":1}}],[\"的差距\",{\"1\":{\"19\":1}}],[\"的预训练语言模型\",{\"1\":{\"19\":1}}],[\"的llms的rlhf数据集\",{\"1\":{\"8\":1}}],[\"的指令数据\",{\"1\":{\"8\":1}}],[\"的框架中\",{\"1\":{\"8\":1}}],[\"的框架中加入了\",{\"1\":{\"8\":1}}],[\"的生成流程\",{\"1\":{\"7\":1}}],[\"的思路\",{\"1\":{\"7\":1}}],[\"的\",{\"1\":{\"7\":2,\"8\":1,\"61\":1,\"65\":1,\"82\":1,\"83\":1,\"142\":2,\"202\":1,\"203\":1,\"204\":1}}],[\"的主要竞品之一\",{\"1\":{\"7\":1}}],[\"guu\",{\"1\":{\"165\":1}}],[\"glu\",{\"1\":{\"151\":1}}],[\"glm\",{\"2\":{\"159\":1}}],[\"glmtransformer\",{\"1\":{\"151\":1}}],[\"glmblock\",{\"1\":{\"151\":2}}],[\"glm130b\",{\"1\":{\"19\":2}}],[\"gsm8k\",{\"1\":{\"146\":1}}],[\"gpu内存使用量都会增加\",{\"1\":{\"156\":1}}],[\"gpu有40\",{\"1\":{\"155\":1}}],[\"gpu中存储单元主要有hbm和sram\",{\"1\":{\"155\":1}}],[\"gpu\",{\"1\":{\"137\":1,\"140\":1}}],[\"gpt中知识回路存在的证据\",{\"0\":{\"201\":1}}],[\"gpt模型的细节\",{\"1\":{\"183\":1}}],[\"gpt模型对知识的提取归纳过程示意图\",{\"1\":{\"82\":1}}],[\"gpt2tokenizer同时也是gpt3的tokenizer\",{\"1\":{\"229\":1}}],[\"gpt2tokenizer\",{\"0\":{\"229\":1}}],[\"gpt2模型总架构图\",{\"1\":{\"184\":1}}],[\"gpt2论文给出的模型架构改动\",{\"1\":{\"184\":1}}],[\"gpt2论文分享与架构分析\",{\"0\":{\"181\":1}}],[\"gpt2mlp\",{\"1\":{\"183\":1}}],[\"gpt2model\",{\"1\":{\"183\":1}}],[\"gpt2attention\",{\"0\":{\"187\":1},\"1\":{\"183\":1,\"187\":1}}],[\"gpt2block\",{\"1\":{\"183\":1}}],[\"gpt2lmheadmodel\",{\"1\":{\"183\":3}}],[\"gpt2\",{\"0\":{\"175\":1},\"1\":{\"183\":1}}],[\"gpt首先根据演示示例生成元梯度\",{\"1\":{\"164\":1}}],[\"gpt系列模型树\",{\"1\":{\"161\":1}}],[\"gpt系列模型发展历程\",{\"0\":{\"161\":1}}],[\"gpt对知识的提取与存储方式\",{\"1\":{\"77\":1}}],[\"gpt架构图\",{\"1\":{\"71\":1}}],[\"gpt论文分享\",{\"0\":{\"70\":1}}],[\"gpt\",{\"0\":{\"82\":1},\"1\":{\"19\":1,\"39\":1,\"43\":1,\"77\":1,\"81\":1,\"82\":3,\"162\":1,\"165\":1,\"181\":1,\"202\":5,\"203\":2,\"204\":3,\"205\":4,\"206\":2,\"219\":1},\"2\":{\"190\":1}}],[\"gpt3\",{\"1\":{\"7\":1,\"81\":1}}],[\"galactica\",{\"1\":{\"125\":1}}],[\"gemm\",{\"0\":{\"65\":1},\"1\":{\"64\":3,\"65\":6}}],[\"generative\",{\"0\":{\"70\":1},\"1\":{\"217\":2}}],[\"generation\",{\"0\":{\"215\":1},\"1\":{\"39\":1,\"48\":1,\"219\":2}}],[\"genci\",{\"1\":{\"8\":1}}],[\"greater\",{\"1\":{\"202\":1,\"206\":1}}],[\"grouped\",{\"0\":{\"65\":1},\"1\":{\"64\":1,\"65\":6}}],[\"grounding\",{\"1\":{\"8\":2}}],[\"gram都不是merge词对为止\",{\"1\":{\"231\":1}}],[\"gram\",{\"1\":{\"231\":1}}],[\"gradient\",{\"1\":{\"165\":1}}],[\"gradients\",{\"1\":{\"100\":1}}],[\"grad\",{\"1\":{\"52\":1,\"185\":2,\"187\":1}}],[\"grad=true\",{\"1\":{\"52\":2}}],[\"grained\",{\"1\":{\"7\":1}}],[\"google\",{\"1\":{\"7\":3,\"8\":1,\"208\":1},\"2\":{\"167\":1}}],[\"github\",{\"1\":{\"7\":4,\"8\":1,\"15\":1,\"37\":1,\"61\":1,\"66\":1,\"86\":1}}],[\"领域较为活跃的一个方向\",{\"1\":{\"7\":1}}],[\"lstm\",{\"1\":{\"215\":1,\"217\":1}}],[\"ln是对hidden的维度去做归一化\",{\"1\":{\"185\":1}}],[\"ln\",{\"0\":{\"185\":1},\"1\":{\"183\":3,\"185\":2}}],[\"ln层被移动到每个子block的输入端\",{\"1\":{\"183\":1}}],[\"l\",{\"1\":{\"165\":1}}],[\"lm\",{\"1\":{\"151\":1,\"152\":1,\"183\":1,\"188\":1}}],[\"lm的参数被冻结\",{\"1\":{\"43\":1}}],[\"li\",{\"1\":{\"165\":1}}],[\"linear\",{\"1\":{\"151\":6,\"156\":1,\"183\":1}}],[\"liu\",{\"1\":{\"137\":1,\"206\":1}}],[\"lille\",{\"1\":{\"107\":1}}],[\"llama\",{\"0\":{\"177\":1}}],[\"llama等\",{\"1\":{\"129\":1}}],[\"llm中的信息回路\",{\"0\":{\"200\":1}}],[\"llm的信息压缩能力与其智能水平的关系\",{\"1\":{\"77\":1}}],[\"llm的信息压缩能力与知识存储方式分享\",{\"0\":{\"77\":1}}],[\"llm如何重映现实世界\",{\"0\":{\"77\":1,\"200\":1}}],[\"llm\",{\"0\":{\"80\":1,\"118\":1},\"1\":{\"44\":1,\"77\":2,\"79\":1,\"81\":5,\"83\":4,\"118\":1,\"122\":1,\"123\":2,\"204\":1,\"217\":1},\"2\":{\"85\":1,\"88\":1,\"91\":1,\"210\":1,\"221\":1}}],[\"llms\",{\"1\":{\"7\":3,\"8\":1}}],[\"loss\",{\"1\":{\"205\":1}}],[\"localizing\",{\"1\":{\"203\":1}}],[\"louisiana\",{\"1\":{\"165\":1}}],[\"long\",{\"0\":{\"215\":1},\"1\":{\"137\":1,\"165\":1,\"217\":1,\"219\":1}}],[\"longformer\",{\"1\":{\"137\":3}}],[\"low\",{\"1\":{\"39\":1}}],[\"lora的微调质量与全模型微调相当\",{\"1\":{\"40\":1}}],[\"lora的做法是\",{\"1\":{\"40\":1}}],[\"lora新增的参数是δ\",{\"1\":{\"40\":1}}],[\"lora也是类似的思想\",{\"1\":{\"40\":1}}],[\"lora冻结预训练模型权重\",{\"1\":{\"40\":1}}],[\"lora\",{\"0\":{\"40\":1},\"1\":{\"39\":2,\"40\":1},\"2\":{\"50\":1}}],[\"lasted\",{\"1\":{\"202\":1}}],[\"last\",{\"1\":{\"201\":1,\"203\":1}}],[\"layer\",{\"1\":{\"151\":1,\"152\":1}}],[\"layernorm\",{\"1\":{\"151\":9,\"152\":6,\"183\":3,\"185\":1}}],[\"layers\",{\"1\":{\"151\":2,\"152\":23,\"171\":2}}],[\"language\",{\"0\":{\"70\":1},\"1\":{\"39\":1,\"40\":1,\"82\":1,\"122\":1,\"162\":1,\"165\":3,\"181\":1,\"202\":1,\"206\":1,\"208\":1,\"217\":1,\"219\":1}}],[\"large\",{\"1\":{\"39\":1,\"40\":1,\"122\":1,\"123\":1,\"208\":1,\"217\":1}}],[\"latex\",{\"1\":{\"27\":1}}],[\"lab\",{\"1\":{\"2\":1,\"15\":1}}],[\"leaked\",{\"1\":{\"219\":1}}],[\"learn\",{\"1\":{\"165\":1}}],[\"learners\",{\"1\":{\"165\":1,\"181\":1,\"219\":1}}],[\"learning是off\",{\"1\":{\"114\":1}}],[\"learning策略更新\",{\"1\":{\"113\":1}}],[\"learning同样根据下一步的状态更新q值\",{\"1\":{\"113\":1}}],[\"learning伪代码\",{\"1\":{\"113\":1}}],[\"learning算法的目标策略是优化下一步的q表中的最大值\",{\"1\":{\"114\":1}}],[\"learning算法\",{\"1\":{\"111\":1}}],[\"learning\",{\"0\":{\"113\":1},\"1\":{\"18\":1,\"93\":1,\"107\":1,\"161\":1,\"162\":1,\"164\":1,\"165\":1},\"2\":{\"98\":1,\"109\":1,\"116\":1,\"167\":1}}],[\"le\",{\"1\":{\"208\":1}}],[\"lester\",{\"1\":{\"165\":1}}],[\"lewis\",{\"1\":{\"137\":1}}],[\"length\",{\"1\":{\"137\":1,\"146\":1}}],[\"levine\",{\"1\":{\"107\":1}}],[\"level\",{\"1\":{\"26\":1}}],[\"360\",{\"1\":{\"188\":2}}],[\"368+4\",{\"1\":{\"188\":1}}],[\"368+2\",{\"1\":{\"188\":1}}],[\"368\",{\"1\":{\"188\":2}}],[\"362\",{\"1\":{\"188\":4}}],[\"3652\",{\"1\":{\"52\":1}}],[\"3=1\",{\"1\":{\"188\":1}}],[\"3037\",{\"1\":{\"187\":1}}],[\"304\",{\"1\":{\"152\":3}}],[\"3论文\",{\"1\":{\"161\":1}}],[\"392\",{\"1\":{\"152\":1}}],[\"338\",{\"1\":{\"152\":3}}],[\"312\",{\"1\":{\"152\":1}}],[\"312=67\",{\"1\":{\"152\":1}}],[\"376\",{\"1\":{\"188\":2}}],[\"379\",{\"1\":{\"152\":2}}],[\"3717\",{\"1\":{\"52\":1}}],[\"3461\",{\"1\":{\"232\":2}}],[\"343\",{\"1\":{\"152\":2}}],[\"34\",{\"1\":{\"137\":1}}],[\"3所示句子\",{\"1\":{\"203\":1}}],[\"3所示\",{\"1\":{\"124\":1}}],[\"32nd\",{\"1\":{\"107\":1}}],[\"32\",{\"1\":{\"65\":2,\"156\":5}}],[\"384=67\",{\"1\":{\"152\":1}}],[\"384+16\",{\"1\":{\"152\":1}}],[\"384\",{\"1\":{\"64\":1,\"152\":1}}],[\"35\",{\"1\":{\"165\":1}}],[\"35656192\",{\"1\":{\"152\":1}}],[\"3559\",{\"1\":{\"52\":1}}],[\"3591\",{\"1\":{\"52\":1}}],[\"3\",{\"0\":{\"18\":1,\"31\":1,\"32\":1,\"33\":1,\"34\":2,\"42\":1,\"47\":1,\"65\":1,\"66\":1,\"81\":1,\"83\":1,\"96\":1,\"105\":1,\"114\":1,\"121\":1,\"122\":1,\"123\":1,\"124\":2,\"125\":1,\"126\":1,\"127\":1,\"128\":1,\"129\":1,\"130\":1,\"131\":1,\"132\":1,\"141\":1,\"142\":1,\"143\":1,\"150\":1,\"151\":1,\"152\":1,\"153\":2,\"154\":1,\"156\":1,\"163\":1,\"174\":1,\"175\":1,\"176\":1,\"177\":2,\"184\":1,\"185\":1,\"186\":1,\"187\":2,\"188\":1,\"194\":1,\"204\":1,\"206\":1,\"218\":1,\"229\":1,\"230\":1,\"231\":1,\"232\":2,\"233\":1},\"1\":{\"7\":1,\"8\":1,\"19\":2,\"32\":1,\"39\":1,\"43\":1,\"48\":1,\"52\":4,\"65\":1,\"81\":1,\"82\":1,\"103\":1,\"105\":1,\"123\":1,\"124\":1,\"129\":1,\"143\":1,\"146\":1,\"162\":2,\"163\":1,\"164\":1,\"165\":1,\"170\":1,\"185\":2,\"186\":2,\"187\":1,\"188\":1,\"192\":1,\"194\":1,\"203\":1,\"227\":2,\"231\":1,\"232\":3}}],[\"链接\",{\"1\":{\"7\":1}}],[\"是可以输出正确答案\",{\"1\":{\"204\":1}}],[\"是可训练参数\",{\"1\":{\"185\":1}}],[\"是之前3\",{\"1\":{\"196\":1}}],[\"是负的\",{\"1\":{\"194\":1}}],[\"是正的\",{\"1\":{\"194\":1}}],[\"是不影响等式的\",{\"1\":{\"194\":1}}],[\"是不同的\",{\"1\":{\"126\":1}}],[\"是累积奖励\",{\"1\":{\"194\":1}}],[\"是一种数据压缩算法\",{\"1\":{\"226\":1}}],[\"是一种策略梯度优化算法\",{\"1\":{\"191\":1}}],[\"是一个超参数β乘以θ和θ\",{\"1\":{\"196\":1}}],[\"是一个学习马尔科夫决策过程策略的算法\",{\"1\":{\"112\":1}}],[\"是k的维度\",{\"1\":{\"186\":1}}],[\"是从类比中学习\",{\"1\":{\"164\":1}}],[\"是使用的基于人类反馈的强化学习的版本指令微调\",{\"1\":{\"161\":1}}],[\"是时下最强大的序列到序列\",{\"1\":{\"137\":1}}],[\"是在整个轨迹的里面的某一个状态和动作的对\",{\"1\":{\"103\":1}}],[\"是在状态st​下按照一定概率分布选择动作\",{\"1\":{\"96\":1}}],[\"是在任意状态s下均选择最优动作\",{\"1\":{\"96\":1}}],[\"是在特定的一种任务类型上进行指令微调的尝试\",{\"1\":{\"7\":1}}],[\"是机器学习中的一个领域\",{\"1\":{\"93\":1}}],[\"是描述这个实体最后的\",{\"1\":{\"82\":1}}],[\"是学会了质数这种抽象概念的\",{\"1\":{\"81\":1}}],[\"是这个逻辑\",{\"1\":{\"81\":1}}],[\"是否意味着它具备越强的\",{\"1\":{\"80\":1}}],[\"是token嵌入矩阵\",{\"1\":{\"73\":1}}],[\"是token的上下文向量\",{\"1\":{\"73\":1}}],[\"是用于解决某个任务的\",{\"1\":{\"205\":1}}],[\"是用于\",{\"1\":{\"43\":1}}],[\"是微软与佐治亚理工学院共同提出的一种微调优化方法\",{\"1\":{\"41\":1}}],[\"是微软的研究人员为了解决大语言模型微调而开发的一项技术\",{\"1\":{\"40\":1}}],[\"是当下最大的开源\",{\"1\":{\"8\":1}}],[\"是\",{\"1\":{\"7\":1,\"203\":1}}],[\"circuit\",{\"1\":{\"204\":1,\"206\":1}}],[\"clip\",{\"0\":{\"197\":1},\"1\":{\"197\":1}}],[\"class\",{\"1\":{\"162\":1,\"187\":1}}],[\"clark\",{\"1\":{\"138\":1}}],[\"claud\",{\"1\":{\"7\":1}}],[\"cv\",{\"1\":{\"187\":2}}],[\"cross\",{\"1\":{\"170\":2,\"174\":1,\"175\":1}}],[\"crisis\",{\"0\":{\"118\":1},\"1\":{\"118\":1}}],[\"center操作不重要\",{\"1\":{\"153\":1}}],[\"center操作\",{\"1\":{\"153\":1}}],[\"ceval\",{\"1\":{\"146\":1}}],[\"cpu\",{\"1\":{\"137\":1}}],[\"cnn\",{\"1\":{\"137\":1}}],[\"cnrs\",{\"1\":{\"8\":1}}],[\"c4\",{\"1\":{\"123\":1}}],[\"cta\",{\"1\":{\"65\":1}}],[\"cutlass\",{\"0\":{\"65\":1},\"1\":{\"64\":1}}],[\"cuda\",{\"1\":{\"52\":8,\"65\":1}}],[\"cat\",{\"1\":{\"233\":1}}],[\"cache\",{\"1\":{\"171\":1}}],[\"carroll\",{\"1\":{\"165\":1}}],[\"capacity\",{\"1\":{\"83\":1}}],[\"case\",{\"1\":{\"83\":1}}],[\"cai等人\",{\"1\":{\"41\":1}}],[\"can\",{\"1\":{\"39\":1,\"165\":1}}],[\"chi\",{\"1\":{\"208\":1}}],[\"chinese\",{\"1\":{\"26\":1}}],[\"chat\",{\"1\":{\"217\":1}}],[\"chatglm的所有layer结构一致\",{\"1\":{\"172\":1}}],[\"chatglm之所以是decoder\",{\"1\":{\"172\":1}}],[\"chatglm和chatglm2对比\",{\"1\":{\"157\":1}}],[\"chatglmmodel\",{\"1\":{\"151\":2}}],[\"chatglmforconditionalgeneration\",{\"1\":{\"151\":2}}],[\"chatglm\",{\"0\":{\"172\":1},\"1\":{\"151\":1,\"152\":1}}],[\"chatglm2\",{\"1\":{\"146\":5,\"151\":1,\"152\":1}}],[\"chatglm2架构升级\",{\"0\":{\"146\":1}}],[\"chatgpt使用了和text\",{\"1\":{\"163\":1}}],[\"chatgpt是如何工作的\",{\"1\":{\"163\":1}}],[\"chatgpt相关技术介绍\",{\"0\":{\"160\":1},\"2\":{\"168\":1}}],[\"chatgpt\",{\"1\":{\"7\":1,\"161\":1,\"215\":2,\"216\":1,\"217\":5},\"2\":{\"167\":1,\"221\":1}}],[\"chain\",{\"0\":{\"208\":1},\"1\":{\"208\":2}}],[\"chunk\",{\"1\":{\"139\":1}}],[\"checkpoint\",{\"1\":{\"137\":1}}],[\"c\",{\"0\":{\"26\":1,\"34\":1},\"1\":{\"26\":2,\"27\":4,\"34\":1,\"44\":1,\"104\":1,\"183\":4,\"187\":2,\"232\":1}}],[\"coding\",{\"1\":{\"226\":1}}],[\"codebase\",{\"1\":{\"86\":1}}],[\"copy\",{\"1\":{\"171\":2}}],[\"coreattention\",{\"1\":{\"151\":1}}],[\"core\",{\"1\":{\"151\":1,\"152\":2}}],[\"conmy\",{\"1\":{\"206\":1}}],[\"conv1d\",{\"1\":{\"183\":4,\"187\":5}}],[\"config\",{\"1\":{\"171\":13}}],[\"conference\",{\"1\":{\"107\":1,\"165\":1}}],[\"considering\",{\"1\":{\"164\":1}}],[\"controlled\",{\"1\":{\"219\":1}}],[\"continuous\",{\"1\":{\"39\":1,\"42\":1,\"48\":1}}],[\"context\",{\"1\":{\"18\":1,\"146\":1,\"164\":1,\"165\":1,\"208\":1},\"2\":{\"167\":1}}],[\"cot的结果表格\",{\"1\":{\"33\":1}}],[\"cot的prompt设置\",{\"1\":{\"30\":1}}],[\"cot\",{\"0\":{\"30\":1,\"33\":1},\"2\":{\"210\":1}}],[\"collection\",{\"1\":{\"7\":1,\"8\":1}}],[\"co\",{\"1\":{\"7\":1,\"8\":2}}],[\"compute\",{\"1\":{\"202\":1,\"206\":1}}],[\"comparable\",{\"1\":{\"39\":1}}],[\"com\",{\"1\":{\"7\":4,\"8\":2,\"15\":1,\"37\":1,\"61\":1,\"77\":1,\"200\":1}}],[\"8从字节解码到字符的规则\",{\"1\":{\"232\":1}}],[\"8规则解码到字符串我们才能发现\",{\"1\":{\"233\":1}}],[\"8规则将字节串解码为人类可以理解的自然语言字符串\",{\"1\":{\"232\":1}}],[\"8规则转换成字节串\",{\"1\":{\"231\":1}}],[\"808\",{\"1\":{\"188\":1}}],[\"80gb的hbm\",{\"1\":{\"155\":1}}],[\"872\",{\"1\":{\"188\":1}}],[\"840\",{\"1\":{\"152\":2}}],[\"8k\",{\"1\":{\"146\":1}}],[\"8\",{\"0\":{\"129\":1},\"1\":{\"7\":1,\"19\":1,\"27\":1,\"52\":1,\"185\":1,\"202\":2,\"232\":1}}],[\"数学能力的知识回路\",{\"0\":{\"202\":1}}],[\"数据存储可以存储在\",{\"1\":{\"137\":1}}],[\"数据时比\",{\"1\":{\"123\":1}}],[\"数据无损压缩\",{\"1\":{\"81\":1}}],[\"数据内在规律的描述\",{\"1\":{\"81\":1}}],[\"数据主要来源于互联网中爬虫得到的试题与一部分作者收集的试题分享\",{\"1\":{\"27\":1}}],[\"数据与一些开源的\",{\"1\":{\"7\":1}}],[\"数据\",{\"1\":{\"7\":3,\"8\":2}}],[\"数据是\",{\"1\":{\"7\":1}}],[\"数据集中\",{\"1\":{\"137\":1}}],[\"数据集token统计\",{\"1\":{\"137\":1}}],[\"数据集上分别训练模型\",{\"1\":{\"125\":1}}],[\"数据集的几个子集\",{\"1\":{\"123\":1}}],[\"数据集重复的次数与模型的性能的关系\",{\"1\":{\"123\":1}}],[\"数据集重复的次数越多\",{\"1\":{\"123\":1}}],[\"数据集优势\",{\"0\":{\"17\":1}}],[\"数据集数据\",{\"0\":{\"16\":1}}],[\"数据集在模型微调方面\",{\"1\":{\"6\":1}}],[\"数据集和prompt\",{\"1\":{\"6\":1}}],[\"数据集\",{\"0\":{\"11\":1},\"1\":{\"4\":1,\"7\":1,\"8\":2},\"2\":{\"9\":1,\"12\":1,\"14\":1}}],[\"数量大得多的\",{\"1\":{\"83\":1}}],[\"数量\",{\"1\":{\"7\":1}}],[\"h10\",{\"1\":{\"202\":1}}],[\"h1\",{\"1\":{\"202\":1}}],[\"h5\",{\"1\":{\"202\":1}}],[\"how\",{\"1\":{\"202\":1,\"206\":1}}],[\"hoffmann的论文中提出用重复的token训练大语言模型会让模型降低性能\",{\"1\":{\"120\":1}}],[\"hbm容量大但是访问速度慢\",{\"1\":{\"155\":1}}],[\"h\",{\"1\":{\"151\":4,\"152\":4,\"183\":1,\"208\":1}}],[\"hanna\",{\"1\":{\"206\":1}}],[\"hao\",{\"1\":{\"165\":1}}],[\"haystack\",{\"1\":{\"83\":1}}],[\"hard+soft\",{\"1\":{\"46\":1}}],[\"hard的结果表格\",{\"1\":{\"34\":1}}],[\"hard\",{\"0\":{\"34\":1},\"1\":{\"27\":1,\"42\":1,\"48\":1}}],[\"harmless\",{\"1\":{\"8\":1}}],[\"heads\",{\"1\":{\"204\":4}}],[\"head回路\",{\"0\":{\"203\":1}}],[\"head=124\",{\"1\":{\"188\":1}}],[\"headed\",{\"1\":{\"73\":1,\"187\":1}}],[\"head\",{\"0\":{\"186\":1},\"1\":{\"64\":2,\"65\":4,\"82\":2,\"151\":1,\"152\":1,\"156\":1,\"183\":1,\"186\":2,\"187\":1,\"188\":1,\"202\":5,\"203\":5,\"204\":1}}],[\"hidden\",{\"1\":{\"63\":2}}],[\"hi\",{\"1\":{\"46\":1}}],[\"hku\",{\"1\":{\"8\":2}}],[\"human\",{\"1\":{\"161\":1,\"162\":1,\"165\":1}}],[\"hub\",{\"1\":{\"8\":1}}],[\"hugging\",{\"1\":{\"8\":2,\"39\":1},\"2\":{\"50\":1}}],[\"huggingface\",{\"1\":{\"7\":1,\"8\":2,\"37\":1}}],[\"hust\",{\"1\":{\"2\":1}}],[\"https\",{\"1\":{\"7\":5,\"8\":4,\"15\":1,\"37\":1,\"61\":2,\"77\":1,\"137\":1,\"200\":1}}],[\"hh\",{\"1\":{\"7\":3,\"8\":2}}],[\"2和式2\",{\"1\":{\"195\":1}}],[\"2的r\",{\"1\":{\"194\":1}}],[\"2换算成式2\",{\"1\":{\"193\":1}}],[\"29\",{\"1\":{\"165\":1}}],[\"2=7\",{\"1\":{\"188\":1}}],[\"2=1\",{\"1\":{\"188\":1}}],[\"2=8\",{\"1\":{\"152\":3}}],[\"2=201\",{\"1\":{\"152\":1}}],[\"248+67\",{\"1\":{\"152\":1}}],[\"248+134\",{\"1\":{\"152\":1}}],[\"248\",{\"1\":{\"152\":2}}],[\"240k\",{\"1\":{\"7\":1,\"8\":1}}],[\"288=50\",{\"1\":{\"152\":1}}],[\"288+12\",{\"1\":{\"152\":1}}],[\"28=5\",{\"1\":{\"152\":1}}],[\"28\",{\"1\":{\"151\":2,\"165\":1}}],[\"234\",{\"1\":{\"232\":3}}],[\"2325\",{\"1\":{\"232\":1}}],[\"238\",{\"1\":{\"152\":2}}],[\"2305\",{\"1\":{\"137\":1,\"206\":1}}],[\"235\",{\"1\":{\"123\":1}}],[\"2^29\",{\"1\":{\"123\":1}}],[\"2^27\",{\"1\":{\"123\":2}}],[\"2^8\",{\"1\":{\"123\":1}}],[\"2^6\",{\"1\":{\"123\":1}}],[\"2211\",{\"1\":{\"206\":1}}],[\"2210\",{\"1\":{\"61\":1}}],[\"229\",{\"1\":{\"123\":1}}],[\"2所示\",{\"1\":{\"123\":1,\"186\":1,\"193\":1,\"202\":1}}],[\"2次\",{\"1\":{\"120\":1}}],[\"2650\",{\"1\":{\"187\":1}}],[\"266\",{\"1\":{\"152\":2}}],[\"2612\",{\"1\":{\"52\":1}}],[\"2604\",{\"1\":{\"52\":1}}],[\"2762\",{\"1\":{\"187\":1}}],[\"2766\",{\"1\":{\"187\":1}}],[\"27744\",{\"1\":{\"165\":1}}],[\"27730\",{\"1\":{\"165\":1}}],[\"27\",{\"1\":{\"151\":2}}],[\"2720\",{\"1\":{\"52\":1}}],[\"270\",{\"1\":{\"8\":1}}],[\"2m\",{\"1\":{\"19\":3}}],[\"2572\",{\"1\":{\"232\":2}}],[\"2571\",{\"1\":{\"52\":1}}],[\"256\",{\"1\":{\"156\":2}}],[\"255\",{\"1\":{\"152\":2}}],[\"25\",{\"1\":{\"19\":2,\"165\":1}}],[\"2094\",{\"1\":{\"187\":1}}],[\"203960832\",{\"1\":{\"152\":1}}],[\"208\",{\"1\":{\"152\":1}}],[\"208+8192\",{\"1\":{\"152\":1}}],[\"206\",{\"1\":{\"152\":2}}],[\"201\",{\"1\":{\"152\":1,\"232\":3}}],[\"2019\",{\"1\":{\"138\":1,\"139\":2}}],[\"2018\",{\"1\":{\"137\":2}}],[\"2015\",{\"1\":{\"107\":3}}],[\"2011\",{\"1\":{\"41\":1}}],[\"2010\",{\"1\":{\"41\":2}}],[\"2023\",{\"1\":{\"165\":1,\"206\":1}}],[\"2023年3月\",{\"1\":{\"39\":1}}],[\"2020年7月\",{\"1\":{\"161\":1}}],[\"2020b\",{\"1\":{\"137\":1}}],[\"2020a\",{\"1\":{\"137\":1}}],[\"2022\",{\"1\":{\"137\":2,\"139\":2,\"162\":1,\"165\":6,\"206\":1}}],[\"2022年5\",{\"1\":{\"161\":1}}],[\"2022年\",{\"1\":{\"120\":1}}],[\"2022年3月20\",{\"1\":{\"39\":1}}],[\"2027年几年的时间里\",{\"1\":{\"120\":1}}],[\"2021年9月\",{\"1\":{\"39\":1}}],[\"2021年3月18\",{\"1\":{\"39\":1}}],[\"2021年8月\",{\"1\":{\"39\":1}}],[\"2021年10月\",{\"1\":{\"39\":1}}],[\"2021数据与一些开源的instruction数据\",{\"1\":{\"8\":1}}],[\"2021\",{\"1\":{\"7\":1,\"8\":1,\"137\":2}}],[\"20\",{\"1\":{\"16\":1}}],[\"2000\",{\"1\":{\"8\":1}}],[\"2\",{\"0\":{\"8\":1,\"17\":1,\"28\":1,\"29\":1,\"30\":2,\"33\":1,\"39\":1,\"40\":1,\"41\":2,\"42\":1,\"43\":1,\"44\":1,\"45\":1,\"46\":1,\"53\":1,\"62\":1,\"63\":1,\"64\":2,\"65\":1,\"72\":1,\"73\":1,\"74\":2,\"80\":1,\"82\":1,\"95\":1,\"103\":1,\"104\":1,\"113\":1,\"120\":1,\"123\":1,\"138\":1,\"139\":1,\"140\":2,\"143\":1,\"149\":1,\"152\":1,\"155\":1,\"162\":1,\"172\":1,\"173\":1,\"176\":1,\"183\":1,\"186\":1,\"193\":1,\"203\":1,\"205\":1,\"217\":1,\"228\":1,\"231\":1},\"1\":{\"7\":1,\"8\":1,\"17\":2,\"18\":2,\"19\":1,\"30\":1,\"39\":1,\"40\":3,\"41\":2,\"42\":1,\"43\":3,\"48\":1,\"52\":2,\"63\":1,\"64\":1,\"65\":3,\"74\":1,\"77\":1,\"81\":1,\"82\":1,\"83\":4,\"95\":1,\"96\":1,\"103\":3,\"112\":1,\"113\":1,\"123\":1,\"140\":1,\"142\":1,\"146\":1,\"155\":1,\"156\":4,\"162\":1,\"163\":1,\"165\":1,\"170\":1,\"174\":2,\"175\":1,\"176\":1,\"177\":1,\"181\":1,\"183\":1,\"185\":1,\"186\":2,\"187\":1,\"188\":2,\"192\":1,\"193\":3,\"202\":3,\"204\":3,\"205\":1,\"206\":2,\"217\":1,\"219\":1,\"227\":2,\"231\":1,\"232\":3}}],[\"语言建模通常被构造为来自一组示例\",{\"1\":{\"182\":1}}],[\"语言建模\",{\"0\":{\"182\":1}}],[\"语言\",{\"1\":{\"8\":1}}],[\"语言说明的模型\",{\"1\":{\"7\":1}}],[\"语言模型进化树\",{\"1\":{\"170\":1}}],[\"语言模型\",{\"0\":{\"89\":1},\"1\":{\"4\":1},\"2\":{\"21\":1,\"36\":1,\"67\":1,\"75\":1,\"84\":1,\"87\":1,\"90\":1,\"92\":1,\"97\":1,\"108\":1,\"115\":1,\"134\":1,\"144\":1,\"158\":1,\"166\":1,\"179\":1,\"189\":1,\"198\":1,\"207\":1}}],[\"配备\",{\"1\":{\"7\":1}}],[\"1红线部分勾勒出的某个任务通路\",{\"1\":{\"205\":1}}],[\"17\",{\"1\":{\"202\":1}}],[\"17yy\",{\"1\":{\"202\":1}}],[\"1+ϵ\",{\"1\":{\"197\":1}}],[\"1−ϵ\",{\"1\":{\"197\":1}}],[\"1中\",{\"1\":{\"196\":1}}],[\"1是严格相等的\",{\"1\":{\"195\":1}}],[\"1所示\",{\"1\":{\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"202\":1}}],[\"1所示类别粗略分类\",{\"1\":{\"96\":1}}],[\"1式给出的\",{\"1\":{\"196\":1}}],[\"1式\",{\"1\":{\"193\":1}}],[\"1为优化目标\",{\"1\":{\"192\":1}}],[\"1568\",{\"1\":{\"187\":1}}],[\"150\",{\"1\":{\"152\":2}}],[\"150528\",{\"1\":{\"151\":1}}],[\"1的极大值\",{\"1\":{\"192\":1}}],[\"1的gpt2模型结构图\",{\"1\":{\"184\":1}}],[\"1的形式\",{\"1\":{\"94\":1}}],[\"192\",{\"1\":{\"152\":3}}],[\"168\",{\"1\":{\"232\":3}}],[\"168296448\",{\"1\":{\"152\":1}}],[\"16777216\",{\"1\":{\"152\":1}}],[\"16\",{\"1\":{\"152\":2,\"156\":1}}],[\"16k\",{\"1\":{\"142\":2}}],[\"16384\",{\"1\":{\"137\":1}}],[\"161k\",{\"1\":{\"7\":1}}],[\"18\",{\"1\":{\"187\":1}}],[\"18878976\",{\"1\":{\"152\":1}}],[\"1889\",{\"1\":{\"107\":1}}],[\"1897\",{\"1\":{\"107\":1}}],[\"1111xxxx\",{\"1\":{\"232\":1}}],[\"1110xxxx\",{\"1\":{\"232\":1}}],[\"110xxxxx\",{\"1\":{\"232\":1}}],[\"112197632\",{\"1\":{\"152\":1}}],[\"112\",{\"1\":{\"152\":2}}],[\"11\",{\"0\":{\"132\":1},\"1\":{\"107\":1,\"183\":1,\"202\":1}}],[\"129\",{\"1\":{\"232\":3}}],[\"12+ln+lm\",{\"1\":{\"188\":1}}],[\"1236\",{\"1\":{\"187\":1}}],[\"1237\",{\"1\":{\"52\":2}}],[\"128\",{\"1\":{\"156\":9}}],[\"12\",{\"1\":{\"152\":1}}],[\"125\",{\"1\":{\"152\":4}}],[\"122\",{\"1\":{\"52\":1}}],[\"127\",{\"1\":{\"52\":2}}],[\"14\",{\"1\":{\"19\":1}}],[\"104\",{\"1\":{\"232\":2}}],[\"101\",{\"1\":{\"232\":2}}],[\"102\",{\"1\":{\"232\":3}}],[\"1024\",{\"1\":{\"137\":1,\"183\":1,\"188\":1}}],[\"10xxxxxx\",{\"1\":{\"232\":1}}],[\"1062\",{\"1\":{\"187\":1}}],[\"10th\",{\"1\":{\"165\":1}}],[\"10\",{\"0\":{\"131\":1},\"1\":{\"86\":1,\"137\":1,\"185\":1,\"202\":3}}],[\"10b\",{\"1\":{\"19\":1}}],[\"100m\",{\"1\":{\"8\":1}}],[\"13+24=37\",{\"1\":{\"205\":1}}],[\"1390\",{\"1\":{\"187\":1}}],[\"13948\",{\"1\":{\"26\":1}}],[\"1371\",{\"1\":{\"187\":1}}],[\"1380\",{\"1\":{\"187\":1}}],[\"130344减小到64794\",{\"1\":{\"149\":1}}],[\"13\",{\"1\":{\"8\":1}}],[\"1b之间的p3数据集\",{\"1\":{\"8\":1}}],[\"1b\",{\"1\":{\"8\":1}}],[\"1\",{\"0\":{\"7\":1,\"16\":1,\"27\":1,\"29\":1,\"32\":1,\"38\":1,\"40\":1,\"52\":1,\"61\":1,\"63\":1,\"71\":1,\"73\":1,\"78\":1,\"79\":2,\"80\":1,\"81\":1,\"94\":1,\"101\":1,\"102\":2,\"103\":1,\"112\":1,\"119\":1,\"122\":1,\"137\":1,\"139\":1,\"142\":1,\"147\":1,\"148\":2,\"149\":1,\"150\":1,\"151\":2,\"152\":1,\"153\":1,\"154\":1,\"161\":1,\"170\":1,\"171\":2,\"172\":1,\"175\":1,\"182\":1,\"185\":1,\"192\":1,\"201\":1,\"202\":2,\"203\":1,\"204\":1,\"216\":1,\"227\":1,\"230\":1},\"1\":{\"7\":1,\"8\":1,\"16\":1,\"17\":2,\"18\":2,\"19\":1,\"29\":1,\"39\":1,\"40\":1,\"41\":2,\"42\":1,\"43\":2,\"47\":1,\"48\":1,\"52\":2,\"61\":1,\"63\":2,\"64\":1,\"65\":3,\"71\":1,\"74\":1,\"77\":1,\"81\":2,\"82\":2,\"83\":4,\"94\":1,\"95\":4,\"96\":1,\"103\":5,\"104\":1,\"105\":1,\"107\":1,\"112\":1,\"113\":1,\"120\":1,\"122\":1,\"123\":1,\"137\":2,\"140\":1,\"142\":1,\"146\":2,\"155\":1,\"156\":29,\"157\":1,\"161\":1,\"162\":1,\"163\":3,\"164\":1,\"165\":2,\"170\":1,\"174\":2,\"175\":2,\"182\":2,\"183\":5,\"184\":1,\"185\":16,\"187\":6,\"188\":1,\"192\":4,\"193\":2,\"194\":1,\"195\":1,\"196\":1,\"197\":1,\"202\":2,\"205\":2,\"217\":1,\"218\":1,\"227\":3,\"231\":1,\"232\":3}}],[\"本身的内容被拷贝到\",{\"1\":{\"203\":1}}],[\"本质上都是x的线性变换\",{\"1\":{\"186\":1}}],[\"本质上是自回归模型\",{\"1\":{\"181\":1}}],[\"本文主要介绍有关llm中的知识回路以及一些相关的猜想\",{\"1\":{\"200\":1}}],[\"本文主要分享的内容为以下两点\",{\"1\":{\"77\":1}}],[\"本文的思路在于使用自然语言模拟循环机制\",{\"1\":{\"217\":1}}],[\"本文的方法也比处理所有输入\",{\"1\":{\"140\":1}}],[\"本文的作者发现\",{\"1\":{\"125\":1}}],[\"本文不是只关注输入的这前\",{\"1\":{\"140\":1}}],[\"本文使用\",{\"1\":{\"139\":1}}],[\"本文按照\",{\"1\":{\"139\":1}}],[\"本文证明\",{\"1\":{\"137\":1}}],[\"本文针对一些质量较高的指令微调数据集和提示微调数据集\",{\"1\":{\"6\":1}}],[\"本页面包含一些论文分享的分类\",{\"1\":{\"4\":1}}],[\"尤其是在模型与人类认识对齐方面\",{\"1\":{\"6\":1}}],[\"txt中位置越靠前优先级越高\",{\"1\":{\"231\":1}}],[\"txt\",{\"1\":{\"231\":1}}],[\"txt来记录所有对merge词对\",{\"1\":{\"229\":1}}],[\"tiling\",{\"1\":{\"155\":1}}],[\"t=1∏t​pθ​\",{\"1\":{\"103\":1}}],[\"t5stack\",{\"1\":{\"171\":2}}],[\"t5模型的encoder和decoder区分的比较明确\",{\"1\":{\"171\":1}}],[\"t5\",{\"0\":{\"171\":1},\"1\":{\"66\":1,\"123\":2}}],[\"thought\",{\"0\":{\"208\":1},\"1\":{\"208\":2}}],[\"than\",{\"1\":{\"202\":1,\"206\":1}}],[\"that\",{\"1\":{\"164\":1}}],[\"thread\",{\"1\":{\"65\":1}}],[\"threadblock\",{\"1\":{\"65\":2}}],[\"the\",{\"1\":{\"39\":1,\"107\":1,\"164\":1,\"165\":1,\"202\":3,\"204\":1,\"206\":1}}],[\"tvm以及nvidia\",{\"1\":{\"61\":1}}],[\"true\",{\"1\":{\"171\":1}}],[\"trust\",{\"1\":{\"107\":1}}],[\"trpo算法的公式如式4\",{\"1\":{\"195\":1}}],[\"trpo算法引入了kl散度\",{\"1\":{\"195\":1}}],[\"trpo\",{\"0\":{\"105\":1}}],[\"trained\",{\"1\":{\"202\":1,\"206\":1,\"217\":2}}],[\"training\",{\"0\":{\"70\":1},\"1\":{\"165\":1,\"219\":1}}],[\"transformer由论文\",{\"1\":{\"169\":1}}],[\"transformer的计算过程缓慢且耗费内存\",{\"1\":{\"155\":1}}],[\"transformer架构\",{\"0\":{\"148\":1}}],[\"transformers\",{\"1\":{\"137\":2,\"183\":1}}],[\"transformer\",{\"0\":{\"66\":1,\"83\":1},\"1\":{\"61\":1,\"63\":1,\"66\":2,\"82\":7,\"83\":2,\"136\":2,\"137\":7,\"138\":1,\"140\":3,\"151\":2,\"152\":33,\"181\":1,\"183\":1,\"201\":1,\"202\":1,\"203\":1,\"204\":1,\"205\":1,\"216\":2,\"217\":2},\"2\":{\"68\":1,\"145\":1,\"180\":1}}],[\"transformer推理库\",{\"1\":{\"60\":1}}],[\"tree\",{\"1\":{\"7\":2}}],[\"t的对角元素\",{\"1\":{\"52\":1}}],[\"term\",{\"1\":{\"217\":1}}],[\"tensorcore\",{\"1\":{\"64\":1}}],[\"tensorrt等\",{\"1\":{\"61\":1}}],[\"tensor\",{\"1\":{\"52\":8,\"185\":3,\"187\":2}}],[\"technology\",{\"1\":{\"27\":1}}],[\"text生成任务\",{\"1\":{\"43\":1}}],[\"text框架中加入knowledge\",{\"1\":{\"8\":1}}],[\"text\",{\"0\":{\"215\":1},\"1\":{\"7\":1,\"8\":2,\"219\":2}}],[\"tunning\",{\"1\":{\"48\":1}}],[\"tune\",{\"1\":{\"45\":1,\"162\":1}}],[\"tuninig数据集分享\",{\"0\":{\"7\":1}}],[\"tuning仅在transformer的\",{\"1\":{\"46\":1}}],[\"tuning应用于在nlu任务\",{\"1\":{\"45\":1}}],[\"tuning技术\",{\"1\":{\"45\":1}}],[\"tuning技术应用而生\",{\"1\":{\"45\":1}}],[\"tuning还提出了prompt\",{\"1\":{\"44\":1}}],[\"tuning给每个任务定义了自己的prompt\",{\"1\":{\"44\":1}}],[\"tuning是做生成任务\",{\"1\":{\"43\":1}}],[\"tuning的deep形式\",{\"1\":{\"46\":1}}],[\"tuning的简化\",{\"1\":{\"46\":1}}],[\"tuning的prompt拼接方式\",{\"1\":{\"43\":1}}],[\"tuning的作者提出了prefix\",{\"1\":{\"43\":1}}],[\"tuning的方法\",{\"1\":{\"8\":1,\"46\":1}}],[\"tuning将模板t中的pi\",{\"1\":{\"46\":1}}],[\"tuning将预训练参数固定\",{\"1\":{\"45\":1}}],[\"tuning将一系列连续的task\",{\"1\":{\"43\":1}}],[\"tuning将prompt对应的token替换为可训练的嵌入\",{\"1\":{\"39\":1}}],[\"tuning与full\",{\"1\":{\"43\":1}}],[\"tuning可理解为针对prompt部分的微调\",{\"1\":{\"39\":1}}],[\"tuning针对每一类任务\",{\"1\":{\"39\":1}}],[\"tuning在input前面加入prefix部分\",{\"1\":{\"39\":1}}],[\"tuning\",{\"0\":{\"43\":1,\"44\":1,\"45\":1},\"1\":{\"6\":2,\"7\":3,\"38\":1,\"39\":11,\"43\":2,\"44\":2,\"45\":9,\"46\":5,\"48\":9,\"161\":2,\"162\":3},\"2\":{\"10\":2,\"50\":3,\"167\":1}}],[\"tuning数据集分享\",{\"0\":{\"6\":1,\"8\":1}}],[\"tuning和prompt\",{\"0\":{\"6\":1}}],[\"turbo和oasst两个模型的回答结果\",{\"1\":{\"81\":1}}],[\"turbo\",{\"1\":{\"19\":1}}],[\"takes\",{\"1\":{\"164\":1}}],[\"taylor在训练银河战舰\",{\"1\":{\"125\":1}}],[\"table\",{\"1\":{\"19\":4,\"43\":1}}],[\"tasks\",{\"1\":{\"39\":1}}],[\"task\",{\"1\":{\"7\":1,\"182\":1}}],[\"tjunlp\",{\"1\":{\"15\":1}}],[\"towards\",{\"1\":{\"219\":1}}],[\"torch\",{\"1\":{\"156\":14,\"185\":3,\"187\":4}}],[\"toh\",{\"1\":{\"41\":1}}],[\"too\",{\"1\":{\"39\":1}}],[\"toolkits\",{\"1\":{\"8\":1}}],[\"to\",{\"0\":{\"118\":2},\"1\":{\"8\":2,\"39\":1,\"43\":1,\"118\":2,\"137\":1,\"151\":4,\"152\":4,\"162\":3,\"165\":1,\"202\":1,\"203\":1}}],[\"token数量似乎并不是很足够\",{\"1\":{\"120\":1}}],[\"token危机\",{\"1\":{\"118\":1}}],[\"tokens作为prefix\",{\"1\":{\"43\":1}}],[\"token\",{\"0\":{\"118\":1,\"222\":1},\"1\":{\"4\":1,\"46\":2,\"79\":2,\"82\":2,\"118\":1,\"120\":1,\"137\":5,\"138\":2,\"140\":3,\"142\":2,\"149\":1,\"201\":2,\"202\":1,\"203\":8,\"204\":7,\"205\":4},\"2\":{\"223\":1,\"224\":1,\"225\":1,\"234\":1}}],[\"t0\",{\"1\":{\"7\":1}}],[\"aaabdaaabac\",{\"1\":{\"228\":1}}],[\"a7\",{\"1\":{\"202\":1}}],[\"a9\",{\"1\":{\"202\":1}}],[\"a5\",{\"1\":{\"202\":1}}],[\"as\",{\"1\":{\"165\":1}}],[\"associations\",{\"1\":{\"82\":1}}],[\"april\",{\"1\":{\"165\":1}}],[\"approaches\",{\"1\":{\"162\":1}}],[\"apple\",{\"1\":{\"82\":1}}],[\"affine=true\",{\"1\":{\"151\":3,\"183\":3}}],[\"abilities\",{\"1\":{\"202\":1,\"206\":1}}],[\"abbeel\",{\"1\":{\"107\":1}}],[\"abs\",{\"1\":{\"61\":1}}],[\"act\",{\"1\":{\"183\":1}}],[\"action\",{\"1\":{\"112\":2}}],[\"acm\",{\"1\":{\"107\":1}}],[\"ac原理\",{\"1\":{\"104\":1}}],[\"across\",{\"1\":{\"39\":1}}],[\"attn\",{\"1\":{\"183\":3,\"187\":3}}],[\"attention层的第一个conv1d相当于是集成了从输入x到q\",{\"1\":{\"187\":1}}],[\"attention计算\",{\"1\":{\"186\":1,\"187\":1}}],[\"attention计算中的维度变化如下所示\",{\"1\":{\"156\":1}}],[\"attention结构如下图所示\",{\"1\":{\"186\":1}}],[\"attention块之后添加了额外的ln层\",{\"1\":{\"183\":1}}],[\"attention架构\",{\"1\":{\"182\":1}}],[\"attention往往不发挥作用甚至不存在\",{\"1\":{\"178\":1}}],[\"attention模块的计算就会被跳过\",{\"1\":{\"175\":1}}],[\"attention添加上一步key和value\",{\"1\":{\"174\":1}}],[\"attention作为中间层\",{\"1\":{\"174\":1}}],[\"attention的最后的linear层的工作\",{\"1\":{\"187\":1}}],[\"attention的计算和拼接\",{\"1\":{\"187\":1}}],[\"attention的计算式如式3\",{\"1\":{\"186\":1}}],[\"attention的输出\",{\"1\":{\"170\":1}}],[\"attention的特殊之处在于输入的k和v来自encoder的输出\",{\"1\":{\"170\":1}}],[\"attention的时间和内存困惑度会随着输入序列长度的增加成二次方增长\",{\"1\":{\"155\":1}}],[\"attention和mlp的transformer层串联起来\",{\"1\":{\"173\":1}}],[\"attention和mlp\",{\"1\":{\"170\":2}}],[\"attention让所有的头之间共享同一份key和value矩阵\",{\"1\":{\"156\":1}}],[\"attention技术\",{\"1\":{\"146\":1}}],[\"attention操作\",{\"1\":{\"73\":1}}],[\"attention\",{\"0\":{\"156\":1,\"186\":1,\"204\":1},\"1\":{\"61\":1,\"63\":2,\"64\":2,\"65\":3,\"82\":8,\"151\":6,\"152\":14,\"156\":1,\"164\":1,\"169\":1,\"170\":1,\"174\":2,\"175\":1,\"178\":1,\"186\":3,\"202\":5,\"203\":6,\"204\":3}}],[\"at​\",{\"1\":{\"103\":3,\"105\":2}}],[\"at​∣st​\",{\"1\":{\"96\":1,\"103\":2,\"105\":2}}],[\"a∣s\",{\"1\":{\"95\":1}}],[\"auto\",{\"1\":{\"82\":1}}],[\"agi\",{\"1\":{\"80\":1,\"93\":1}}],[\"adversarial\",{\"1\":{\"219\":1}}],[\"advances\",{\"1\":{\"165\":1}}],[\"addmm\",{\"1\":{\"187\":1}}],[\"add\",{\"1\":{\"64\":1}}],[\"adams\",{\"1\":{\"165\":1}}],[\"adam\",{\"1\":{\"103\":1}}],[\"adaptive\",{\"1\":{\"39\":1,\"196\":1}}],[\"adaptation\",{\"1\":{\"39\":1,\"40\":1}}],[\"adalora提出了一种新的重要性度量\",{\"1\":{\"41\":1}}],[\"adalora将增量矩阵p∧q划分为三元组\",{\"1\":{\"41\":1}}],[\"adalora通过重要性评分动态调整∆=p\",{\"1\":{\"41\":1}}],[\"adalora根据重要性评分自适应地分配参数预算\",{\"1\":{\"41\":1}}],[\"adalora包含两个重要组成部分\",{\"1\":{\"41\":1}}],[\"adalora调整增量矩阵的秩\",{\"1\":{\"41\":1}}],[\"adalora\",{\"0\":{\"41\":1},\"1\":{\"39\":1,\"41\":2},\"2\":{\"50\":1}}],[\"aml\",{\"1\":{\"61\":1,\"63\":1,\"66\":1}}],[\"arbitrarily\",{\"0\":{\"215\":1}}],[\"are\",{\"1\":{\"165\":1,\"181\":1,\"219\":1}}],[\"arxiv\",{\"1\":{\"61\":1,\"137\":1,\"165\":1,\"206\":4}}],[\"artificial\",{\"1\":{\"2\":1}}],[\"a2​∣s2​\",{\"1\":{\"103\":1}}],[\"a2\",{\"1\":{\"44\":1}}],[\"a100\",{\"1\":{\"155\":1}}],[\"a1​\",{\"1\":{\"103\":2}}],[\"a1​∣s1​\",{\"1\":{\"103\":1}}],[\"a1\",{\"1\":{\"44\":1}}],[\"alec\",{\"1\":{\"219\":1}}],[\"almeida\",{\"1\":{\"165\":1}}],[\"aligning\",{\"1\":{\"162\":1}}],[\"al\",{\"1\":{\"137\":8,\"138\":1,\"139\":4,\"165\":3,\"206\":1}}],[\"albert直接用两个小矩阵替换了原来的大矩阵\",{\"1\":{\"40\":1}}],[\"all\",{\"1\":{\"169\":1}}],[\"allocation\",{\"1\":{\"39\":1}}],[\"allen\",{\"1\":{\"7\":1,\"8\":1}}],[\"ao的结果表格\",{\"1\":{\"32\":1}}],[\"ao的prompt设置\",{\"1\":{\"29\":1}}],[\"ao\",{\"0\":{\"29\":1,\"32\":1}}],[\"a\",{\"1\":{\"26\":1,\"44\":1,\"83\":1,\"95\":5,\"96\":6,\"112\":2,\"162\":1,\"197\":1,\"202\":1,\"204\":1,\"206\":5}}],[\"anchor\",{\"1\":{\"45\":1,\"46\":1}}],[\"answer\",{\"0\":{\"29\":1}}],[\"anthropics\",{\"1\":{\"7\":1}}],[\"anthropic\",{\"1\":{\"7\":4,\"8\":1}}],[\"and\",{\"1\":{\"2\":1,\"8\":1,\"39\":1,\"83\":1,\"164\":1}}],[\"aiac\",{\"1\":{\"218\":1}}],[\"ai作为内容\",{\"1\":{\"218\":1}}],[\"aigc\",{\"1\":{\"218\":1}}],[\"ai训练的llama\",{\"1\":{\"120\":1}}],[\"ai\",{\"1\":{\"7\":1,\"8\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,Et(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(st(t,v[s],n)):e==="search"?self.postMessage(et(t,v[s],n)):self.postMessage({suggestions:st(t,v[s],n),results:et(t,v[s],n)})};
//# sourceMappingURL=index.js.map
