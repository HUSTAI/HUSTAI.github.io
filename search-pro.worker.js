const nt="ENTRIES",V="KEYS",T="VALUES",p="";class D{constructor(t,s){const n=t._tree,o=Array.from(n.keys());this.set=t,this._type=s,this._path=o.length>0?[{node:n,keys:o}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:s}=z(this._path);if(z(s)===p)return{done:!1,value:this.result()};const n=t.get(z(s));return this._path.push({node:n,keys:Array.from(n.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=z(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>z(t)).filter(t=>t!==p).join("")}value(){return z(this._path).node.get(p)}result(){switch(this._type){case T:return this.value();case V:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const z=e=>e[e.length-1],ot=(e,t,s)=>{const n=new Map;if(t===void 0)return n;const o=t.length+1,u=o+s,i=new Uint8Array(u*o).fill(s+1);for(let r=0;r<o;++r)i[r]=r;for(let r=1;r<u;++r)i[r*o]=r;return W(e,t,s,n,i,1,o,""),n},W=(e,t,s,n,o,u,i,r)=>{const h=u*i;t:for(const c of e.keys())if(c===p){const d=o[h-1];d<=s&&n.set(r,[e.get(c),d])}else{let d=u;for(let l=0;l<c.length;++l,++d){const m=c[l],f=i*d,g=f-i;let a=o[f];const F=Math.max(0,d-s-1),y=Math.min(i-1,d+s);for(let _=F;_<y;++_){const b=m!==t[_],E=o[g+_]+ +b,A=o[g+_+1]+1,w=o[f+_]+1,L=o[f+_+1]=Math.min(E,A,w);L<a&&(a=L)}if(a>s)continue t}W(e.get(c),t,s,n,o,d,i,r+c)}};class C{constructor(t=new Map,s=""){this._size=void 0,this._tree=t,this._prefix=s}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[s,n]=x(this._tree,t.slice(this._prefix.length));if(s===void 0){const[o,u]=M(n);for(const i of o.keys())if(i!==p&&i.startsWith(u)){const r=new Map;return r.set(i.slice(u.length),o.get(i)),new C(r,t)}}return new C(s,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,ut(this._tree,t)}entries(){return new D(this,nt)}forEach(t){for(const[s,n]of this)t(s,n,this)}fuzzyGet(t,s){return ot(this._tree,t,s)}get(t){const s=I(this._tree,t);return s!==void 0?s.get(p):void 0}has(t){const s=I(this._tree,t);return s!==void 0&&s.has(p)}keys(){return new D(this,V)}set(t,s){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,O(this._tree,t).set(p,s),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);return n.set(p,s(n.get(p))),this}fetch(t,s){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const n=O(this._tree,t);let o=n.get(p);return o===void 0&&n.set(p,o=s()),o}values(){return new D(this,T)}[Symbol.iterator](){return this.entries()}static from(t){const s=new C;for(const[n,o]of t)s.set(n,o);return s}static fromObject(t){return C.from(Object.entries(t))}}const x=(e,t,s=[])=>{if(t.length===0||e==null)return[e,s];for(const n of e.keys())if(n!==p&&t.startsWith(n))return s.push([e,n]),x(e.get(n),t.slice(n.length),s);return s.push([e,t]),x(void 0,"",s)},I=(e,t)=>{if(t.length===0||e==null)return e;for(const s of e.keys())if(s!==p&&t.startsWith(s))return I(e.get(s),t.slice(s.length))},O=(e,t)=>{const s=t.length;t:for(let n=0;e&&n<s;){for(const u of e.keys())if(u!==p&&t[n]===u[0]){const i=Math.min(s-n,u.length);let r=1;for(;r<i&&t[n+r]===u[r];)++r;const h=e.get(u);if(r===u.length)e=h;else{const c=new Map;c.set(u.slice(r),h),e.set(t.slice(n,n+r),c),e.delete(u),e=c}n+=r;continue t}const o=new Map;return e.set(t.slice(n),o),o}return e},ut=(e,t)=>{const[s,n]=x(e,t);if(s!==void 0){if(s.delete(p),s.size===0)$(n);else if(s.size===1){const[o,u]=s.entries().next().value;R(n,o,u)}}},$=e=>{if(e.length===0)return;const[t,s]=M(e);if(t.delete(s),t.size===0)$(e.slice(0,-1));else if(t.size===1){const[n,o]=t.entries().next().value;n!==p&&R(e.slice(0,-1),n,o)}},R=(e,t,s)=>{if(e.length===0)return;const[n,o]=M(e);n.set(o+t,s),n.delete(o)},M=e=>e[e.length-1],it=/[\n\r -#%-*,-/:;?@[-\]_{}\u00A0\u00A1\u00A7\u00AB\u00B6\u00B7\u00BB\u00BF\u037E\u0387\u055A-\u055F\u0589\u058A\u05BE\u05C0\u05C3\u05C6\u05F3\u05F4\u0609\u060A\u060C\u060D\u061B\u061E\u061F\u066A-\u066D\u06D4\u0700-\u070D\u07F7-\u07F9\u0830-\u083E\u085E\u0964\u0965\u0970\u09FD\u0A76\u0AF0\u0C77\u0C84\u0DF4\u0E4F\u0E5A\u0E5B\u0F04-\u0F12\u0F14\u0F3A-\u0F3D\u0F85\u0FD0-\u0FD4\u0FD9\u0FDA\u104A-\u104F\u10FB\u1360-\u1368\u1400\u166E\u1680\u169B\u169C\u16EB-\u16ED\u1735\u1736\u17D4-\u17D6\u17D8-\u17DA\u1800-\u180A\u1944\u1945\u1A1E\u1A1F\u1AA0-\u1AA6\u1AA8-\u1AAD\u1B5A-\u1B60\u1BFC-\u1BFF\u1C3B-\u1C3F\u1C7E\u1C7F\u1CC0-\u1CC7\u1CD3\u2000-\u200A\u2010-\u2029\u202F-\u2043\u2045-\u2051\u2053-\u205F\u207D\u207E\u208D\u208E\u2308-\u230B\u2329\u232A\u2768-\u2775\u27C5\u27C6\u27E6-\u27EF\u2983-\u2998\u29D8-\u29DB\u29FC\u29FD\u2CF9-\u2CFC\u2CFE\u2CFF\u2D70\u2E00-\u2E2E\u2E30-\u2E4F\u3000-\u3003\u3008-\u3011\u3014-\u301F\u3030\u303D\u30A0\u30FB\uA4FE\uA4FF\uA60D-\uA60F\uA673\uA67E\uA6F2-\uA6F7\uA874-\uA877\uA8CE\uA8CF\uA8F8-\uA8FA\uA8FC\uA92E\uA92F\uA95F\uA9C1-\uA9CD\uA9DE\uA9DF\uAA5C-\uAA5F\uAADE\uAADF\uAAF0\uAAF1\uABEB\uFD3E\uFD3F\uFE10-\uFE19\uFE30-\uFE52\uFE54-\uFE61\uFE63\uFE68\uFE6A\uFE6B\uFF01-\uFF03\uFF05-\uFF0A\uFF0C-\uFF0F\uFF1A\uFF1B\uFF1F\uFF20\uFF3B-\uFF3D\uFF3F\uFF5B\uFF5D\uFF5F-\uFF65]+/u,B="or",q="and",rt="and_not",ct=(e,t)=>{e.includes(t)||e.push(t)},P=(e,t)=>{for(const s of t)e.includes(s)||e.push(s)},N=({score:e},{score:t})=>t-e,lt=()=>new Map,k=e=>{const t=new Map;for(const s of Object.keys(e))t.set(parseInt(s,10),e[s]);return t},G=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[B]:(e,t)=>{for(const s of t.keys()){const n=e.get(s);if(n==null)e.set(s,t.get(s));else{const{score:o,terms:u,match:i}=t.get(s);n.score=n.score+o,n.match=Object.assign(n.match,i),P(n.terms,u)}}return e},[q]:(e,t)=>{const s=new Map;for(const n of t.keys()){const o=e.get(n);if(o==null)continue;const{score:u,terms:i,match:r}=t.get(n);P(o.terms,i),s.set(n,{score:o.score+u,terms:o.terms,match:Object.assign(o.match,r)})}return s},[rt]:(e,t)=>{for(const s of t.keys())e.delete(s);return e}},dt=(e,t,s,n,o,u)=>{const{k:i,b:r,d:h}=u;return Math.log(1+(s-t+.5)/(t+.5))*(h+e*(i+1)/(e+i*(1-r+r*n/o)))},at=e=>(t,s,n)=>{const o=typeof e.fuzzy=="function"?e.fuzzy(t,s,n):e.fuzzy||!1,u=typeof e.prefix=="function"?e.prefix(t,s,n):e.prefix===!0;return{term:t,fuzzy:o,prefix:u}},ft={k:1.2,b:.7,d:.5},gt={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(it),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{typeof(console==null?void 0:console[e])=="function"&&console[e](t)},autoVacuum:!0},J={combineWith:B,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:ft},Ft={combineWith:q,prefix:(e,t,s)=>t===s.length-1},mt={batchSize:1e3,batchWait:10},U={minDirtFactor:.1,minDirtCount:20},pt={...mt,...U};class _t{constructor(t){if((t==null?void 0:t.fields)==null)throw new Error('SlimSearch: option "fields" must be provided');const s=t.autoVacuum==null||t.autoVacuum===!0?pt:t.autoVacuum;this._options={...gt,...t,autoVacuum:s,searchOptions:{...J,...t.searchOptions||{}},autoSuggestOptions:{...Ft,...t.autoSuggestOptions||{}}},this._index=new C,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=U,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[s,n]of this._index){const o={};for(const[u,i]of n)o[u]=Object.fromEntries(i);t.push([s,o])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,serializationVersion:2}}addFields(t){for(let s=0;s<t.length;s++)this._fieldIds[t[s]]=s}}const yt=(e,t)=>{const s=e._idToShortId.get(t);if(s!=null)return e._storedFields.get(s)},H=(e,t,s,n)=>{for(const o of Object.keys(e._fieldIds))if(e._fieldIds[o]===s){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${n}" was not present in field "${o}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},At=(e,t,s,n)=>{if(!e._index.has(n)){H(e,s,t,n);return}const o=e._index.fetch(n,lt),u=o.get(t);u==null||u.get(s)==null?H(e,s,t,n):u.get(s)<=1?u.size<=1?o.delete(t):u.delete(s):u.set(s,u.get(s)-1),e._index.get(n).size===0&&e._index.delete(n)},K=(e,t=B)=>{if(e.length===0)return new Map;const s=t.toLowerCase();return e.reduce(ht[s])||new Map},S=(e,t,s,n,o,u,i,r,h=new Map)=>{if(o==null)return h;for(const c of Object.keys(u)){const d=u[c],l=e._fieldIds[c],m=o.get(l);if(m==null)continue;let f=m.size;const g=e._avgFieldLength[l];for(const a of m.keys()){if(!e._documentIds.has(a)){At(e,l,a,s),f-=1;continue}const F=i?i(e._documentIds.get(a),s,e._storedFields.get(a)):1;if(!F)continue;const y=m.get(a),_=e._fieldLength.get(a)[l],b=dt(y,f,e._documentCount,_,g,r),E=n*d*F*b,A=h.get(a);if(A){A.score+=E,ct(A.terms,t);const w=G(A.match,s);w?w.push(c):A.match[s]=[c]}else h.set(a,{score:E,terms:[t],match:{[s]:[c]}})}}return h},Ct=(e,t,s)=>{const n={...e._options.searchOptions,...s},o=(n.fields||e._options.fields).reduce((a,F)=>({...a,[F]:G(n.boost,F)||1}),{}),{boostDocument:u,weights:i,maxFuzzy:r,bm25:h}=n,{fuzzy:c,prefix:d}={...J.weights,...i},l=e._index.get(t.term),m=S(e,t.term,t.term,1,l,o,u,h);let f,g;if(t.prefix&&(f=e._index.atPrefix(t.term)),t.fuzzy){const a=t.fuzzy===!0?.2:t.fuzzy,F=a<1?Math.min(r,Math.round(t.term.length*a)):a;F&&(g=e._index.fuzzyGet(t.term,F))}if(f)for(const[a,F]of f){const y=a.length-t.term.length;if(!y)continue;g==null||g.delete(a);const _=d*a.length/(a.length+.3*y);S(e,t.term,a,_,F,o,u,h,m)}if(g)for(const a of g.keys()){const[F,y]=g.get(a);if(!y)continue;const _=c*a.length/(a.length+y);S(e,t.term,a,_,F,o,u,h,m)}return m},X=(e,t,s={})=>{if(typeof t!="string"){const d={...s,...t,queries:void 0},l=t.queries.map(m=>X(e,m,d));return K(l,d.combineWith)}const{tokenize:n,processTerm:o,searchOptions:u}=e._options,i={tokenize:n,processTerm:o,...u,...s},{tokenize:r,processTerm:h}=i,c=r(t).flatMap(d=>h(d)).filter(d=>!!d).map(at(i)).map(d=>Ct(e,d,i));return K(c,i.combineWith)},Y=(e,t,s={})=>{const n=X(e,t,s),o=[];for(const[u,{score:i,terms:r,match:h}]of n){const c=r.length,d={id:e._documentIds.get(u),score:i*c,terms:Object.keys(h),match:h};Object.assign(d,e._storedFields.get(u)),(s.filter==null||s.filter(d))&&o.push(d)}return o.sort(N),o},zt=(e,t,s={})=>{s={...e._options.autoSuggestOptions,...s};const n=new Map;for(const{score:u,terms:i}of Y(e,t,s)){const r=i.join(" "),h=n.get(r);h!=null?(h.score+=u,h.count+=1):n.set(r,{score:u,terms:i,count:1})}const o=[];for(const[u,{score:i,terms:r,count:h}]of n)o.push({suggestion:u,terms:r,score:i/h});return o.sort(N),o},Et=({index:e,documentCount:t,nextId:s,documentIds:n,fieldIds:o,fieldLength:u,averageFieldLength:i,storedFields:r,dirtCount:h,serializationVersion:c},d)=>{if(c!==1&&c!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const l=new _t(d);l._documentCount=t,l._nextId=s,l._documentIds=k(n),l._idToShortId=new Map,l._fieldIds=o,l._fieldLength=k(u),l._avgFieldLength=i,l._storedFields=k(r),l._dirtCount=h||0,l._index=new C;for(const[m,f]of l._documentIds)l._idToShortId.set(f,m);for(const[m,f]of e){const g=new Map;for(const a of Object.keys(f)){let F=f[a];c===1&&(F=F.ds),g.set(parseInt(a,10),k(F))}l._index.set(m,g)}return l},Q=Object.entries,wt=Object.fromEntries,j=(e,t)=>{const s=e.toLowerCase(),n=t.toLowerCase(),o=[];let u=0,i=0;const r=(c,d=!1)=>{let l="";i===0?l=c.length>20?`… ${c.slice(-20)}`:c:d?l=c.length+i>100?`${c.slice(0,100-i)}… `:c:l=c.length>20?`${c.slice(0,20)} … ${c.slice(-20)}`:c,l&&o.push(l),i+=l.length,d||(o.push(["mark",t]),i+=t.length,i>=100&&o.push(" …"))};let h=s.indexOf(n,u);if(h===-1)return null;for(;h>=0;){const c=h+n.length;if(r(e.slice(u,h)),u=c,i>100)break;h=s.indexOf(n,u)}return i<100&&r(e.slice(u),!0),o},Z=/[\u4e00-\u9fa5]/g,tt=(e={})=>({fuzzy:.2,prefix:!0,processTerm:t=>{const s=t.match(Z)||[],n=t.replace(Z,"").toLowerCase();return n?[n,...s]:[...s]},...e}),et=(e,t,s={})=>{const n={};return Y(t,e,tt({boost:{h:2,t:1,c:4},...s})).forEach(o=>{const{id:u,terms:i,score:r}=o,h=u.includes("@"),c=u.includes("#"),[d,l]=u.split(/[#@]/),{contents:m}=n[d]??={title:"",contents:[]};if(h)m.push([{type:"customField",key:d,index:l,display:i.map(f=>o.c.map(g=>j(g,f))).flat().filter(f=>f!==null)},r]);else{const f=i.map(g=>j(o.h,g)).filter(g=>g!==null);if(f.length&&m.push([{type:c?"heading":"title",key:d,...c&&{anchor:l},display:f},r]),"t"in o)for(const g of o.t){const a=i.map(F=>j(g,F)).filter(F=>F!==null);a.length&&m.push([{type:"text",key:d,...c&&{anchor:l},display:a},r])}}}),Q(n).sort(([,o],[,u])=>u.contents.reduce((i,[,r])=>i+r,0)-o.contents.reduce((i,[,r])=>i+r,0)).map(([o,{title:u,contents:i}])=>{if(!u){const r=yt(t,o);r&&(u=r.h)}return{title:u,contents:i.map(([r])=>r)}})},st=(e,t,s={})=>zt(t,e,tt(s)).map(({suggestion:n})=>n),v=wt(Q(JSON.parse("{\"/zh/\":{\"documentCount\":105,\"nextId\":105,\"documentIds\":{\"0\":\"v-2d0ad528\",\"1\":\"v-2d0ad528@2\",\"2\":\"v-858cfdd6\",\"3\":\"v-564155e4\",\"4\":\"v-564155e4#目录\",\"5\":\"v-564155e4@2\",\"6\":\"v-230f5516\",\"7\":\"v-230f5516#_1-各数据集概述\",\"8\":\"v-230f5516@0\",\"9\":\"v-230f5516@1\",\"10\":\"v-947fe6ca\",\"11\":\"v-947fe6ca@0\",\"12\":\"v-947fe6ca@1\",\"13\":\"v-947fe6ca@2\",\"14\":\"v-01231baf\",\"15\":\"v-01231baf@0\",\"16\":\"v-01231baf@1\",\"17\":\"v-01231baf@2\",\"18\":\"v-6676e606\",\"19\":\"v-6676e606#_1-peft定义\",\"20\":\"v-6676e606#_2-peft分类\",\"21\":\"v-6676e606#_2-1-lora\",\"22\":\"v-6676e606#_2-2-adalora\",\"23\":\"v-6676e606#_2-3-prompt分类\",\"24\":\"v-6676e606#_2-4-prefix-tuning\",\"25\":\"v-6676e606#_2-5-prompt-tuning\",\"26\":\"v-6676e606#_2-6-p-tuning\",\"27\":\"v-6676e606#_2-7-各类提示微调对比\",\"28\":\"v-6676e606#_3-实验结果\",\"29\":\"v-6676e606#_4-参考文章\",\"30\":\"v-6676e606@0\",\"31\":\"v-6676e606@1\",\"32\":\"v-dfe0bb22\",\"33\":\"v-dfe0bb22#_1-公式解析\",\"34\":\"v-dfe0bb22#_2-非对称量化\",\"35\":\"v-dfe0bb22@0\",\"36\":\"v-dfe0bb22@1\",\"37\":\"v-33571859\",\"38\":\"v-33571859@0\",\"39\":\"v-33571859@1\",\"40\":\"v-33571859@2\",\"41\":\"v-60ef646e\",\"42\":\"v-60ef646e#_1-介绍\",\"43\":\"v-60ef646e#_2-优化算法\",\"44\":\"v-60ef646e#_2-1-remove-padding-算法\",\"45\":\"v-60ef646e#_2-2-融合的多头注意力\",\"46\":\"v-60ef646e#_2-3-cutlass-grouped-gemm\",\"47\":\"v-60ef646e#_3-变种-transformer-支持\",\"48\":\"v-60ef646e@0\",\"49\":\"v-60ef646e@1\",\"50\":\"v-60ef646e@2\",\"51\":\"v-1f54a3f4\",\"52\":\"v-1f54a3f4#_1-模型架构\",\"53\":\"v-1f54a3f4#_2-训练框架\",\"54\":\"v-1f54a3f4#_2-1-无监督预训练\",\"55\":\"v-1f54a3f4#_2-2-监督微调\",\"56\":\"v-1f54a3f4@0\",\"57\":\"v-1f54a3f4@1\",\"58\":\"v-6e6e5be0\",\"59\":\"v-6e6e5be0#_1-预备知识\",\"60\":\"v-6e6e5be0#_1-1-什么是ntp任务\",\"61\":\"v-6e6e5be0#_1-2-利用-llm-进行数据压缩\",\"62\":\"v-6e6e5be0#_1-3-压缩即智能\",\"63\":\"v-6e6e5be0#_2-gpt-模型对知识的提取过程\",\"64\":\"v-6e6e5be0#_3-知识点在-transformer-中的分布\",\"65\":\"v-6e6e5be0@0\",\"66\":\"v-6e6e5be0@1\",\"67\":\"v-5b6573b9\",\"68\":\"v-5b6573b9@0\",\"69\":\"v-5b6573b9@1\",\"70\":\"v-084e7ec6\",\"71\":\"v-084e7ec6@0\",\"72\":\"v-084e7ec6@1\",\"73\":\"v-084e7ec6@2\",\"74\":\"v-618590a0\",\"75\":\"v-618590a0#_1-问题提出\",\"76\":\"v-618590a0#_2-unlimiformer技术原理\",\"77\":\"v-618590a0#_2-1-unlimiformer编码\",\"78\":\"v-618590a0#_2-2-检索增强的交叉注意力机制\",\"79\":\"v-618590a0#_3-实验结果\",\"80\":\"v-618590a0#_3-1-长文档摘要\",\"81\":\"v-618590a0#_3-2-书籍摘要\",\"82\":\"v-618590a0@0\",\"83\":\"v-618590a0@1\",\"84\":\"v-2f6eb7a8\",\"85\":\"v-2f6eb7a8#_1-gpt系列模型发展历程\",\"86\":\"v-2f6eb7a8#_2-指令微调\",\"87\":\"v-2f6eb7a8#_3-模型的训练方法和数据集\",\"88\":\"v-2f6eb7a8#_4-上下文学习\",\"89\":\"v-2f6eb7a8#_5-参考\",\"90\":\"v-2f6eb7a8@0\",\"91\":\"v-2f6eb7a8@1\",\"92\":\"v-2f6eb7a8@2\",\"93\":\"v-e581d6e0\",\"94\":\"v-e581d6e0#_1-策略梯度算法\",\"95\":\"v-e581d6e0@0\",\"96\":\"v-e581d6e0@1\",\"97\":\"v-f6ba5632\",\"98\":\"v-f6ba5632@0\",\"99\":\"v-f6ba5632@1\",\"100\":\"v-f6ba5632@2\",\"101\":\"v-3c7ae03a\",\"102\":\"v-3c7ae03a@0\",\"103\":\"v-3c7ae03a@1\",\"104\":\"v-3c7ae03a@2\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[null,null,1],\"2\":[1,6],\"3\":[1],\"4\":[1,7],\"5\":[null,null,1],\"6\":[1,6],\"7\":[2,244],\"8\":[null,null,1],\"9\":[null,null,3],\"10\":[1],\"11\":[null,null,1],\"12\":[null,null,1],\"13\":[null,null,1],\"14\":[1],\"15\":[null,null,1],\"16\":[null,null,1],\"17\":[null,null,1],\"18\":[2,19],\"19\":[2,21],\"20\":[2,70],\"21\":[3,97],\"22\":[2,90],\"23\":[3,22],\"24\":[4,83],\"25\":[4,35],\"26\":[4,66],\"27\":[3,45],\"28\":[2,6],\"29\":[2,31],\"30\":[null,null,1],\"31\":[null,null,9],\"32\":[1,6],\"33\":[2,67],\"34\":[2,8],\"35\":[null,null,1],\"36\":[null,null,3],\"37\":[1],\"38\":[null,null,1],\"39\":[null,null,1],\"40\":[null,null,1],\"41\":[2,21],\"42\":[2,39],\"43\":[2],\"44\":[5,38],\"45\":[2,62],\"46\":[5,78],\"47\":[4,23],\"48\":[null,null,1],\"49\":[null,null,3],\"50\":[null,null,2],\"51\":[8,7],\"52\":[2,3],\"53\":[2],\"54\":[3,27],\"55\":[2,22],\"56\":[null,null,1],\"57\":[null,null,2],\"58\":[3,30],\"59\":[2],\"60\":[2,15],\"61\":[5,10],\"62\":[3,61],\"63\":[3,111],\"64\":[4,80],\"65\":[null,null,1],\"66\":[null,null,1],\"67\":[2,18],\"68\":[null,null,1],\"69\":[null,null,2],\"70\":[1],\"71\":[null,null,1],\"72\":[null,null,1],\"73\":[null,null,1],\"74\":[2,7],\"75\":[2,137],\"76\":[2,30],\"77\":[3,18],\"78\":[2,40],\"79\":[2],\"80\":[3,17],\"81\":[3,13],\"82\":[null,null,1],\"83\":[null,null,3],\"84\":[1,4],\"85\":[2,31],\"86\":[2,54],\"87\":[2,51],\"88\":[2,41],\"89\":[2,113],\"90\":[null,null,1],\"91\":[null,null,8],\"92\":[null,null,1],\"93\":[2,11],\"94\":[2,189],\"95\":[null,null,1],\"96\":[null,null,2],\"97\":[1],\"98\":[null,null,1],\"99\":[null,null,1],\"100\":[null,null,1],\"101\":[1],\"102\":[null,null,1],\"103\":[null,null,1],\"104\":[null,null,1]},\"averageFieldLength\":[2.1371580941154513,40.26642347678295,1.3123513550908044],\"storedFields\":{\"0\":{\"h\":\"主页\"},\"1\":{\"c\":[\"主页\"]},\"2\":{\"h\":\"介绍页\",\"t\":[\"HUST Artificial Intelligence and Embedded Lab\"]},\"3\":{\"h\":\"论文分享\"},\"4\":{\"h\":\"目录\",\"t\":[\"本页面包含一些论文分享的分类：\",\"语言模型\",\"提示技术\",\"微调技术\",\"评估方法\",\"数据集\",\"Token\"]},\"5\":{\"c\":[\"论文分享\"]},\"6\":{\"h\":\"指令数据集和提示数据集分享\",\"t\":[\"指令数据集和提示数据集在模型微调方面，尤其是在模型与人类认识对齐方面，作用巨大。本文针对一些质量较高的指令数据集和提示数据集，进行了简要介绍。\"]},\"7\":{\"h\":\"1 各数据集概述\",\"t\":[\"（1） Super-Natural Instruction 【Allen AI】\",\"这些自然语言指令清楚而完整地描述了一项任务（传统上定义为将输入字符串映射到输出字符串）。配备“理解”语言说明的模型，如果提供了任务说明，应该可以成功解决任何看不见的任务。\",\"（2）PromptSource【BigScience】\",\"项目链接：https://github.com/bigscience-workshop/promptsource BigScience 由 Hugging Face 和法国 CNRS，IDRIS，GENCI 等联合组织，是当下最大的开源 LLMs 组织之一。 BigScience 在 2021 年末开发了PromptSource项目，开源了一系列工具 toolkits，帮助研究者基于现有NLP 任务构建 prompt。截止目前，PromptSource 项目包含了 270 个 NLP 任务的超过 2000 个 prompt 模版。\",\"（3）P3【BigScience】\",\"项目链接：https://huggingface.co/datasets/bigscience/P3 语言：英文 在promptsource基础上，BigScience 构建了 P3 数据集。在 Hugging Face Hub 上你可以找到 P3 数据，P3 的数据规模在 100M-1B 之间。\",\"（4）xMTF 【BigScience，包含中文】\",\"项目链接：https://huggingface.co/datasets/bigscience/P3\",\"BigScience 在英语 prompt 的基础上，扩展其 prompt 到多种非英语语言。 该项目包含了 13 个 NLP 任务，并采用了 46 个不同的语言的版本。对应的 prompt 包含的语种个数不定。\",\"（5）HH-RLHF【Anthropic】\",\"项目链接：https://github.com/anthropics/hh-rlhf 数量： 训练集：161k 测试集：8.55k Anthropic 公司旗下的 Claud 是 ChatGPT 的主要竞品之一。 Anthropic 开源了其在自己产品线中使用的 RLHF 数据集： 链接：https://huggingface.co/datasets/Anthropic/hh-rlhf\",\"（6）Unnatural Instruction【orhonovich】\",\"使用 LLMs 自主生成 instruction 数据是 instruct-tuning 领域较为活跃的一个方向。 Unnatural Instruction 使用 GPT3（text-davinci-002）生成了 64k 的 instruction prompt 数据。并使用同样的模型将 64k 的 prompt 进行改写，最终得到了 240k 条 instruction 数据。 论文中显示，在 Instruct-Tuning 中 LLMs 自主生成的 prompt 表现出了良好的效果，甚至超过了在 P3 等数据上进行微调的 T0 等模型。\",\"（7）Self-Instruct【yizhongw】\",\"项目链接：https://github.com/yizhongw/self-instruct Self-Instruct 同样是使用 LLMs 生成 prompt 进行 instruct-tuning 的思路。不过使用了更 fine-grained 的生成流程。 Task pool 和 Quality filtering 等概念被引入，部分缓解了 self-intrauct 类型数据的 noise 问题\",\"（8）UnifiedSKG 【HKU】\",\"项目主页 ：https://unifiedskg.com/\",\"UnifiedSKG 在 Text-to-Text 的框架中加入了 knowledge grounding，也就是在 prompt-output 的框架中，加入了结构化数据做辅助，共21个任务数据集，\",\"解决问题：做打破彼此任务之间的边界的第一次简单尝试，使得这些可以在同一个UnifiedSKG framework下进行学习并在这些任务上取得不错的结果\",\"（9）Flan Collection【Google】\",\"项目链接：https://github.com/google-research/FLAN/tree/main/flan/v2 Google 在这个项目中将自己的 Flan 2021 数据与一些开源的 instruction 数据（P3，super-natural instruction 等）进行了合并\",\"（10）InstructDial【prakharguptaz】\",\"项目链接：https://github.com/prakharguptaz/Instructdial/tree/main/datasets InstructDial 是在特定的一种任务类型上进行指令微调的尝试。实验结果表明，在对话指令数据上微调后，模型在对话任务上的表现强于在超大规模任务集上的结果\",\"为方便读者阅读，上述数据集可以总结概括为以下表格\",\"数据集/项目名称\",\"组织/作者\",\"简介\",\"Natural Instruction / Super-Natural Instruction\",\"Allen AI\",\"包含61个NLP任务（Natural Instruction）和1600个NLP任务（Super-Natural Instruction）的指令数据\",\"PromptSource / P3\",\"BigScience\",\"包含270个NLP任务的2000多个prompt模版（PromptSource）和规模在100M-1B之间的P3数据集\",\"xMTF\",\"BigScience\",\"包含13个NLP任务、46种语言的多语言prompt数据\",\"HH-RLHF\",\"Anthropic\",\"旨在训练Helpful and Harmless（HH）的LLMs的RLHF数据集\",\"Unnatural Instruction\",\"orhonovich\",\"使用GPT3生成64k的instruction prompt数据，经改写后得到240k条instruction数据\",\"Self-Instruct\",\"yizhongw\",\"使用LLMs生成prompt进行instruct-tuning的方法，引入Task pool和Quality filtering等概念\",\"UnifiedSKG\",\"HKU\",\"在Text-to-Text框架中加入knowledge grounding，将结构化数据序列化并嵌入到prompt中\",\"Flan Collection\",\"Google\",\"将Flan 2021数据与一些开源的instruction数据（P3，super-natural instruction等）进行合并\",\"InstructDial\",\"prakharguptaz\",\"在特定的一种任务类型（对话指令）上进行指令微调的尝试\",\"Alpaca\",\"Stanford\",\"53k 数据, 非常优秀的表现(GPT-3.5 level).\",\"阅读原文\"]},\"8\":{\"c\":[\"数据集\"]},\"9\":{\"c\":[\"Instruct Tuning\",\"Prompt Tuning\"]},\"10\":{\"h\":\"数据集\"},\"11\":{\"c\":[\"数据集\"]},\"12\":{\"c\":[\"Dataset\"]},\"13\":{\"c\":[\"数据集\"]},\"14\":{\"h\":\"评估方法\"},\"15\":{\"c\":[\"评估方法\"]},\"16\":{\"c\":[\"Eval\"]},\"17\":{\"c\":[\"评估方法\"]},\"18\":{\"h\":\"PEFT：最先进的参数高效微调方法\",\"t\":[\"参数高效微调 （PEFT） 方法能够将预训练的语言模型 （PLM） 有效地适应各种下游应用程序，而无需微调模型的所有参数。微调大型 PLM 的成本通常高得令人望而却步。在这方面，PEFT方法仅微调少量（额外）模型参数，从而大大降低了计算和存储成本。\",\"代码地址：https://github.com/huggingface/peft\"]},\"19\":{\"h\":\"1 PEFT定义\",\"t\":[\"PEFT，即参数高效微调 （Parameter-Efficient Fine-Tuning）技术，同时是Hugging Face开源的一个高效微调大模型的库。\",\"PEFT能够将预训练的语言模型 （PLM） 有效地适应各种下游应用程序，而无需微调模型的所有参数。在微调大型 PLM时，PEFT方法仅微调少量（额外）模型参数，从而大大降低了计算和存储成本。最近的PEFT技术实现了与完全微调相当的性能。\"]},\"20\":{\"h\":\"2 PEFT分类\",\"t\":[\"Hugging Face开源的PEFT库目前支持5种方法，分别是：\",\"（1）LoRA: LoRA: Low-Rank Adaptation of Large Language Models(微软，2021年10月)\",\"（2）AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning(微软，2023年3月)\",\"（3）Prefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation(斯坦福，2021年8月)；P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks(清华KEG，2022年3月20)；Prefix Tuning在input前面加入prefix部分，并针对拥有自由参数的prefix部分进行微调训练\",\"（4）P-Tuning: GPT Understands, Too(清华，北京智源，2021年3月18)；P-Tuning将prompt对应的token替换为可训练的嵌入，并进行微调训练\",\"（5）Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning(谷歌，2021年9月)；Prompt Tuning针对每一类任务，训练出任务对应prompt的embedding向量\",\"其中，Prefix Tuning、P-Tuning、Prompt Tuning可理解为针对prompt部分的微调。\"]},\"21\":{\"h\":\"2.1 LoRA\",\"t\":[\"LoRA，英文全称Low-Rank Adaptation of Large Language Models，直译为大语言模型的低阶适应，是微软的研究人员为了解决大语言模型微调而开发的一项技术。\",\"LoRA的做法是，冻结预训练好的模型权重参数，然后在每个Transformer块里注入可训练的层，由于不需要对模型的权重参数重新计算梯度，所以，大大减少了需要训练的计算量。\",\"结合上图，可以直观地理解LoRA的实现原理。LoRA冻结预训练模型权重，并将可训练的秩分解矩阵注入到Transformer层的每个权重中，大大减少了下游任务的可训练参数数量。直白的来说，实际上是增加了右侧的“旁支”，也就是先用一个Linear层A，将数据从 d维降到r，再用第二个Linear层B，将数据从r变回d维。最后再将左右两部分的结果相加融合，得到输出的hidden_state。\",\"对于左右两个部分，右侧看起来像是左侧原有矩阵W的分解，从而将参数量从 n ∗ n 变成了n * r + n * r ，在 r < < n 的情况下，参数量就大大地降低了。\",\"事实上，该思想与Albert的思想有异曲同工之处，在Albert中，作者通过两个策略降低了训练的参数量，其一是Embedding矩阵分解，其二是跨层参数共享。\",\"在Albert中，作者考虑到词表的维度很大，所以将Embedding矩阵分解成两个相对较小的矩阵，用来模拟Embedding矩阵的效果，这样一来需要训练的参数量就减少了很多。\",\"LORA也是类似的思想，并且它不再局限于Embedding层，而是所有出现大矩阵的地方，理论上都可以用到这样的分解。\",\"但是与Albert不同的是，Albert直接用两个小矩阵替换了原来的大矩阵，而LORA保留了原来的矩阵W，但是不让W参与训练，所以需要计算梯度的部分就只剩下旁支的A和B两个小矩阵。\",\"从论文中的公式来看，在加入LORA之前，模型训练的优化表示为：\",\"其中，模型的参数用 Φ 表示。\",\"而加入了LORA之后，模型的优化表示为：\",\"其中，模型原有的参数是Φ ，LORA新增的参数是Δ Φ ( Θ )。\",\"从第二个式子可以看到，尽管参数看起来增加了（多了Δ Φ ( Θ ) ），但是从前面的max的目标来看，需要优化的参数只有Θ ，而根据假设，Θ < < Φ，这就使得训练过程中，梯度计算量少了很多，所以就在低资源的情况下，我们可以只消耗Θ这部分的资源，这样一来就可以在单卡低显存的情况下训练大模型了。\",\"但是相应地，引入LoRA部分的参数，并不会在推理阶段加速，因为在前向计算的时候，Φ部分还是需要参与计算的，而Θ部分是凭空增加了的参数，所以理论上，推理阶段应该比原来的计算量增大一点。\",\"根据论文的研究结果分析，LoRA的微调质量与全模型微调相当。\"]},\"22\":{\"h\":\"2.2 AdaLoRA\",\"t\":[\"AdaLoRA，即自适应预算分配以实现参数有效的微调，是微软与佐治亚理工学院共同提出的一种微调优化方法。\",\"由于在不太重要的权重矩阵添加更多的参数会产生很少的收益，甚至会损害模型性能，因此论文提出了以下问题：\",\"如何根据模块的重要性自适应地分配参数预算，以提高参数高效微调的性能？\",\"为了回答这个问题，论文提出了一种新的方法——AdaLoRA（自适应的低秩自适应），该方法在类似LoRA的微调过程中在权重矩阵之间动态分配参数预算。具体而言，AdaLoRA调整增量矩阵的秩，以控制其预算。\",\"关键的增量矩阵被分配了高秩，这样它们可以捕获更细粒度和特定于任务的信息。不太重要的增量矩阵被修剪为具有较低的秩，以防止过度拟合并节省计算预算。\",\"AdaLoRA包含两个重要组成部分：\",\"（1）基于SVD的自适应，它以奇异值分解的形式表示增量矩阵∆；\",\"（2）重要性感知秩分配，它根据我们新设计的重要性度量修剪冗余奇异值。\",\"提示\",\"奇异值：特征值的平方根\",\"论文提出了两种重要性度量的方式，分别是：\",\"（1）基于奇异值的重要性度量\",\"（2）基于敏感性的重要性度量\",\"在AdaLoRA中，以奇异值分解的形式对权重矩阵的增量更新进行参数化。然后，根据新的重要性指标，通过操纵奇异值，在增量矩阵之间动态地分配参数预算。这种方法可以有效地提高模型性能和参数效率。\",\"AdaLoRA根据重要性评分自适应地分配参数预算，通过对权重矩阵进行重要性评分，有效地分配参数预算。\",\"在现有的矩阵近似文献中，有一些控制矩阵秩的方法（Cai等人，2010；Koltchinskii等人，2011；Toh & Yun，2010）。它们大多直接计算矩阵的奇异值分解（SVD），然后截断最小的奇异值。这样的操作可以显式地操纵秩，更重要的是，最小化结果矩阵和原始矩阵之间的差异。\",\"然而，对于微调大型模型，迭代地将SVD应用于大量高维权重矩阵会变得非常昂贵。因此，论文没有精确计算SVD，而是将∆参数化为∆=P∧Q，以模拟SVD。对角矩阵∧包含奇异值，而正交矩阵P和Q表示∆的左/右奇异向量。为了正则化P和Q的正交性，在训练损失中增加了额外的惩罚。这样的参数化避免了SVD的密集计算。此外，另一个优点是，该方法只需要在保持奇异向量的同时删除不重要的奇异值。这保留了未来恢复的可能性，并稳定了训练。\",\"基于SVD参数化，AdaLoRA通过重要性评分动态调整∆=P V Q的等级。\",\"具体来说，AdaLoRA将增量矩阵P∧Q划分为三元组，其中每个三元组Gi包含第i个奇异值和相应的奇异向量。为了量化三元组的重要性，AdaLoRA提出了一种新的重要性度量，它考虑了Gi中每个条目对模型性能的贡献。\",\"具有低重要性分数的三元组被授予低优先级，因此奇异值被清零。具有高度重要性的三元组会被保留，并进行微调。\"]},\"23\":{\"h\":\"2.3 prompt分类\",\"t\":[\"prompt分为hard prompt与soft prompt两种，这两种prompt的含义如下。\",\"（1）hard prompt 又称为 Discrete Prompt，离散prompt是一个实际的文本字符串\",\"（2）soft prompt 又称为 Continuous Prompts，连续prompt直接在底层语言模型的嵌入空间中进行描述\",\"prompt的制作分为手工创建prompt和自动化生成prompt，而自动化生成prompt又分为离散提示（又叫做硬提示）和连续提示（又叫做软提示）\"]},\"24\":{\"h\":\"2.4 Prefix Tuning\",\"t\":[\"前缀微调（Prefix-Tuning），是用于 生成任务(NLG) 的轻量微调。\",\"Prefix-Tuning与Full-finetuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。\",\"该方法其实和构造Prompt类似，只是利用多层感知编码prefix，注意多层感知机就是prefix的编码器，不再像Prompt是人为构造的“显式”的提示,并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。\",\"对于Decoder-Only的GPT，prefix只加在句首，[PREFIX, x, y]，对于Encoder-Decoder的BART，不同的prefix同时加在编码器和解码器的开头，[PREFIX, x, PREFIX', y]。在下游微调时，LM的参数被冻结，只有prefix部分的参数进行更新。不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新。\",\"Prefix-Tuning将一系列连续的task-specific向量添加到input前面，称之为前缀，如下图中的红色块所示。\",\"Prefix-Tuning的作者提出了Prefix Tuning，该方法冻结LM参数，并且只优化Prefix（红色前缀块）。因此，只需要为每个任务存储前缀，使前缀调优模块化并节省空间。\",\"与提示（prompt ）不同的是，前缀完全由自由参数组成，与真正的token不对应。相比于传统的微调，前缀微调只优化了前缀。因此，我们只需要存储一个大型Transformer和已知任务特定前缀的副本，对每个额外任务产生非常小的开销。\",\"原论文仅在以下任务中进行了比较：\",\"（1）table-to-text生成任务：GPT-2\",\"（2）生成式摘要任务：BART\",\"Prefix-tuning的prompt拼接方式\",\"Prefix-tuning是做生成任务，它根据不同的模型结构定义了不同的Prompt拼接方式，在GPT类的自回归模型上采用[PREFIX, x, y]，在T5类的encoder-decoder模型上采用[PREFIX, x, PREFIX', y]：\",\"值得注意的还有三个改动：\",\"（1）把预训练大模型freeze住，因为大模型参数量大，精调起来效率低，毕竟prompt的出现就是要解决大模型少样本的适配；\",\"（2）作者发现直接优化Prompt参数不太稳定，加了个更大的MLP，训练完只保存MLP变换后的参数就行了；\",\"（3）实验证实只加到embedding上的效果不太好，因此作者在每层都加了prompt的参数，改动较大。\"]},\"25\":{\"h\":\"2.5 Prompt Tuning\",\"t\":[\"Prompt-tuning 固定预训练参数，为每一个任务（a1、a2、b1、b2）额外添加一个或多个 embedding（A、B、C）。\",\"之后拼接 query 正常输入 LLM ，并只训练这些 embedding 。左图为单任务全参数微调，右图为 prompt tuning 。\",\"Prompt-tuning给每个任务定义了自己的Prompt，拼接到数据上作为输入，同时freeze预训练模型进行训练，在没有加额外层的情况下，可以看到随着模型体积增大效果越来越好，最终追上了精调的效果：\",\"同时，Prompt-tuning还提出了Prompt-ensembling，也就是在一个batch里同时训练同一个任务的不同prompt，这样相当于训练了不同「模型」，比模型集成的成本小多了。\"]},\"26\":{\"h\":\"2.6 P-Tuning\",\"t\":[\"Prompting最初由人工设计Prompt，自然语言提示本身十分脆弱（如下图所示，选择不同的Prompt对下游任务的性能影响较大），而且从优化角度无法达到最优。\",\"为消除这一影响，P Tuning技术应用而生：P-Tuning v1将自然语言提示的token，替换为可训练的嵌入，同时利用LSTM进行Reparamerization加速训练，并引入少量自然语言提示的锚字符（Anchor，例如Britain）进一步提升效果，如下图b所示：\",\"P-Tuning v1，对于BERT类双向语言模型采用模版(P1, x, P2, [MASK], P3)，对于单向语言模型采用(P1, x, P2, [MASK])。\",\"P-Tuning v2提升小模型上的Prompt Tuning，最关键的就是引入Prefix-tuning技术。\",\"Prefix-tuning（前缀微调）最开始应用在NLG任务上，由[Prefix, x, y]三部分构成，如上图所示：Prefix为前缀，x为输入，y为输出。Prefix-tuning将预训练参数固定，Prefix参数进行微调：不仅只在embedding上进行微调，也在TransFormer上的embedding输入每一层进行微调。\",\"P-Tuning v2将Prefix-tuning应用于在NLU任务，如下图所示：\",\"p tuning v2简单来说其实是soft prompt的一种改进。\",\"soft prompt是只作用在embedding层中，实际测试下来只作用在embedding层的话交互能力会变弱，而且冻结模型所有参数去学习插入token，改变量偏小使得效果有时候不太稳定，会差于微调。\",\"p tuning v2则不只是针对embedding层，而是将连续型token插入每一层，增大改变量和交互性。\",\"soft prompt比较依靠模型参数量，在参数量超过10B的模型上，效果追上了fine-tune，但是p tuning v2因为每层插入了token，增大模型训练的改变量，更加适用于小一点的模型。\"]},\"27\":{\"h\":\"2.7 各类提示微调对比\",\"t\":[\"模型：P-tuning （自动化地寻找连续空间中的知识模板） 特点：hard+soft 方法：传统离散prompt直接将模板T的每个token映射为对应的embedding，而P-Tuning将模板T中的Pi（Psedo Prompt）映射为一个可训练的参数 hi。使用BiLSTM对Pi序列进行表征，并加入锚字符（Anchor）提升效果。\",\"模型：Prefix-Tuning 特点：生成任务，soft prompt 方法：在每层transformer 之前加入prefix，Prefix不是真实的 token，而是连续向量 （soft prompt）。\",\"模型：Prompt tuning 特点：prefix-tuning的简化 方法：固定预训练模型，只对下游任务的输入添加额外的 k个 可学习的 token。\",\"模型：P-tuning v2 特点：prefix-tuning的deep形式 方法：prefix-tuning仅在transformer的 第一层加入soft prompt，p tuning v2 提出 Deep Prompt Tuning的方法，在transformer 的每一层之前都加入了soft prompt。\"]},\"28\":{\"h\":\"3 实验结果\",\"t\":[\"根据结果可以看出，在只训练1个epoch的情况下，只有LoRA与AdaLoRA的效果接近全参数微调，并且LoRA与全参数微调的差距不超过0.1%\"]},\"29\":{\"h\":\"4 参考文章\",\"t\":[\"[1] 使用PEFT微调LLMs\",\"[2] 《Prefix-Tuning: Optimizing Continuous Prompts for Generation》阅读笔记\",\"[3] Prefix-Tunning\",\"[4] 【prompt】什么是 Soft Prompt 和 Hard Prompt ?\",\"[5] 【调研】Soft Prompt Tuning 模型发展调研：P-tuning,Prefix-tuning,Prompt-tuning,P-tuning v2\",\"[6] prompt综述\",\"[7] Prompt范式第二阶段｜Prefix-tuning、P-tuning、Prompt-tuning\"]},\"30\":{\"c\":[\"微调技术\"]},\"31\":{\"c\":[\"PEFT\",\"Hugging Face\",\"LoRA\",\"AdaLoRA\",\"Prefix Tuning\",\"P-Tuning\",\"Prompt Tuning\"]},\"32\":{\"h\":\"int8量化技术原理讲解\",\"t\":[\"int量化技术是一种节约大模型推理或训练的过程中占用的显存的技术。量化的目是为了减少计算时间和计算能耗 。在一些场景下对能耗和时间的要求，要高于模型的指标，所以在这种情况下量化是一个必然的选择。\"]},\"33\":{\"h\":\"1 公式解析\",\"t\":[\"基准：普通的Linear层：y=Wx+b\",\"x：tensor([1., 2., 3., 4.], device='cuda:0') W：tensor([[ 0.4753, 0.4548, -0.2720, 0.0310], [-0.3591, -0.4820, -0.3717, -0.2604]], device='cuda:0',requires_grad=True) b：tensor([-0.4314, 0.1237], device='cuda:0', requires_grad=True) y：tensor([ 0.2612, -3.3559], device='cuda:0', grad_fn=<AddBackward0>) \",\"（1）令W=TW′，其中T是一个对角矩阵，相当于W′的每行乘以一个系数。\",\"（2）选定T保证W′的每一行四舍五入到整型之后最大值为127或者最小值为−127即可，因此T完全由W决定。\",\"T的对角元素：tensor([0.0037, 0.0038], device='cuda:0', dtype=torch.float16)\",\"W'：tensor([[ 127, 122, -73, 8], [ -95, -127, -98, -69]], device='cuda:0', dtype=torch.int8) b：tensor([-0.4314, 0.1237], device='cuda:0', dtype=torch.float16) \",\"（3）前向传播的计算公式变成了 y=TW′x+b。\",\"（4）量化操作仅针对W，不针对b。量化之后，网络相当于舍弃了W，而保留了W′和T。W′由于变成了int8整型，因此对显存来说相当于多存了T的对角元素，少存了W的一半大小，总体上显存的压力是大大变小了。\",\"y：tensor([ 0.2571, -3.3652], device='cuda:0', dtype=torch.float16) \"]},\"34\":{\"h\":\"2 非对称量化\",\"t\":[\"以上描述的过程是对称量化，对称量化把每一行的绝对值的最大值变换到127，而非对称量化是把每一行的最大值变换到127，最小值变换到−128，因此非对称量化的W′=TW−p，除了多一个T的对角元素之外还多一个偏移向量。\"]},\"35\":{\"c\":[\"微调技术\"]},\"36\":{\"c\":[\"优化\",\"内存\",\"机器学习\"]},\"37\":{\"h\":\"微调技术\"},\"38\":{\"c\":[\"微调技术\"]},\"39\":{\"c\":[\"Finetune\"]},\"40\":{\"c\":[\"微调技术\"]},\"41\":{\"h\":\"大幅优化推理速度-ByteTransformer\",\"t\":[\"论文提出了字节跳动的GPU Transformer推理库——ByteTransformer。针对自然语言处理常见的可变长输入，论文提出了一套优化算法，这些算法在保证运算正确性的前提下，成功避免了传统实现中的冗余运算，实现了端到端的推理过程的大幅优化。\",\"图1 论文信息\",\"论文地址：https://arxiv.org/abs/2210.03052 代码地址：https://github.com/bytedance/ByteTransformer\"]},\"42\":{\"h\":\"1 介绍\",\"t\":[\"现有的一些深度学习框架，如Tensorflow，PyTorch，TVM以及NVIDIA TensorRT等，要求输入序列长度相同，才能利用批处理加速Transformer计算。然而，在实际场景中，输入序列通常是变长的，而零填充会引入大量的额外计算开销。字节跳动AML团队先前提出的“effective Transformer”，通过对输入的重排列，实现了 QKV projection 和 MLP 的 padding free，但 self attention 部分仍然需要 padding。 为了解决这个问题，字节跳动 AML 团队提出了 ByteTransformer，它实现了变长输入的 padding free 计算，并且实现了全面的 kernel fusion 以进一步提高性能。\"]},\"43\":{\"h\":\"2 优化算法\"},\"44\":{\"h\":\"2.1 Remove padding 算法\",\"t\":[\"这个算法源自字节跳动 AML 团队之前的工作 \\\"effective Transformer\\\"，在 NVIDIA 开源 FasterTransformer 中也有集成。ByteTransformer 同样使用该算法去除对 attention 外矩阵乘的额外计算。\",\"图2 Remove padding 算法\",\"算法步骤如下。\",\"（1）计算 attention mask 的前缀和，作为 offsets。\",\"（2）根据 offsets 把输入张量从 [batch_size, seqlen, hidden_size] 重排列为 [valid_seqlen, hidden_size] ，再参与后续的矩阵乘计算，实现 padding free。\"]},\"45\":{\"h\":\"2.2 融合的多头注意力\",\"t\":[\"旧版的多头注意力：多头注意力 (Multi-Head)，具体是在计算时对注意力做一些变形，每个输入产生多组 Q、K、V（生成几组就是几个头），每组各自计算互不影响，最后把输出拼接在一起作为总输出（可能要再乘一个矩阵来调整形状）。\",\"为了优化 attention 部分的性能，ByteTransformer 中实现了融合的多头注意力（Fused Multi-Head Attention）算子。对于 seqlen 长度，以 384 为界划分为两种实现方式。\",\"（1）对于短 seqlen, 因为可以把 QK 整行放在共享内存进行 softmax 操作，通过手写 kernel 的方式实现，矩阵乘通过调用 wmma 接口使用 TensorCore 保证高性能。\",\"（2）对于长 seqlen, 因为共享内存大小限制，不能在一个手写 kernel 中完成所有操作。基于高性能的 CUTLASS grouped GEMM, 分成两个 gemm kernel 实现，并把 add_bias, softmax 等操作 fused 到 GEMM kernel 中。\"]},\"46\":{\"h\":\"2.3 CUTLASS grouped GEMM\",\"t\":[\"NVIDIA 开发的 grouped GEMM 可以在一个 kernel 中完成多个独立矩阵乘问题的计算，利用这个性质可以实现 Attention 中的 padding free。\",\"（1）Attention 中的两次矩阵乘操作，都可以拆解为 batch_size x head_num 个独立的矩阵乘子问题。\",\"（2）每个矩阵乘子问题，把问题大小传入到 grouped GEMM，其中 seqlen 传递真实的 valid seqlen 即可。\",\"grouped GEMM 原理：kernel 中每个 threadblock (CTA) 固定分块大小，每个矩阵乘子问题根据问题大小和分块大小，拆解为不同数量的待计算块，再把这些块平均分配到每个 threadblock 中进行计算。\",\"图3 grouped GEMM 原理\",\"使用 grouped GEMM 实现 attention 时，由于子问题的数量 batch_size x head_num 通常较大，读取子问题参数会有不小的开销，因为从线程角度看，每个线程都需要遍历读取所有的子问题大小。为了解决这个问题，ByteTransformer 对 grouped GEMM 中读取子问题参数进行了性能优化，使其可以忽略不计。\",\"（1）共享子问题参数。对同一个输入，不同 head 的 valid seqlen 相同，problem size 也相同，通过共享使参数存储量从 batch_size x head_num 减少到 batch_size。\",\"（2）warp prefetch. 原始实现中，每个 CUDA thread 依次读取所有的子问题 problem size，效率很低。改为一个 warp 内线程读取连续的 32 个子问题参数，然后通过 warp 内线程通信交换数据，每个线程的读取次数降低到 1/32。\",\"图4 warp prefetch\"]},\"47\":{\"h\":\"3 变种 Transformer 支持\",\"t\":[\"目前，字节跳动 AML 团队已经在 GitHub 上开源了 ByteTransformer 的标准 BERT 实现。除此之外，字节内部版本还支持了许多 Transformer 变种，比如 Deberta, Roformer，T5 等等。代码实现易于拓展，并且上述各种优化手段也可以方便地应用到变种 Transformer 中。\"]},\"48\":{\"c\":[\"语言模型\"]},\"49\":{\"c\":[\"transformer\",\"优化\",\"字节\"]},\"50\":{\"c\":[\"大幅优化推理速度-ByteTransformer\"]},\"51\":{\"h\":\"GPT论文分享：Improving Language Understanding by Generative Pre-Training\",\"t\":[\"作者证明了通过在大量未标注文本上对语言模型进行生成式预训练，然后在每个特定任务上进行歧视性微调，可以在这些任务上实现巨大收益。与以前的方法相比，他们在微调期间利用面向任务的输入转换来实现有效的转移，同时对模型架构所需的更改最小。\"]},\"52\":{\"h\":\"1 模型架构\",\"t\":[\"图1.1 GPT架构图\"]},\"53\":{\"h\":\"2 训练框架\"},\"54\":{\"h\":\"2.1 无监督预训练\",\"t\":[\"给定一个无监督的token语料库U={u1​,⋯,un​}，作者使用标准语言建模目标来最大化以下概率。\",\"其中k是上下文窗口的大小，条件概率P使用具有参数Θ的神经网络来建模。使用随机梯度下降训练这些参数。\",\"在作者的实验中，作者将多层Transformer decoder用于语言模型，这是Transformer的变体。该模型在输入上下文token上应用multi-headed self-attention操作，然后是position-wise前馈层，以在目标token上产生输出分布。\",\"其中U=(U−k,⋯,U−1)是token的上下文向量，n是层数，是token嵌入矩阵，Wp是position嵌入矩阵。\"]},\"55\":{\"h\":\"2.2 监督微调\",\"t\":[\"在预训练之后，作者将参数调整为受监督的目标任务。假设有一个标记的数据集C，其中每个实例由一系列输入token以及标签。输入通过作者的预训练模型，以获得最终Transformer块的激活，然后将其送到添加的具有参数的线性输出层来以预测。\",\"因此，优化目标变成了以下式子。\",\"作者还发现，将语言建模作为微调的辅助目标，通过以下方面体现。\",\"（1）改进监督模型的泛化；\",\"（2）加速收敛，有助于学习。\",\"之前的工作也观察到了这种辅助目标的改进性能。具体而言，作者优化了以下目标（带参数λ）。\"]},\"56\":{\"c\":[\"语言模型\"]},\"57\":{\"c\":[\"模型\",\"深度学习\"]},\"58\":{\"h\":\"LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享\",\"t\":[\"本文主要分享的内容为以下两点。 (1) LLM的信息压缩能力与其智能水平的关系 (2) GPT对知识的提取与存储方式\",\"知乎原文：https://zhuanlan.zhihu.com/p/632795115 版权归属原作者，如涉侵权，请联系删除\",\"一种观点认为GPT 4 这种 LLM 模型仅仅学会了语言中的单词共现等浅层的表面统计关系，其实并未具备智能，只是类似鹦鹉学舌的语言片段缝合怪而已；另外一种观点则认为：GPT 4 不仅学会了语言元素间的表面统计关系，而且学到了人类语言甚至包括物理世界的内在运行规律，文字是由内在智能产生的，所以 LLM 具备类人智能。\"]},\"59\":{\"h\":\"1 预备知识\"},\"60\":{\"h\":\"1.1 什么是NTP任务\",\"t\":[\"目前规模够大的 LLM 模型，在训练基座模型的时候，都采用「Next Token Prediction，NTP」 (后文为了书写简洁，有时会简称为 NTP) 任务。Next Token Prediction 如此简单的操作，就是通过语言中前面的单词，来产生下一个单词\"]},\"61\":{\"h\":\"1.2 利用 LLM 进行数据压缩\",\"t\":[\"如果大语言模型具备越强的数据压缩能力，是否意味着它具备越强的 AGI 智能呢？ 可以举个例子来解释这种数据压缩能力 把LLM看做函数，根据已有的token，计算下一个token的在词表中的概率分布，根据输出的下一个token的概率分布进行算术编码，使用编码后的数据进行数据传输\"]},\"62\":{\"h\":\"1.3 压缩即智能\",\"t\":[\"如果 GPT 模型智能程度越高，NTP 预测得越准确，则其压缩效率就越高。所以，我们可以根据模型的压缩效率来评估模型的智能程度，模型压缩效率越高，则模型智能程度越高，这是目前 OpenAI 照此思路推进大模型研发方向的一个核心理念。\",\"可以就这个思路深入思考两个相关问题。 （1）第一个问题： 上面讲述内容是以数据压缩的视角来看待 LLM 的智能水准，问题是为何模型压缩能力越强，就代表了它具备更高的智能呢？\",\"相对大量数据，数据内在规律的描述，自然就短得多，而模型若能给出越短的描述，说明这个模型学到了更多的内在规律，所以就越聪明。是这个逻辑，举个例子。 假设要传输的序列是连续质数数字序列 下面是gpt-3.5-turbo和oasst两个模型的回答结果。\",\"图1 两个模型针对质数概念理解的测试对比\",\"可以看出，gpt3.5 是学会了质数这种抽象概念的，否则这道题很难回答好，如果不理解这个概念，就会出现图右小模型这种不知所云的回答。这一方面说明大模型确实可以学习一些抽象概念，另一方面说明大模型在这方面表现确实比小模型要好。\",\"（2）第二个问题： 如果我们更严谨地来看，会发现尽管 LLM 训练过程可以看成是对数据的无损压缩，但是能够达成「无损」 的效果，并不单单靠 LLM，其实是「LLM + 算术编码」一起完成的。 数据无损压缩 = LLM 模型的有损数据压缩能力 + 算术编码的编码补偿能力\"]},\"63\":{\"h\":\"2 GPT 模型对知识的提取过程\",\"t\":[\"论文：Dissecting Recall of Factual Associations in Auto-Regressive Language Models 剖析自回归语言模型中事实关联的回忆\",\"图2 GPT模型对知识的提取归纳过程示意图\",\"经过研究，发现 GPT 在提取这条知识的时候，经历了明显的三阶段过程： （1） 主题补充 单词 「music」是描述这个实体最后的、也是最关键的词汇，它的信息在顺着 Transformer block 往上走的过程中，先通过 Attention 把之前的修饰语「beats」 相关信息集成到「music」 对应位置。之后，随着 Transformer 层数越来越高，通过每个 Transformer Block 的 FFN 层，不断往「music」对应的 Embedding 里增加信息，所以随着信息往上层流动，「music」这个单词对应层数的 Embedding，能够触发越来越多的与「Beat music」 相关 「属性」 词汇。这是第一个步骤，整个过程总体发生在 Transformer 的低层。 （2） 关系传播 GPT 模型在 「by」单词这个位置，也就是 NTP 要产生输出 token 的最后一个位置，通过 Attention 把单词「own」 的信息集成到最后位置。这里需要注意一下，最后一个单词对应的 Transformer 位置是比较关键的，因为在它的最上层会给出 Next Token 输出。在推理过程中，GPT 会把输入上文中的重要信息通过 Attention 逐步集成到这个位置上来。这个操作也发生在 Transformer 的低层。 （3） 关系抽取 在「by」 单词位置，也就是最后一个位置的 Transformer 高层，它在低层已经集成了单词「own」 的信息，这个信息在高层，通过 Attention 把「Beat music」 对应的属性「apple」 提取出来。具体提取动作是通过某个 Attention Head 来做到的，而且这篇文章证明了 Attention Head 里会编码 < 实体 - 属性 > 信息，具体例子可以参照下图，这点对应该是个新知识（过去一般认为 Attention 主要是用来进行信息比较和搬运的，它证明了 Attention 也会存储某种知识）\"]},\"64\":{\"h\":\"3 知识点在 Transformer 中的分布\",\"t\":[\"图3 单语义神经元与多语义神经元示意图\",\"（1）目前发现 LLM 中存在很多单个的神经元，它们各自只对输入里某个特殊的知识点产生响应，也就是说只会被特定输入模式激活，对其它无关输入保持沉默。\",\" 1）一个神经元编码一个知识，完美一一对应，这类 Transformer 中的神经元被称为 「单语义神经元」 2）很多不同语言含义的知识点都会激活某个神经元，这类神经元被称为「多语义神经元」。\",\"Superposition 概念的含义是：假设要编码的特征的数量 n 远远多于网络参数 d，可找到办法，来用 d 维神经元编码比 d 数量大得多的 n 个特征，这种编码机制被称为 superposition，所以它是被发现存在 Transformer 结构里的一种信息压缩编码机制。\",\"图4 重叠编码示意图\",\"Superposition 和「多语义神经元」 关系密切，目前发现 LLM 内部是这样做的（参考 Finding Neurons in a Haystack: Case Studies with Sparse Probing）：如上图所示，LLM 的 Superposition 机制是由多个「多语义神经元」 联合构成的，每个神经元会对输入中的多个不同知识点都有响应，所以仅仅通过一个多语义神经元是无法探测当前是对谁在做出响应，但是如果有多个对某个知识点都有响应的「多语义神经元」，在它们的响应之上做个线性组合，就能探测到输入中我们想识别的那个知识点（上图中蓝色部分)。也就是说，LLM 通过组合多个「多语义神经元」来对某个具体特征或知识点进行编码。所以，多语义神经元和知识点之间的关系是多对多的映射，一个知识点会激发很多对它进行编码的「多语义神经元」，而一个 「多语义神经元」也会对多个输入知识点产生响应。\",\"（2）另外，「Polysemanticity and Capacity in Neural Networks」这个文章指出了：在模型学习过程中，为了增加模型参数的利用效率，\\n  1）「单语义神经元」 会被分配给重要特征，\\n  2）「多语义神经元」会分配给不太重要的特征，\"]},\"65\":{\"c\":[\"语言模型\"]},\"66\":{\"c\":[\"LLM\"]},\"67\":{\"h\":\"PEARL: 长文档推理提示框架\",\"t\":[\"该文介绍了 PEARL 框架，旨在提升大型语言模型对长篇文档的理解能力，在 Zero-shot 情况下，性能比GPT-4高 10.5%！PEARL 被认为是利用语言模型进行复杂推理的重要步骤，为新的推理可能性打开了大门。\",\"提示\",\"GitHub CodeBase 代码目前还没放出来\"]},\"68\":{\"c\":[\"语言模型\"]},\"69\":{\"c\":[\"推理\",\"LLM\"]},\"70\":{\"h\":\"语言模型\"},\"71\":{\"c\":[\"语言模型\"]},\"72\":{\"c\":[\"LLM\"]},\"73\":{\"c\":[\"语言模型\"]},\"74\":{\"h\":\"Unlimiformer 介绍\",\"t\":[\"Unlimiformer 可以被注入到任何现有的编码器 - 解码器 transformer 中，能够处理长度不限的输入。\"]},\"75\":{\"h\":\"1 问题提出\",\"t\":[\"Transformer 是时下最强大的 seq2seq 架构。预训练 transformer 通常具有 512（例如 BERT）或 1024 个（例如 BART）token 的个上下文窗口，这对于目前许多文本摘要数据集（XSum、CNN/DM）来说是足够长的。\",\"但 16384 并不是生成所需上下文长度的上限：涉及长篇叙事的任务，如书籍摘要（Krys-´cinski et al.，2021）或叙事问答（Kociskýet al.，2018），通常输入超过 10 万个 token。维基百科文章生成的挑战集（Liu*et al.，2018）包含超过 50 万个 token 的输入。生成式问答中的开放域任务可以从更大的输入中综合信息，例如回答关于维基百科上所有健在作者的文章的聚合属性的问题。图 1 根据常见的上下文窗口长度绘制了几个流行的摘要和问答数据集的大小；最长的输入比 Longformer 的上下文窗口长 34 倍以上。\",\"图1 数据集token统计\",\"在这些超长输入的情况下，vanilla transformer 无法进行缩放，因为原生注意力机制具有平方级的复杂度。长输入 transformer 虽然比标准 transformer 更高效，但仍需要大量的计算资源，这些资源随着上下文窗口大小的增加而增加。此外，增加上下文窗口需要用新的上下文窗口大小从头开始重新训练模型，计算上和环境上的代价都不小。\",\"在「Unlimiformer: Long-Range Transformers with Unlimited Length Input」一文中，来自卡内基梅隆大学的研究者引入了 Unlimiformer。这是一种基于检索的方法，这种方法增强了预训练的语言模型，以在测试时接受无限长度的输入。\",\"论文链接：https://arxiv.org/pdf/2305.01625v1.pdf\",\"Unlimiformer 可以被注入到任何现有的编码器 - 解码器 transformer 中，能够处理长度不限的输入。给定一个长的输入序列，Unlimiformer 可以在所有输入 token 的隐藏状态上构建一个数据存储。然后，解码器的标准交叉注意力机制能够查询数据存储，并关注前 k 个输入 token。数据存储可以存储在 GPU 或 CPU 内存中，能够次线性查询。\",\"Unlimiformer 可以直接应用于经过训练的模型，并且可以在没有任何进一步训练的情况下改进现有的 checkpoint。Unlimiformer 经过微调后，性能会得到进一步提高。本文证明，Unlimiformer 可以应用于多个基础模型，如 BART（Lewis et al.，2020a）或 PRIMERA（Xiao et al.，2022），且无需添加权重和重新训练。在各种长程 seq2seq 数据集中，Unlimiformer 不仅在这些数据集上比 Longformer（Beltagy et al.，2020b）、SLED（Ivgi et al.，2022）和 Memorizing transformers（Wu et al.，2021）等强长程 Transformer 表现更好，而且本文还发现 Unlimiform 可以应用于 Longformer 编码器模型之上，以进行进一步改进。\"]},\"76\":{\"h\":\"2 Unlimiformer技术原理\",\"t\":[\"由于编码器上下文窗口的大小是固定的，Transformer 的最大输入长度受到限制。然而，在解码过程中，不同的信息可能是相关的；此外，不同的注意力头可能会关注不同类型的信息（Clark et al.，2019）。因此，固定的上下文窗口可能会在注意力不那么关注的 token 上浪费精力。\",\"在每个解码步骤中，Unlimiformer 中每个注意力头都会从全部输入中选择一个单独的上下文窗口。通过将 Unlimiformer 查找注入解码器来实现：在进入交叉注意力模块之前，该模型在外部数据存储中执行 k 最近邻 (kNN) 搜索，在每个解码器层中的每个注意力头中选一组 token 来参与。\"]},\"77\":{\"h\":\"2.1 Unlimiformer编码\",\"t\":[\"为了将比模型的上下文窗口长度更长的输入序列进行编码，本文按照 Ivgi et al. (2022) 的方法对输入的重叠块进行编码 (Ivgi et al. ,2022)，只保留每个 chunk 的输出的中间一半，以确保编码过程前后都有足够的上下文。最后，本文使用 Faiss (Johnson et al., 2019) 等库对数据存储中的编码输入进行索引（Johnson et al.，2019）。\"]},\"78\":{\"h\":\"2.2 检索增强的交叉注意力机制\",\"t\":[\"在标准的交叉注意力机制中，transformer 的解码器关注编码器的最终隐状态，编码器通常截断输入，并仅对输入序列中的前 k 个 token 进行编码。\",\"本文不是只关注输入的这前 k 个 token，对于每个交叉注意头，都检索更长的输入系列的前 k 个隐状态，并只关注这前 k 个。这样就能从整个输入序列中检索关键字，而不是截断关键字。在计算和 GPU 内存方面，本文的方法也比处理所有输入 token 更便宜，同时通常还能保留 99% 以上的注意力性能。\",\"图 2 显示了本文对 seq2seq transformer 架构的更改。使用编码器对完整输入进行块编码，并将其存储在数据存储中；然后，解码时查询编码的隐状态数据存储。kNN 搜索是非参数的，并且可以被注入到任何预训练的 seq2seq transformer 中，详情如下。\",\"图2 Unlimiformer原理图\"]},\"79\":{\"h\":\"3 实验结果\"},\"80\":{\"h\":\"3.1 长文档摘要\",\"t\":[\"图3显示了长文本（4k 及 16k 的 token 输入）摘要数据集中的结果。\",\"图3 长文本（4k 及 16k 的 token 输入）摘要数据集中的结果\",\"在图 4 的训练方法中，Unlimiformer 能够在各项指标上达到最优。\",\"图4 使用长范围训练方法的试验结果\"]},\"81\":{\"h\":\"3.2 书籍摘要\",\"t\":[\"图 5 显示了在书籍摘要上的结果。可以看到，基于 BARTbase 和 PRIMERA，应用Unlimiformer 都能取得一定的改进效果。\",\"图5 书籍摘要的试验结果\",\"原文链接\"]},\"82\":{\"c\":[\"语言模型\"]},\"83\":{\"c\":[\"摘要\",\"Transformer\",\"机器学习\"]},\"84\":{\"h\":\"ChatGPT相关技术介绍\",\"t\":[\"首先回顾了GPT系列模型的发展历程，然后介绍了ChatGPT模型最重要的技术指令微调，最后介绍了上下文学习。\"]},\"85\":{\"h\":\"1 GPT系列模型发展历程\",\"t\":[\"2020年7月，OpenAI发布了模型索引为的davinci的初代GPT-3论文，从此它就开始不断进化。总体分为两大类，第一类是在代码上训练，称其为Codex系列；第二类是使用指令微调的InstructGPT系列。\",\"2022年5-6月发布的text-davinci-002是一个基于code-davinci-002的有监督指令微调（Supervised Instruction Tuning）模型。然后是text-davinci-003和 ChatGPT，它们都在2022年11月发布，是使用的基于人类反馈的强化学习的版本指令微调（Instruction Tuning with Reinforcement Learning from Human Feedback）模型的两种不同变体。\",\"图1 GPT系列模型树\"]},\"86\":{\"h\":\"2 指令微调\",\"t\":[\"指令微调（Instruction Tuning）的提出来自于Google的一篇论文[1]，结合了微调和提示两个范式的优点，即用prompt格式的训练数据进行finetune，以使模型具备人类倾向的回答问题能力。\",\"在 2022 年 3 月，OpenAI 发布了指令微调[2]的论文，其监督微调（Supervised Instruction Tuning，SFT）的部分对应了davinci-instruct-beta和text-davinci-001。\",\"We focus on fine-tuning approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF) to fine-tune GPT-3 to follow a broad class of written instructions.\"]},\"87\":{\"h\":\"3 模型的训练方法和数据集\",\"t\":[\"图2 模型训练步骤\",\"（1）SFT阶段，使用人工标注prompt数据集的答案用来finetune模型。这一步得到的模型是davinci-instruct-beta。\",\"（2）奖励模型阶段，通过对模型输出答案打分来训练奖励模型（Reward Model，RM）。RM就是基于第一步生成的SFT6B版本，去除最后一次反嵌入层，起到了扩充LLM模型高质量训练数据的作用。 推理打分：选择了一部分prompt，由SFT模型随机生成多个答案（4-9个），人工对这些答案从到坏进行排序。这构成了一个新的监督训练数据集，排序是这些数据的label。新的数据集被用来训练RM。--ChatGPT是如何工作的\",\"（3）PPO阶段，使用RM来更新ppo策略，从而使GPT产生的答案更偏向于标注人员的喜好。\",\"表1 InstructGPT的训练数据构成\",\"据推测，ChatGPT使用了和text-davinci-003相同的训练方法，采用了不同的数据集，而且更加注重生成答案的无害性和对话性。\",\"合理分析：OpenAI官网的ChatGPT的训练流程和InstructGPT基本一致，除了ChatGPT是基于GPT3.5系列的，再根据InstructGPT发布后半年多才发布ChatGPT，推测是因为初始PPO策略训练的模型太过随心所欲，不能满足无害性等要求，而在调试的过程中GPT3.5系列已经训练完成，所以直接基于GPT3.5系列进行训练。\"]},\"88\":{\"h\":\"4 上下文学习\",\"t\":[\"上下文学习（In-context Learning，ICL）[3]是从类比中学习，和人类的决策相似。\",\"ICL只存在一次前向传播中，还是会被模型记住？论文中ICL的测试数据，类似于下图所示，每次预测都需要结合之前的几个demonstration，由此推测ICL并不会被模型记住。结合对text-davinci-003的测试，在一次调用中教会它数学题，之后单独询问，模型并不能正确回答，由此可以证明ICL只存在于一次前向传播。\",\"图3 ICL和微调的区别\",\"ICL是一个元优化的过程，可以看做隐性微调。GPT首先根据演示示例生成元梯度，然后将这些元梯度应用于原始GPT以构建ICL模型。\",\"Considering that ICL directly takes effect on only the attention keys and values.\",\"ICL只对attention有影响。\"]},\"89\":{\"h\":\"5 参考\",\"t\":[\"[1] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, et al. Finetuned language models are zero-shot learners. In: Proceedings of the 10th International Conference on Learning Representations (ICLR 2022), Online, April 25-29, 2022, OpenReview.net, 2022: 1-46\",\"[2] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, et al. Training language models to follow instructions with human feedback. In: Advances in Neural Information Processing Systems 35 (NeurIPS 2022), New Orleans, Louisiana, USA, November 28-December 9, 2022, MIT Press, 2022: 27730-27744\",\"[3] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, et al. Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. arXiv, 2023\"]},\"90\":{\"c\":[\"语言模型\"]},\"91\":{\"c\":[\"OpenAI\",\"Google\",\"Instruction Tuning\",\"In-context Learning\",\"ChatGPT\"]},\"92\":{\"c\":[\"ChatGPT相关技术介绍\"]},\"93\":{\"h\":\"PPO：从策略梯度算法到近端策略优化算法\",\"t\":[\"PPO（Proximal Policy Optimization）是一种策略梯度优化算法，它对标准的策略梯度方法做了改进，使得训练更加稳定。PPO的主要思想是：在每个更新步骤中，我们要确保当前的策略参数不会偏离旧策略参数太远。\"]},\"94\":{\"h\":\"1 策略梯度算法\",\"t\":[\"策略梯度算法带来了原始算法和总体框架，它告诉我们只要以奖励的期望式1.1为优化目标，通过采样足够多的样本来用均值估算数学期望，再用这个估算值对分布做梯度上升求式1.1的极大值，就可以优化我们所要优化的分布θ。\",\"\\nR_\\\\theta=E_{\\\\tau\\\\sim{p_\\\\theta(\\\\tau)}}R(\\\\tau)=\\\\sum\\\\limits_{\\\\tau}[R(\\\\tau)p_\\\\theta(\\\\tau)]\\n$$（1.1）\\n\\n$\\n\\\\begin{aligned}\\n\\\\nabla R_\\\\theta\\n&=\\\\sum\\\\limits_{\\\\tau}[R(\\\\tau)\\\\nabla p_\\\\theta(\\\\tau)] \\\\\\\\\\n&=\\\\sum\\\\limits_{\\\\tau}[R(\\\\tau)p_\\\\theta(\\\\tau)\\\\nabla \\\\log p_\\\\theta(\\\\tau)] \\\\\\\\\\n&=E_{\\\\tau \\\\sim p_\\\\theta(\\\\tau)}[R(\\\\tau)\\\\nabla \\\\log p_\\\\theta(\\\\tau)] \\\\\\\\\\n&\\\\approx \\\\frac{1}{N}\\\\sum\\\\limits_{i=1}^{N}[R(\\\\tau)\\\\nabla \\\\log p_\\\\theta(\\\\tau)]\\n\\\\end{aligned}\\n$（1.2）\\n\\n$\\n\\\\theta\\\\gets\\\\theta+\\\\eta\\\\nabla{R_\\\\theta}\\n$（1.3）\\n\\n但是策略梯度算法存在问题，每轮训练结束之后参数$\\\\theta$都要更新，导致下一轮计算均值前仍要重新采样大量数据，训练的时间开销集中在了数据采样。\\n\\n## 2 重要性采样\\n为了解决采样时间开销大的问题，引入了重要性采样，将式1.2换算成式2.1。这样我们可以对$\\\\theta^\\\\prime$采样一次之后，多次更新$\\\\theta$，大大节省了训练中采样数据的时间开销。\\n\\n$\\n\\\\begin{aligned}\\n\\\\nabla R_\\\\theta\\n&=E_{\\\\tau \\\\sim p_{\\\\theta^\\\\prime }(\\\\tau)}[\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)} R(\\\\tau)\\\\nabla \\\\log p_\\\\theta(\\\\tau)] \\\\\\\\\\n&\\\\approx \\\\frac{1}{N}\\\\sum\\\\limits_{i=1}^{N}[\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)}R(\\\\tau)\\\\nabla \\\\log p_\\\\theta(\\\\tau)]\\n\\\\end{aligned}\\n$（2.1）\\n\\n还原2.1式，得到我们的新的优化目标，如式2.2所示。\\n\\n$\\nR_\\\\theta\\n=E_{\\\\tau \\\\sim p_{\\\\theta^\\\\prime }(\\\\tau)}[\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)} R(\\\\tau)]\\n$（2.2）\\n\\n## 3 优势函数\\n式2.2的$R(\\\\tau)$是累积奖励，我们要优化的$R_\\\\theta$函数的实际意义是奖励关于完整路径$\\\\tau$的数学期望，我们希望这个值正负参半，因为这样就可以衡量策略是好还是坏，而不是比较谁更好。定义$A(\\\\tau)$等于$R(\\\\tau)$减去一个与路径无关的基线函数，比如状态价值函数，是不影响等式的。最终我们的优化目标确定了，如式3.1所示。\\n\\n$\\nR_\\\\theta\\n=E_{\\\\tau \\\\sim p_{\\\\theta^\\\\prime }(\\\\tau)}[\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)} A(\\\\tau)]\\n$（3.1）\\n\\n总之，如果$A(\\\\tau)$是正的，那就用梯度调整策略$\\\\theta$增大$\\\\tau$出现的概率；反之，如果$A(\\\\tau)$是负的，那就用梯度调整策略$\\\\theta$减小$\\\\tau$出现的概率。\\n\\n## 4 KL散度的外在约束\\n在加入重要性采样之后，我们可以对$\\\\theta^\\\\prime$采样来计算$\\\\theta$的更新梯度了。在理想情况，即采样的次数足够多的情况下式1.2和式2.1是严格相等的，然而$\\\\theta$和$\\\\theta^\\\\prime$的分布有差异会带来估算结果差异很大的问题，因此必须有一个约束。TRPO算法引入了KL散度，并将其作为一个外在约束。KL散度可以计算两个分布的不相似度，两个完全相同时，它们的KL散度值为0，不相似度越高，KL散度也越高。TRPO算法的公式如式4.1所示。\\n\\n$\\n\\\\begin{cases}\\nR_\\\\theta\\n=E_{\\\\tau \\\\sim p_{\\\\theta^\\\\prime }(\\\\tau)}[\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)} A(\\\\tau)] \\\\\\\\\\nKL(\\\\theta, \\\\theta^\\\\prime)< \\\\delta\\n\\\\end{cases}\\n$（4.1）\\n\\n但是TRPO算法也存在问题，因为它把 KL 散度约束当作一个额外的约束，没有放在目标里面，所以它处理起来非常困难。\\n\\n## 5 KL惩罚\\n我们现在既需要一个KL散度来约束$\\\\theta$和$\\\\theta^\\\\prime$分布的差异程度，又不能像TRPO算法那样将KL散度作为外在约束难以融入到梯度更新的操作中。因此考虑将KL散度加入到优化目标式3.1中，得到的新的优化目标如式5.1所示。\\n\\n$\\nR_\\\\theta\\n=E_{\\\\tau \\\\sim p_{\\\\theta^\\\\prime }(\\\\tau)}[\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)} A(\\\\tau)]-\\\\beta KL(\\\\theta,\\\\theta^\\\\prime)\\n$（5.1）\\n\\n我们的新优化目标和之前一样，也是越“大”，策略$\\\\theta$就越“好”。这个式子前半部分的数学期望，是之前3.1式给出的，用来计量策略$\\\\theta^\\\\prime$采样的好坏程度，对我们来说，这个值越大越好；而后半部分，是一个超参数$\\\\beta$乘以$\\\\theta$和$\\\\theta^\\\\prime$的KL散度，用来计量$\\\\theta$和$\\\\theta^\\\\prime$的不相似程度，对我们来说，这个值越小越好。用梯度上升来优化这个新的优化目标，就是PPO算法。\\n\\n在这个基础上，还能对算法进一步改进，引入自适应KL惩罚（adaptive KL penalty），给出一个KL的可接受区间$[KL_{min},KL_{max}]$，当KL散度小于最小值时，说明$\\\\theta$和$\\\\theta^\\\\prime$更新的幅度太小，即后面这一项效果太强了，应当减小$\\\\beta$值；当KL散度大于最大值时，说明$\\\\theta$和$\\\\theta^\\\\prime$的差距过大，即后面这一项效果太弱了，需要增大$\\\\beta$值。\\n\\n总之，KL惩罚的优势在于，新的优化目标既将原始的优化目标包含在内，又包含了一个描述$\\\\theta$和$\\\\theta^\\\\prime$分布的不相似度的值，减小了对$\\\\theta^\\\\prime$采样来估算$\\\\theta$的优化梯度的误差。\\n\\n## 6 PPO裁剪（clip）\\n\\n近端策略优化裁剪是解决$\\\\theta$和$\\\\theta^\\\\prime$分布差异过大的另一种方法，它不使用KL散度来描述两种分布的不相似度，而是使用裁剪函数clip。近端策略优化裁剪的优化目标如式6.1所示。\\n\\n$\\nR_\\\\theta\\n\\\\approx \\n\\\\frac{1}{N}\\\\sum\\\\limits_{\\\\tau}\\\\min(\\n\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)} A(\\\\tau),\\n\\\\rm{clip}(\\\\frac{p_\\\\theta(\\\\tau)}{p_{\\\\theta^\\\\prime}(\\\\tau)},1-\\\\epsilon,1+\\\\epsilon)A(\\\\tau))\\n$(6.1)\\n\\nPPO裁剪实现的功能和KL惩罚一样，通过限定$\\\\frac{p_\\\\theta}{p_{\\\\theta^\\\\prime}}$的范围来约束$\\\\theta$和$\\\\theta^\\\\prime$分布的差异程度。一般基于KL惩罚的PPO算法称为PPO1算法，基于clip的PPO算法称为PPO2算法。\\n\"]},\"95\":{\"c\":[\"语言模型\"]},\"96\":{\"c\":[\"模型\",\"强化学习\"]},\"97\":{\"h\":\"提示技术\"},\"98\":{\"c\":[\"提示技术\"]},\"99\":{\"c\":[\"Prompt\"]},\"100\":{\"c\":[\"提示技术\"]},\"101\":{\"h\":\"Token\"},\"102\":{\"c\":[\"token\"]},\"103\":{\"c\":[\"token\"]},\"104\":{\"c\":[\"Token\"]}},\"dirtCount\":0,\"index\":[[\"强化学习\",{\"2\":{\"96\":1}}],[\"近端策略优化裁剪的优化目标如式6\",{\"1\":{\"94\":1}}],[\"近端策略优化裁剪是解决$\",{\"1\":{\"94\":1}}],[\"减小了对$\",{\"1\":{\"94\":1}}],[\"减少到\",{\"1\":{\"46\":1}}],[\"新的优化目标既将原始的优化目标包含在内\",{\"1\":{\"94\":1}}],[\"新的数据集被用来训练rm\",{\"1\":{\"87\":1}}],[\"需要增大$\",{\"1\":{\"94\":1}}],[\"需要优化的参数只有θ\",{\"1\":{\"21\":1}}],[\"当kl散度大于最大值时\",{\"1\":{\"94\":1}}],[\"当kl散度小于最小值时\",{\"1\":{\"94\":1}}],[\"说明$\",{\"1\":{\"94\":2}}],[\"说明这个模型学到了更多的内在规律\",{\"1\":{\"62\":1}}],[\"给出一个kl的可接受区间$\",{\"1\":{\"94\":1}}],[\"给定一个长的输入序列\",{\"1\":{\"75\":1}}],[\"给定一个无监督的token语料库u=\",{\"1\":{\"54\":1}}],[\"用梯度上升来优化这个新的优化目标\",{\"1\":{\"94\":1}}],[\"用来计量$\",{\"1\":{\"94\":1}}],[\"用来计量策略$\",{\"1\":{\"94\":1}}],[\"用来模拟embedding矩阵的效果\",{\"1\":{\"21\":1}}],[\"好\",{\"1\":{\"94\":1}}],[\"策略$\",{\"1\":{\"94\":1}}],[\"策略梯度算法带来了原始算法和总体框架\",{\"1\":{\"94\":1}}],[\"策略梯度算法\",{\"0\":{\"94\":1}}],[\"没有放在目标里面\",{\"1\":{\"94\":1}}],[\"散度约束当作一个额外的约束\",{\"1\":{\"94\":1}}],[\"两个完全相同时\",{\"1\":{\"94\":1}}],[\"两个模型针对质数概念理解的测试对比\",{\"1\":{\"62\":1}}],[\"反之\",{\"1\":{\"94\":1}}],[\"那就用梯度调整策略$\",{\"1\":{\"94\":2}}],[\"总之\",{\"1\":{\"94\":2}}],[\"总体分为两大类\",{\"1\":{\"85\":1}}],[\"总体上显存的压力是大大变小了\",{\"1\":{\"33\":1}}],[\"定义$a\",{\"1\":{\"94\":1}}],[\"式2\",{\"1\":{\"94\":1}}],[\"优势函数\",{\"1\":{\"94\":1}}],[\"优化目标变成了以下式子\",{\"1\":{\"55\":1}}],[\"优化算法\",{\"0\":{\"43\":1}}],[\"优化\",{\"2\":{\"36\":1,\"49\":1}}],[\"得到的新的优化目标如式5\",{\"1\":{\"94\":1}}],[\"得到我们的新的优化目标\",{\"1\":{\"94\":1}}],[\"得到输出的hidden\",{\"1\":{\"21\":1}}],[\"还能对算法进一步改进\",{\"1\":{\"94\":1}}],[\"还原2\",{\"1\":{\"94\":1}}],[\"还是会被模型记住\",{\"1\":{\"88\":1}}],[\"导致下一轮计算均值前仍要重新采样大量数据\",{\"1\":{\"94\":1}}],[\"^\",{\"1\":{\"94\":2}}],[\"$的范围来约束$\",{\"1\":{\"94\":1}}],[\"$是负的\",{\"1\":{\"94\":1}}],[\"$是正的\",{\"1\":{\"94\":1}}],[\"$是累积奖励\",{\"1\":{\"94\":1}}],[\"$减去一个与路径无关的基线函数\",{\"1\":{\"94\":1}}],[\"$等于$r\",{\"1\":{\"94\":1}}],[\"$\",{\"1\":{\"94\":17}}],[\"$$\",{\"1\":{\"94\":1}}],[\"jiang\",{\"1\":{\"89\":1}}],[\"jeff\",{\"1\":{\"89\":1}}],[\"jason\",{\"1\":{\"89\":1}}],[\"johnson\",{\"1\":{\"77\":2}}],[\"类似于下图所示\",{\"1\":{\"88\":1}}],[\"类型数据的\",{\"1\":{\"7\":1}}],[\"推测是因为初始ppo策略训练的模型太过随心所欲\",{\"1\":{\"87\":1}}],[\"推理打分\",{\"1\":{\"87\":1}}],[\"推理\",{\"2\":{\"69\":1}}],[\"推理阶段应该比原来的计算量增大一点\",{\"1\":{\"21\":1}}],[\"合理分析\",{\"1\":{\"87\":1}}],[\"采用了不同的数据集\",{\"1\":{\"87\":1}}],[\"据推测\",{\"1\":{\"87\":1}}],[\"排序是这些数据的label\",{\"1\":{\"87\":1}}],[\"人工对这些答案从到坏进行排序\",{\"1\":{\"87\":1}}],[\"起到了扩充llm模型高质量训练数据的作用\",{\"1\":{\"87\":1}}],[\"去除最后一次反嵌入层\",{\"1\":{\"87\":1}}],[\"奖励模型阶段\",{\"1\":{\"87\":1}}],[\"发布了指令微调\",{\"1\":{\"86\":1}}],[\"发现\",{\"1\":{\"63\":1}}],[\"月\",{\"1\":{\"86\":1}}],[\"年\",{\"1\":{\"86\":1}}],[\"年末开发了promptsource项目\",{\"1\":{\"7\":1}}],[\"指令微调\",{\"0\":{\"86\":1},\"1\":{\"86\":1}}],[\"指令数据集和提示数据集在模型微调方面\",{\"1\":{\"6\":1}}],[\"指令数据集和提示数据集分享\",{\"0\":{\"6\":1}}],[\"称其为codex系列\",{\"1\":{\"85\":1}}],[\"称之为前缀\",{\"1\":{\"24\":1}}],[\"首先回顾了gpt系列模型的发展历程\",{\"1\":{\"84\":1}}],[\"摘要\",{\"2\":{\"83\":1}}],[\"摘要数据集中的结果\",{\"1\":{\"80\":2}}],[\"应当减小$\",{\"1\":{\"94\":1}}],[\"应用unlimiformer\",{\"1\":{\"81\":1}}],[\"应该可以成功解决任何看不见的任务\",{\"1\":{\"7\":1}}],[\"书籍摘要的试验结果\",{\"1\":{\"81\":1}}],[\"书籍摘要\",{\"0\":{\"81\":1}}],[\"及\",{\"1\":{\"80\":2}}],[\"详情如下\",{\"1\":{\"78\":1}}],[\"显示了在书籍摘要上的结果\",{\"1\":{\"81\":1}}],[\"显示了本文对\",{\"1\":{\"78\":1}}],[\"显式\",{\"1\":{\"24\":1}}],[\"编码器通常截断输入\",{\"1\":{\"78\":1}}],[\"编码器模型之上\",{\"1\":{\"75\":1}}],[\"检索增强的交叉注意力机制\",{\"0\":{\"78\":1}}],[\"搜索是非参数的\",{\"1\":{\"78\":1}}],[\"搜索\",{\"1\":{\"76\":1}}],[\"查找注入解码器来实现\",{\"1\":{\"76\":1}}],[\"且无需添加权重和重新训练\",{\"1\":{\"75\":1}}],[\"性能会得到进一步提高\",{\"1\":{\"75\":1}}],[\"性能比gpt\",{\"1\":{\"67\":1}}],[\"增加上下文窗口需要用新的上下文窗口大小从头开始重新训练模型\",{\"1\":{\"75\":1}}],[\"增大模型训练的改变量\",{\"1\":{\"26\":1}}],[\"增大改变量和交互性\",{\"1\":{\"26\":1}}],[\"虽然比标准\",{\"1\":{\"75\":1}}],[\"倍以上\",{\"1\":{\"75\":1}}],[\"维基百科文章生成的挑战集\",{\"1\":{\"75\":1}}],[\"维神经元编码比\",{\"1\":{\"64\":1}}],[\"万个\",{\"1\":{\"75\":2}}],[\"´cinski\",{\"1\":{\"75\":1}}],[\"涉及长篇叙事的任务\",{\"1\":{\"75\":1}}],[\"或叙事问答\",{\"1\":{\"75\":1}}],[\"或\",{\"1\":{\"75\":3}}],[\"例如回答关于维基百科上所有健在作者的文章的聚合属性的问题\",{\"1\":{\"75\":1}}],[\"例如\",{\"1\":{\"75\":2}}],[\"例如britain\",{\"1\":{\"26\":1}}],[\"架构的更改\",{\"1\":{\"78\":1}}],[\"架构\",{\"1\":{\"75\":1}}],[\"能够在各项指标上达到最优\",{\"1\":{\"80\":1}}],[\"能够次线性查询\",{\"1\":{\"75\":1}}],[\"能够处理长度不限的输入\",{\"1\":{\"74\":1,\"75\":1}}],[\"能够触发越来越多的与\",{\"1\":{\"63\":1}}],[\"解码时查询编码的隐状态数据存储\",{\"1\":{\"78\":1}}],[\"解码器的标准交叉注意力机制能够查询数据存储\",{\"1\":{\"75\":1}}],[\"解码器\",{\"1\":{\"74\":1,\"75\":1}}],[\"解决问题\",{\"1\":{\"7\":1}}],[\"被认为是利用语言模型进行复杂推理的重要步骤\",{\"1\":{\"67\":1}}],[\"情况下\",{\"1\":{\"67\":1}}],[\"zero\",{\"1\":{\"67\":1,\"89\":1}}],[\"zhifang\",{\"1\":{\"89\":1}}],[\"zhihu\",{\"1\":{\"58\":1}}],[\"zhao\",{\"1\":{\"89\":1}}],[\"zhuanlan\",{\"1\":{\"58\":1}}],[\"旨在提升大型语言模型对长篇文档的理解能力\",{\"1\":{\"67\":1}}],[\"旨在训练helpful\",{\"1\":{\"7\":1}}],[\"框架\",{\"1\":{\"67\":1}}],[\"长文本\",{\"1\":{\"80\":1}}],[\"长文档摘要\",{\"0\":{\"80\":1}}],[\"长文档推理提示框架\",{\"0\":{\"67\":1}}],[\"长输入\",{\"1\":{\"75\":1}}],[\"长度\",{\"1\":{\"45\":1}}],[\"联合构成的\",{\"1\":{\"64\":1}}],[\"机制是由多个\",{\"1\":{\"64\":1}}],[\"机器学习\",{\"2\":{\"36\":1,\"83\":1}}],[\"结合对text\",{\"1\":{\"88\":1}}],[\"结合了微调和提示两个范式的优点\",{\"1\":{\"86\":1}}],[\"结合上图\",{\"1\":{\"21\":1}}],[\"结构里的一种信息压缩编码机制\",{\"1\":{\"64\":1}}],[\"远远多于网络参数\",{\"1\":{\"64\":1}}],[\"概念的含义是\",{\"1\":{\"64\":1}}],[\"很多不同语言含义的知识点都会激活某个神经元\",{\"1\":{\"64\":1}}],[\"完美一一对应\",{\"1\":{\"64\":1}}],[\"单语义神经元\",{\"1\":{\"64\":2}}],[\"单语义神经元与多语义神经元示意图\",{\"1\":{\"64\":1}}],[\"单词位置\",{\"1\":{\"63\":1}}],[\"单词这个位置\",{\"1\":{\"63\":1}}],[\"单词\",{\"1\":{\"63\":1}}],[\"知识点在\",{\"0\":{\"64\":1}}],[\"知乎原文\",{\"1\":{\"58\":1}}],[\"过去一般认为\",{\"1\":{\"63\":1}}],[\"信息\",{\"1\":{\"63\":1}}],[\">\",{\"1\":{\"63\":1}}],[\"里会编码\",{\"1\":{\"63\":1}}],[\"里增加信息\",{\"1\":{\"63\":1}}],[\"来参与\",{\"1\":{\"76\":1}}],[\"来自卡内基梅隆大学的研究者引入了\",{\"1\":{\"75\":1}}],[\"来说是足够长的\",{\"1\":{\"75\":1}}],[\"来对某个具体特征或知识点进行编码\",{\"1\":{\"64\":1}}],[\"来用\",{\"1\":{\"64\":1}}],[\"来做到的\",{\"1\":{\"63\":1}}],[\"来产生下一个单词\",{\"1\":{\"60\":1}}],[\"高层\",{\"1\":{\"63\":1}}],[\"逐步集成到这个位置上来\",{\"1\":{\"63\":1}}],[\"输出\",{\"1\":{\"63\":1}}],[\"输入\",{\"1\":{\"80\":2}}],[\"输入通过作者的预训练模型\",{\"1\":{\"55\":1}}],[\"输入序列通常是变长的\",{\"1\":{\"42\":1}}],[\"位置是比较关键的\",{\"1\":{\"63\":1}}],[\"关系密切\",{\"1\":{\"64\":1}}],[\"关系抽取\",{\"1\":{\"63\":1}}],[\"关系传播\",{\"1\":{\"63\":1}}],[\"关键的增量矩阵被分配了高秩\",{\"1\":{\"22\":1}}],[\"整个过程总体发生在\",{\"1\":{\"63\":1}}],[\"整行放在共享内存进行\",{\"1\":{\"45\":1}}],[\"词汇\",{\"1\":{\"63\":1}}],[\"属性\",{\"1\":{\"63\":2}}],[\"层\",{\"1\":{\"63\":1}}],[\"层数越来越高\",{\"1\":{\"63\":1}}],[\"随着\",{\"1\":{\"63\":1}}],[\"先通过\",{\"1\":{\"63\":1}}],[\"往上走的过程中\",{\"1\":{\"63\":1}}],[\"主要是用来进行信息比较和搬运的\",{\"1\":{\"63\":1}}],[\"主题补充\",{\"1\":{\"63\":1}}],[\"主页\",{\"0\":{\"0\":1},\"2\":{\"1\":1}}],[\"经过微调后\",{\"1\":{\"75\":1}}],[\"经过研究\",{\"1\":{\"63\":1}}],[\"经历了明显的三阶段过程\",{\"1\":{\"63\":1}}],[\"经改写后得到240k条instruction数据\",{\"1\":{\"7\":1}}],[\"剖析自回归语言模型中事实关联的回忆\",{\"1\":{\"63\":1}}],[\"=e\",{\"1\":{\"94\":6}}],[\"=\",{\"1\":{\"62\":1,\"94\":3}}],[\"=tw−p\",{\"1\":{\"34\":1}}],[\"无法进行缩放\",{\"1\":{\"75\":1}}],[\"无损\",{\"1\":{\"62\":1}}],[\"无监督预训练\",{\"0\":{\"54\":1}}],[\"会分配给不太重要的特征\",{\"1\":{\"64\":1}}],[\"会被分配给重要特征\",{\"1\":{\"64\":1}}],[\"会把输入上文中的重要信息通过\",{\"1\":{\"63\":1}}],[\"会发现尽管\",{\"1\":{\"62\":1}}],[\"会差于微调\",{\"1\":{\"26\":1}}],[\"第二类是使用指令微调的instructgpt系列\",{\"1\":{\"85\":1}}],[\"第二个问题\",{\"1\":{\"62\":1}}],[\"第一类是在代码上训练\",{\"1\":{\"85\":1}}],[\"第一个问题\",{\"1\":{\"62\":1}}],[\"第一层加入soft\",{\"1\":{\"27\":1}}],[\"否则这道题很难回答好\",{\"1\":{\"62\":1}}],[\"下面是gpt\",{\"1\":{\"62\":1}}],[\"假设要编码的特征的数量\",{\"1\":{\"64\":1}}],[\"假设要传输的序列是连续质数数字序列\",{\"1\":{\"62\":1}}],[\"假设有一个标记的数据集c\",{\"1\":{\"55\":1}}],[\"举个例子\",{\"1\":{\"62\":1}}],[\"就是ppo算法\",{\"1\":{\"94\":1}}],[\"就是通过语言中前面的单词\",{\"1\":{\"60\":1}}],[\"就可以优化我们所要优化的分布θ\",{\"1\":{\"94\":1}}],[\"就能探测到输入中我们想识别的那个知识点\",{\"1\":{\"64\":1}}],[\"就会出现图右小模型这种不知所云的回答\",{\"1\":{\"62\":1}}],[\"就代表了它具备更高的智能呢\",{\"1\":{\"62\":1}}],[\"照此思路推进大模型研发方向的一个核心理念\",{\"1\":{\"62\":1}}],[\"则模型智能程度越高\",{\"1\":{\"62\":1}}],[\"则其压缩效率就越高\",{\"1\":{\"62\":1}}],[\"预训练\",{\"1\":{\"75\":1}}],[\"预测得越准确\",{\"1\":{\"62\":1}}],[\"预备知识\",{\"0\":{\"59\":1}}],[\"压缩即智能\",{\"0\":{\"62\":1}}],[\"智能呢\",{\"1\":{\"61\":1}}],[\"利用\",{\"0\":{\"61\":1}}],[\"利用这个性质可以实现\",{\"1\":{\"46\":1}}],[\"后文为了书写简洁\",{\"1\":{\"60\":1}}],[\"都能取得一定的改进效果\",{\"1\":{\"81\":1}}],[\"都检索更长的输入系列的前\",{\"1\":{\"78\":1}}],[\"都采用\",{\"1\":{\"60\":1}}],[\"都可以拆解为\",{\"1\":{\"46\":1}}],[\"文字是由内在智能产生的\",{\"1\":{\"58\":1}}],[\"另外\",{\"1\":{\"64\":1}}],[\"另外一种观点则认为\",{\"1\":{\"58\":1}}],[\"另一方面说明大模型在这方面表现确实比小模型要好\",{\"1\":{\"62\":1}}],[\"另一个优点是\",{\"1\":{\"22\":1}}],[\"请联系删除\",{\"1\":{\"58\":1}}],[\"版权归属原作者\",{\"1\":{\"58\":1}}],[\"一般基于kl惩罚的ppo算法称为ppo1算法\",{\"1\":{\"94\":1}}],[\"一文中\",{\"1\":{\"75\":1}}],[\"一个知识点会激发很多对它进行编码的\",{\"1\":{\"64\":1}}],[\"一个神经元编码一个知识\",{\"1\":{\"64\":1}}],[\"一起完成的\",{\"1\":{\"62\":1}}],[\"一种观点认为gpt\",{\"1\":{\"58\":1}}],[\"一\",{\"0\":{\"58\":1}}],[\"深度学习\",{\"2\":{\"57\":1}}],[\"带参数λ\",{\"1\":{\"55\":1}}],[\"监督微调\",{\"0\":{\"55\":1}}],[\"⋯\",{\"1\":{\"54\":2}}],[\"usa\",{\"1\":{\"89\":1}}],[\"use\",{\"1\":{\"86\":1}}],[\"u−1\",{\"1\":{\"54\":1}}],[\"u−k\",{\"1\":{\"54\":1}}],[\"u1​\",{\"1\":{\"54\":1}}],[\"unlimiform\",{\"1\":{\"75\":1}}],[\"unlimiformer原理图\",{\"1\":{\"78\":1}}],[\"unlimiformer编码\",{\"0\":{\"77\":1}}],[\"unlimiformer技术原理\",{\"0\":{\"76\":1}}],[\"unlimiformer\",{\"0\":{\"74\":1},\"1\":{\"74\":1,\"75\":8,\"76\":2,\"80\":1}}],[\"unlimited\",{\"1\":{\"75\":1}}],[\"un​\",{\"1\":{\"54\":1}}],[\"understanding\",{\"0\":{\"51\":1}}],[\"understands\",{\"1\":{\"20\":1}}],[\"universally\",{\"1\":{\"20\":1}}],[\"unifiedskg\",{\"1\":{\"7\":4}}],[\"unnatural\",{\"1\":{\"7\":3}}],[\"他们在微调期间利用面向任务的输入转换来实现有效的转移\",{\"1\":{\"51\":1}}],[\"代码目前还没放出来\",{\"1\":{\"67\":1}}],[\"代码实现易于拓展\",{\"1\":{\"47\":1}}],[\"代码地址\",{\"1\":{\"18\":1,\"41\":1}}],[\"比如状态价值函数\",{\"1\":{\"94\":1}}],[\"比如\",{\"1\":{\"47\":1}}],[\"比模型集成的成本小多了\",{\"1\":{\"25\":1}}],[\"字节\",{\"2\":{\"49\":1}}],[\"字节内部版本还支持了许多\",{\"1\":{\"47\":1}}],[\"字节跳动\",{\"1\":{\"42\":1,\"47\":1}}],[\"字节跳动aml团队先前提出的\",{\"1\":{\"42\":1}}],[\"除了chatgpt是基于gpt3\",{\"1\":{\"87\":1}}],[\"除了多一个t的对角元素之外还多一个偏移向量\",{\"1\":{\"34\":1}}],[\"除此之外\",{\"1\":{\"47\":1}}],[\"目前发现\",{\"1\":{\"64\":2}}],[\"目前规模够大的\",{\"1\":{\"60\":1}}],[\"目前\",{\"1\":{\"47\":1}}],[\"目录\",{\"0\":{\"4\":1}}],[\"支持\",{\"0\":{\"47\":1}}],[\"变种\",{\"0\":{\"47\":1},\"1\":{\"47\":1}}],[\"变成了n\",{\"1\":{\"21\":1}}],[\"内部是这样做的\",{\"1\":{\"64\":1}}],[\"内线程通信交换数据\",{\"1\":{\"46\":1}}],[\"内线程读取连续的\",{\"1\":{\"46\":1}}],[\"内存方面\",{\"1\":{\"78\":1}}],[\"内存中\",{\"1\":{\"75\":1}}],[\"内存\",{\"2\":{\"36\":1}}],[\"效率很低\",{\"1\":{\"46\":1}}],[\"效果追上了fine\",{\"1\":{\"26\":1}}],[\"依次读取所有的子问题\",{\"1\":{\"46\":1}}],[\"共享子问题参数\",{\"1\":{\"46\":1}}],[\"共21个任务数据集\",{\"1\":{\"7\":1}}],[\"读取子问题参数会有不小的开销\",{\"1\":{\"46\":1}}],[\"通常输入超过\",{\"1\":{\"75\":1}}],[\"通常具有\",{\"1\":{\"75\":1}}],[\"通常较大\",{\"1\":{\"46\":1}}],[\"通过限定$\",{\"1\":{\"94\":1}}],[\"通过采样足够多的样本来用均值估算数学期望\",{\"1\":{\"94\":1}}],[\"通过将\",{\"1\":{\"76\":1}}],[\"通过组合多个\",{\"1\":{\"64\":1}}],[\"通过\",{\"1\":{\"63\":2}}],[\"通过每个\",{\"1\":{\"63\":1}}],[\"通过以下方面体现\",{\"1\":{\"55\":1}}],[\"通过共享使参数存储量从\",{\"1\":{\"46\":1}}],[\"通过手写\",{\"1\":{\"45\":1}}],[\"通过对模型输出答案打分来训练奖励模型\",{\"1\":{\"87\":1}}],[\"通过对输入的重排列\",{\"1\":{\"42\":1}}],[\"通过对权重矩阵进行重要性评分\",{\"1\":{\"22\":1}}],[\"通过操纵奇异值\",{\"1\":{\"22\":1}}],[\"时\",{\"1\":{\"46\":1}}],[\"拆解为不同数量的待计算块\",{\"1\":{\"46\":1}}],[\"固定的上下文窗口可能会在注意力不那么关注的\",{\"1\":{\"76\":1}}],[\"固定分块大小\",{\"1\":{\"46\":1}}],[\"固定预训练模型\",{\"1\":{\"27\":1}}],[\"固定预训练参数\",{\"1\":{\"25\":1}}],[\"原文链接\",{\"1\":{\"81\":1}}],[\"原始实现中\",{\"1\":{\"46\":1}}],[\"原理\",{\"1\":{\"46\":2}}],[\"原论文仅在以下任务中进行了比较\",{\"1\":{\"24\":1}}],[\"传递真实的\",{\"1\":{\"46\":1}}],[\"传统离散prompt直接将模板t的每个token映射为对应的embedding\",{\"1\":{\"27\":1}}],[\"传统上定义为将输入字符串映射到输出字符串\",{\"1\":{\"7\":1}}],[\"开发的\",{\"1\":{\"46\":1}}],[\"开源\",{\"1\":{\"44\":1}}],[\"开源了其在自己产品线中使用的\",{\"1\":{\"7\":1}}],[\"开源了一系列工具\",{\"1\":{\"7\":1}}],[\"到\",{\"1\":{\"45\":1}}],[\"到多种非英语语言\",{\"1\":{\"7\":1}}],[\"分成两个\",{\"1\":{\"45\":1}}],[\"分别是\",{\"1\":{\"20\":1,\"22\":1}}],[\"保证高性能\",{\"1\":{\"45\":1}}],[\"接口使用\",{\"1\":{\"45\":1}}],[\"矩阵乘通过调用\",{\"1\":{\"45\":1}}],[\"操作\",{\"1\":{\"45\":1}}],[\"算术编码的编码补偿能力\",{\"1\":{\"62\":1}}],[\"算术编码\",{\"1\":{\"62\":1}}],[\"算子\",{\"1\":{\"45\":1}}],[\"算法步骤如下\",{\"1\":{\"44\":1}}],[\"算法\",{\"0\":{\"44\":1},\"1\":{\"44\":1}}],[\"每轮训练结束之后参数$\",{\"1\":{\"94\":1}}],[\"每次预测都需要结合之前的几个demonstration\",{\"1\":{\"88\":1}}],[\"每个神经元会对输入中的多个不同知识点都有响应\",{\"1\":{\"64\":1}}],[\"每个线程的读取次数降低到\",{\"1\":{\"46\":1}}],[\"每个线程都需要遍历读取所有的子问题大小\",{\"1\":{\"46\":1}}],[\"每个\",{\"1\":{\"46\":1}}],[\"每个矩阵乘子问题根据问题大小和分块大小\",{\"1\":{\"46\":1}}],[\"每个矩阵乘子问题\",{\"1\":{\"46\":1}}],[\"每个输入产生多组\",{\"1\":{\"45\":1}}],[\"每组各自计算互不影响\",{\"1\":{\"45\":1}}],[\"多次更新$\",{\"1\":{\"94\":1}}],[\"多语义神经元和知识点之间的关系是多对多的映射\",{\"1\":{\"64\":1}}],[\"多语义神经元\",{\"1\":{\"64\":8}}],[\"多头注意力\",{\"1\":{\"45\":1}}],[\"多了δ\",{\"1\":{\"21\":1}}],[\"旧版的多头注意力\",{\"1\":{\"45\":1}}],[\"融合的多头注意力\",{\"0\":{\"45\":1}}],[\"再用这个估算值对分布做梯度上升求式1\",{\"1\":{\"94\":1}}],[\"再用第二个linear层b\",{\"1\":{\"21\":1}}],[\"再根据instructgpt发布后半年多才发布chatgpt\",{\"1\":{\"87\":1}}],[\"再把这些块平均分配到每个\",{\"1\":{\"46\":1}}],[\"再参与后续的矩阵乘计算\",{\"1\":{\"44\":1}}],[\"重要性采样\",{\"1\":{\"94\":1}}],[\"重要性感知秩分配\",{\"1\":{\"22\":1}}],[\"重叠编码示意图\",{\"1\":{\"64\":1}}],[\"重排列为\",{\"1\":{\"44\":1}}],[\"把\",{\"1\":{\"63\":1}}],[\"把单词\",{\"1\":{\"63\":1}}],[\"把之前的修饰语\",{\"1\":{\"63\":1}}],[\"把llm看做函数\",{\"1\":{\"61\":1}}],[\"把问题大小传入到\",{\"1\":{\"46\":1}}],[\"把输入张量从\",{\"1\":{\"44\":1}}],[\"把预训练大模型freeze住\",{\"1\":{\"24\":1}}],[\"图5\",{\"1\":{\"81\":1}}],[\"图\",{\"1\":{\"75\":1,\"78\":1,\"81\":1}}],[\"图4\",{\"1\":{\"46\":1,\"64\":1,\"80\":1}}],[\"图3显示了长文本\",{\"1\":{\"80\":1}}],[\"图3\",{\"1\":{\"46\":1,\"64\":1,\"80\":1,\"88\":1}}],[\"图2\",{\"1\":{\"44\":1,\"63\":1,\"78\":1,\"87\":1}}],[\"图1\",{\"1\":{\"41\":1,\"52\":1,\"62\":1,\"75\":1,\"85\":1}}],[\"外矩阵乘的额外计算\",{\"1\":{\"44\":1}}],[\"团队已经在\",{\"1\":{\"47\":1}}],[\"团队之前的工作\",{\"1\":{\"44\":1}}],[\"团队提出了\",{\"1\":{\"42\":1}}],[\"计算上和环境上的代价都不小\",{\"1\":{\"75\":1}}],[\"计算下一个token的在词表中的概率分布\",{\"1\":{\"61\":1}}],[\"计算\",{\"1\":{\"42\":1,\"44\":1}}],[\"部分的性能\",{\"1\":{\"45\":1}}],[\"部分仍然需要\",{\"1\":{\"42\":1}}],[\"部分缓解了\",{\"1\":{\"7\":1}}],[\"但仍需要大量的计算资源\",{\"1\":{\"75\":1}}],[\"但\",{\"1\":{\"42\":1,\"75\":1}}],[\"但是trpo算法也存在问题\",{\"1\":{\"94\":1}}],[\"但是策略梯度算法存在问题\",{\"1\":{\"94\":1}}],[\"但是如果有多个对某个知识点都有响应的\",{\"1\":{\"64\":1}}],[\"但是能够达成\",{\"1\":{\"62\":1}}],[\"但是p\",{\"1\":{\"26\":1}}],[\"但是相应地\",{\"1\":{\"21\":1}}],[\"但是从前面的max的目标来看\",{\"1\":{\"21\":1}}],[\"但是不让w参与训练\",{\"1\":{\"21\":1}}],[\"但是与albert不同的是\",{\"1\":{\"21\":1}}],[\"才能利用批处理加速transformer计算\",{\"1\":{\"42\":1}}],[\"要产生输出\",{\"1\":{\"63\":1}}],[\"要求输入序列长度相同\",{\"1\":{\"42\":1}}],[\"要高于模型的指标\",{\"1\":{\"32\":1}}],[\"现有的一些深度学习框架\",{\"1\":{\"42\":1}}],[\"介绍\",{\"0\":{\"42\":1,\"74\":1}}],[\"介绍页\",{\"0\":{\"2\":1}}],[\"成功避免了传统实现中的冗余运算\",{\"1\":{\"41\":1}}],[\"针对自然语言处理常见的可变长输入\",{\"1\":{\"41\":1}}],[\"大\",{\"1\":{\"94\":1}}],[\"大大节省了训练中采样数据的时间开销\",{\"1\":{\"94\":1}}],[\"大大减少了下游任务的可训练参数数量\",{\"1\":{\"21\":1}}],[\"大大减少了需要训练的计算量\",{\"1\":{\"21\":1}}],[\"大幅优化推理速度\",{\"0\":{\"41\":1},\"2\":{\"50\":1}}],[\"非对称量化\",{\"0\":{\"34\":1}}],[\"非常优秀的表现\",{\"1\":{\"7\":1}}],[\"少存了w的一半大小\",{\"1\":{\"33\":1}}],[\"网络相当于舍弃了w\",{\"1\":{\"33\":1}}],[\"量化之后\",{\"1\":{\"33\":1}}],[\"量化操作仅针对w\",{\"1\":{\"33\":1}}],[\"量化的目是为了减少计算时间和计算能耗\",{\"1\":{\"32\":1}}],[\"前向传播的计算公式变成了\",{\"1\":{\"33\":1}}],[\"前缀完全由自由参数组成\",{\"1\":{\"24\":1}}],[\"前缀微调只优化了前缀\",{\"1\":{\"24\":1}}],[\"前缀微调\",{\"1\":{\"24\":1,\"26\":1}}],[\"选择了一部分prompt\",{\"1\":{\"87\":1}}],[\"选择不同的prompt对下游任务的性能影响较大\",{\"1\":{\"26\":1}}],[\"选定t保证w\",{\"1\":{\"33\":1}}],[\"相关\",{\"1\":{\"63\":1}}],[\"相关信息集成到\",{\"1\":{\"63\":1}}],[\"相对大量数据\",{\"1\":{\"62\":1}}],[\"相同\",{\"1\":{\"46\":1}}],[\"相当于w\",{\"1\":{\"33\":1}}],[\"相比于传统的微调\",{\"1\":{\"24\":1}}],[\"令w=tw\",{\"1\":{\"33\":1}}],[\"why\",{\"1\":{\"89\":1}}],[\"wainwright\",{\"1\":{\"89\":1}}],[\"warp\",{\"1\":{\"46\":4}}],[\"written\",{\"1\":{\"86\":1}}],[\"wei\",{\"1\":{\"89\":2}}],[\"we\",{\"1\":{\"86\":2}}],[\"wu\",{\"1\":{\"75\":1,\"89\":1}}],[\"with\",{\"1\":{\"64\":1,\"75\":1,\"85\":1,\"89\":1}}],[\"wise前馈层\",{\"1\":{\"54\":1}}],[\"wp是position嵌入矩阵\",{\"1\":{\"54\":1}}],[\"wmma\",{\"1\":{\"45\":1}}],[\"w\",{\"1\":{\"33\":3}}],[\"workshop\",{\"1\":{\"7\":1}}],[\"01625v1\",{\"1\":{\"75\":1}}],[\"03052\",{\"1\":{\"41\":1}}],[\"0310\",{\"1\":{\"33\":1}}],[\"001\",{\"1\":{\"86\":1}}],[\"003的测试\",{\"1\":{\"88\":1}}],[\"003相同的训练方法\",{\"1\":{\"87\":1}}],[\"003和\",{\"1\":{\"85\":1}}],[\"0038\",{\"1\":{\"33\":1}}],[\"0037\",{\"1\":{\"33\":1}}],[\"002的有监督指令微调\",{\"1\":{\"85\":1}}],[\"002是一个基于code\",{\"1\":{\"85\":1}}],[\"002\",{\"1\":{\"7\":1}}],[\"0\",{\"1\":{\"33\":24}}],[\"普通的linear层\",{\"1\":{\"33\":1}}],[\"基准\",{\"1\":{\"33\":1}}],[\"基于clip的ppo算法称为ppo2算法\",{\"1\":{\"94\":1}}],[\"基于\",{\"1\":{\"81\":1}}],[\"基于高性能的\",{\"1\":{\"45\":1}}],[\"基于svd参数化\",{\"1\":{\"22\":1}}],[\"基于svd的自适应\",{\"1\":{\"22\":1}}],[\"基于敏感性的重要性度量\",{\"1\":{\"22\":1}}],[\"基于奇异值的重要性度量\",{\"1\":{\"22\":1}}],[\"公式解析\",{\"0\":{\"33\":1}}],[\"公司旗下的\",{\"1\":{\"7\":1}}],[\"调研\",{\"1\":{\"29\":1}}],[\"什么是ntp任务\",{\"0\":{\"60\":1}}],[\"什么是\",{\"1\":{\"29\":1}}],[\"阅读笔记\",{\"1\":{\"29\":1}}],[\"阅读原文\",{\"1\":{\"7\":1}}],[\"参考\",{\"0\":{\"89\":1},\"1\":{\"64\":1}}],[\"参考文章\",{\"0\":{\"29\":1}}],[\"参数量就大大地降低了\",{\"1\":{\"21\":1}}],[\"参数高效微调\",{\"1\":{\"18\":1}}],[\"可找到办法\",{\"1\":{\"64\":1}}],[\"可能要再乘一个矩阵来调整形状\",{\"1\":{\"45\":1}}],[\"可学习的\",{\"1\":{\"27\":1}}],[\"可以应用于\",{\"1\":{\"75\":1}}],[\"可以应用于多个基础模型\",{\"1\":{\"75\":1}}],[\"可以直接应用于经过训练的模型\",{\"1\":{\"75\":1}}],[\"可以直观地理解lora的实现原理\",{\"1\":{\"21\":1}}],[\"可以被注入到任何现有的编码器\",{\"1\":{\"74\":1,\"75\":1}}],[\"可以看做隐性微调\",{\"1\":{\"88\":1}}],[\"可以看到\",{\"1\":{\"81\":1}}],[\"可以看到随着模型体积增大效果越来越好\",{\"1\":{\"25\":1}}],[\"可以看出\",{\"1\":{\"62\":1}}],[\"可以就这个思路深入思考两个相关问题\",{\"1\":{\"62\":1}}],[\"可以举个例子来解释这种数据压缩能力\",{\"1\":{\"61\":1}}],[\"可以在所有输入\",{\"1\":{\"75\":1}}],[\"可以在这些任务上实现巨大收益\",{\"1\":{\"51\":1}}],[\"可以在一个\",{\"1\":{\"46\":1}}],[\"提取出来\",{\"1\":{\"63\":1}}],[\"提出\",{\"1\":{\"27\":1}}],[\"提升效果\",{\"1\":{\"27\":1}}],[\"提示\",{\"1\":{\"22\":1,\"67\":1}}],[\"提示技术\",{\"0\":{\"97\":1},\"1\":{\"4\":1},\"2\":{\"98\":1,\"100\":1}}],[\"映射为一个可训练的参数\",{\"1\":{\"27\":1}}],[\"方法\",{\"1\":{\"27\":4}}],[\"方法能够将预训练的语言模型\",{\"1\":{\"18\":1}}],[\"特点\",{\"1\":{\"27\":4}}],[\"特征值的平方根\",{\"1\":{\"22\":1}}],[\"各类提示微调对比\",{\"0\":{\"27\":1}}],[\"各数据集概述\",{\"0\":{\"7\":1}}],[\"更便宜\",{\"1\":{\"78\":1}}],[\"更高效\",{\"1\":{\"75\":1}}],[\"更加适用于小一点的模型\",{\"1\":{\"26\":1}}],[\"更重要的是\",{\"1\":{\"22\":1}}],[\"改进监督模型的泛化\",{\"1\":{\"55\":1}}],[\"改为一个\",{\"1\":{\"46\":1}}],[\"改变量偏小使得效果有时候不太稳定\",{\"1\":{\"26\":1}}],[\"改动较大\",{\"1\":{\"24\":1}}],[\"也是越\",{\"1\":{\"94\":1}}],[\"也是最关键的词汇\",{\"1\":{\"63\":1}}],[\"也会对多个输入知识点产生响应\",{\"1\":{\"64\":1}}],[\"也会存储某种知识\",{\"1\":{\"63\":1}}],[\"也相同\",{\"1\":{\"46\":1}}],[\"也在transformer上的embedding输入每一层进行微调\",{\"1\":{\"26\":1}}],[\"也就是说\",{\"1\":{\"64\":1}}],[\"也就是说只会被特定输入模式激活\",{\"1\":{\"64\":1}}],[\"也就是最后一个位置的\",{\"1\":{\"63\":1}}],[\"也就是\",{\"1\":{\"63\":1}}],[\"也就是先用一个linear层a\",{\"1\":{\"21\":1}}],[\"也就是在一个batch里同时训练同一个任务的不同prompt\",{\"1\":{\"25\":1}}],[\"也就是在\",{\"1\":{\"7\":1}}],[\"三部分构成\",{\"1\":{\"26\":1}}],[\"进一步提升效果\",{\"1\":{\"26\":1}}],[\"进行编码\",{\"1\":{\"78\":1}}],[\"进行数据压缩\",{\"0\":{\"61\":1}}],[\"进行合并\",{\"1\":{\"7\":1}}],[\"进行了合并\",{\"1\":{\"7\":1}}],[\"进行了简要介绍\",{\"1\":{\"6\":1}}],[\"进行\",{\"1\":{\"7\":1}}],[\"进行改写\",{\"1\":{\"7\":1}}],[\"替换为可训练的嵌入\",{\"1\":{\"26\":1}}],[\"拼接到数据上作为输入\",{\"1\":{\"25\":1}}],[\"左图为单任务全参数微调\",{\"1\":{\"25\":1}}],[\"正常输入\",{\"1\":{\"25\":1}}],[\"之后单独询问\",{\"1\":{\"88\":1}}],[\"之后\",{\"1\":{\"63\":1}}],[\"之后拼接\",{\"1\":{\"25\":1}}],[\"之前的工作也观察到了这种辅助目标的改进性能\",{\"1\":{\"55\":1}}],[\"之前加入prefix\",{\"1\":{\"27\":1}}],[\"之间\",{\"1\":{\"7\":1}}],[\"加速收敛\",{\"1\":{\"55\":1}}],[\"加了个更大的mlp\",{\"1\":{\"24\":1}}],[\"加入了结构化数据做辅助\",{\"1\":{\"7\":1}}],[\"毕竟prompt的出现就是要解决大模型少样本的适配\",{\"1\":{\"24\":1}}],[\"精调起来效率低\",{\"1\":{\"24\":1}}],[\"值得注意的还有三个改动\",{\"1\":{\"24\":1}}],[\"我们的新优化目标和之前一样\",{\"1\":{\"94\":1}}],[\"我们现在既需要一个kl散度来约束$\",{\"1\":{\"94\":1}}],[\"我们希望这个值正负参半\",{\"1\":{\"94\":1}}],[\"我们要优化的$r\",{\"1\":{\"94\":1}}],[\"我们要确保当前的策略参数不会偏离旧策略参数太远\",{\"1\":{\"93\":1}}],[\"我们可以对$\",{\"1\":{\"94\":1}}],[\"我们可以根据模型的压缩效率来评估模型的智能程度\",{\"1\":{\"62\":1}}],[\"我们可以只消耗θ这部分的资源\",{\"1\":{\"21\":1}}],[\"我们只需要存储一个大型transformer和已知任务特定前缀的副本\",{\"1\":{\"24\":1}}],[\"与以前的方法相比\",{\"1\":{\"51\":1}}],[\"与真正的token不对应\",{\"1\":{\"24\":1}}],[\"与提示\",{\"1\":{\"24\":1}}],[\"红色前缀块\",{\"1\":{\"24\":1}}],[\"只保留每个\",{\"1\":{\"77\":1}}],[\"只是类似鹦鹉学舌的语言片段缝合怪而已\",{\"1\":{\"58\":1}}],[\"只是利用多层感知编码prefix\",{\"1\":{\"24\":1}}],[\"只有lora与adalora的效果接近全参数微调\",{\"1\":{\"28\":1}}],[\"只有prefix部分的参数进行更新\",{\"1\":{\"24\":1}}],[\"只对下游任务的输入添加额外的\",{\"1\":{\"27\":1}}],[\"只需要为每个任务存储前缀\",{\"1\":{\"24\":1}}],[\"xu\",{\"1\":{\"89\":1}}],[\"xiao\",{\"1\":{\"75\":1}}],[\"xsum\",{\"1\":{\"75\":1}}],[\"x+b\",{\"1\":{\"33\":1}}],[\"x为输入\",{\"1\":{\"26\":1}}],[\"x\",{\"1\":{\"24\":4,\"26\":3,\"33\":1,\"46\":3}}],[\"xmtf\",{\"1\":{\"7\":2}}],[\"隐式\",{\"1\":{\"24\":1}}],[\"注意多层感知机就是prefix的编码器\",{\"1\":{\"24\":1}}],[\"又包含了一个描述$\",{\"1\":{\"94\":1}}],[\"又不能像trpo算法那样将kl散度作为外在约束难以融入到梯度更新的操作中\",{\"1\":{\"94\":1}}],[\"又叫做软提示\",{\"1\":{\"23\":1}}],[\"又叫做硬提示\",{\"1\":{\"23\":1}}],[\"又称为\",{\"1\":{\"23\":2}}],[\"连续prompt直接在底层语言模型的嵌入空间中进行描述\",{\"1\":{\"23\":1}}],[\"离散prompt是一个实际的文本字符串\",{\"1\":{\"23\":1}}],[\"具备类人智能\",{\"1\":{\"58\":1}}],[\"具有高度重要性的三元组会被保留\",{\"1\":{\"22\":1}}],[\"具有低重要性分数的三元组被授予低优先级\",{\"1\":{\"22\":1}}],[\"具体例子可以参照下图\",{\"1\":{\"63\":1}}],[\"具体提取动作是通过某个\",{\"1\":{\"63\":1}}],[\"具体是在计算时对注意力做一些变形\",{\"1\":{\"45\":1}}],[\"具体来说\",{\"1\":{\"22\":1}}],[\"具体而言\",{\"1\":{\"22\":1,\"55\":1}}],[\"qk\",{\"1\":{\"45\":1}}],[\"qkv\",{\"1\":{\"42\":1}}],[\"q\",{\"1\":{\"45\":1}}],[\"query\",{\"1\":{\"25\":1}}],[\"quality\",{\"1\":{\"7\":1}}],[\"q的等级\",{\"1\":{\"22\":1}}],[\"vincent\",{\"1\":{\"89\":1}}],[\"values\",{\"1\":{\"88\":1}}],[\"valid\",{\"1\":{\"44\":1,\"46\":2}}],[\"vanilla\",{\"1\":{\"75\":1}}],[\"v1\",{\"1\":{\"26\":1}}],[\"v1将自然语言提示的token\",{\"1\":{\"26\":1}}],[\"v\",{\"1\":{\"22\":1,\"45\":1}}],[\"v2因为每层插入了token\",{\"1\":{\"26\":1}}],[\"v2则不只是针对embedding层\",{\"1\":{\"26\":1}}],[\"v2简单来说其实是soft\",{\"1\":{\"26\":1}}],[\"v2将prefix\",{\"1\":{\"26\":1}}],[\"v2提升小模型上的prompt\",{\"1\":{\"26\":1}}],[\"v2\",{\"1\":{\"7\":1,\"20\":1,\"27\":2,\"29\":1}}],[\"此外\",{\"1\":{\"22\":1,\"75\":1,\"76\":1}}],[\"右图为\",{\"1\":{\"25\":1}}],[\"右奇异向量\",{\"1\":{\"22\":1}}],[\"右侧看起来像是左侧原有矩阵w的分解\",{\"1\":{\"21\":1}}],[\"迭代地将svd应用于大量高维权重矩阵会变得非常昂贵\",{\"1\":{\"22\":1}}],[\"然而$\",{\"1\":{\"94\":1}}],[\"然而\",{\"1\":{\"22\":1,\"42\":1,\"76\":1}}],[\"然后将这些元梯度应用于原始gpt以构建icl模型\",{\"1\":{\"88\":1}}],[\"然后将其送到添加的具有参数的线性输出层来以预测\",{\"1\":{\"55\":1}}],[\"然后是text\",{\"1\":{\"85\":1}}],[\"然后是position\",{\"1\":{\"54\":1}}],[\"然后介绍了chatgpt模型最重要的技术指令微调\",{\"1\":{\"84\":1}}],[\"然后在每个特定任务上进行歧视性微调\",{\"1\":{\"51\":1}}],[\"然后在每个transformer块里注入可训练的层\",{\"1\":{\"21\":1}}],[\"然后通过\",{\"1\":{\"46\":1}}],[\"然后训练的时候只更新prefix部分的参数\",{\"1\":{\"24\":1}}],[\"然后截断最小的奇异值\",{\"1\":{\"22\":1}}],[\"然后\",{\"1\":{\"22\":1,\"75\":1,\"78\":1}}],[\"yaru\",{\"1\":{\"89\":1}}],[\"yutao\",{\"1\":{\"89\":1}}],[\"yu\",{\"1\":{\"89\":1}}],[\"yun\",{\"1\":{\"22\":1}}],[\"y=tw\",{\"1\":{\"33\":1}}],[\"y=wx+b\",{\"1\":{\"33\":1}}],[\"y为输出\",{\"1\":{\"26\":1}}],[\"y\",{\"1\":{\"24\":4,\"26\":1,\"33\":2,\"89\":1}}],[\"yizhongw\",{\"1\":{\"7\":3}}],[\"kl惩罚的优势在于\",{\"1\":{\"94\":1}}],[\"kl惩罚\",{\"1\":{\"94\":1}}],[\"kl\",{\"1\":{\"94\":6}}],[\"kl散度也越高\",{\"1\":{\"94\":1}}],[\"kl散度可以计算两个分布的不相似度\",{\"1\":{\"94\":1}}],[\"kl散度的外在约束\",{\"1\":{\"94\":1}}],[\"kelvin\",{\"1\":{\"89\":1}}],[\"keys\",{\"1\":{\"88\":1}}],[\"kernel\",{\"1\":{\"42\":1,\"45\":4,\"46\":2}}],[\"knn\",{\"1\":{\"76\":1,\"78\":1}}],[\"knowledge\",{\"1\":{\"7\":1}}],[\"kociskýet\",{\"1\":{\"75\":1}}],[\"koltchinskii等人\",{\"1\":{\"22\":1}}],[\"krys\",{\"1\":{\"75\":1}}],[\"k\",{\"1\":{\"45\":1,\"75\":1,\"76\":1,\"78\":4}}],[\"k个\",{\"1\":{\"27\":1}}],[\"有时会简称为\",{\"1\":{\"60\":1}}],[\"有助于学习\",{\"1\":{\"55\":1}}],[\"有一些控制矩阵秩的方法\",{\"1\":{\"22\":1}}],[\"有效地分配参数预算\",{\"1\":{\"22\":1}}],[\"有效地适应各种下游应用程序\",{\"1\":{\"18\":1,\"19\":1}}],[\"根据常见的上下文窗口长度绘制了几个流行的摘要和问答数据集的大小\",{\"1\":{\"75\":1}}],[\"根据输出的下一个token的概率分布进行算术编码\",{\"1\":{\"61\":1}}],[\"根据已有的token\",{\"1\":{\"61\":1}}],[\"根据\",{\"1\":{\"44\":1}}],[\"根据结果可以看出\",{\"1\":{\"28\":1}}],[\"根据新的重要性指标\",{\"1\":{\"22\":1}}],[\"根据论文的研究结果分析\",{\"1\":{\"21\":1}}],[\"奇异值\",{\"1\":{\"22\":1}}],[\"它不使用kl散度来描述两种分布的不相似度\",{\"1\":{\"94\":1}}],[\"它告诉我们只要以奖励的期望式1\",{\"1\":{\"94\":1}}],[\"它对标准的策略梯度方法做了改进\",{\"1\":{\"93\":1}}],[\"它们的kl散度值为0\",{\"1\":{\"94\":1}}],[\"它们都在2022年11月发布\",{\"1\":{\"85\":1}}],[\"它们各自只对输入里某个特殊的知识点产生响应\",{\"1\":{\"64\":1}}],[\"它们大多直接计算矩阵的奇异值分解\",{\"1\":{\"22\":1}}],[\"它证明了\",{\"1\":{\"63\":1}}],[\"它在低层已经集成了单词\",{\"1\":{\"63\":1}}],[\"它的信息在顺着\",{\"1\":{\"63\":1}}],[\"它实现了变长输入的\",{\"1\":{\"42\":1}}],[\"它根据不同的模型结构定义了不同的prompt拼接方式\",{\"1\":{\"24\":1}}],[\"它根据我们新设计的重要性度量修剪冗余奇异值\",{\"1\":{\"22\":1}}],[\"它考虑了gi中每个条目对模型性能的贡献\",{\"1\":{\"22\":1}}],[\"它以奇异值分解的形式表示增量矩阵∆\",{\"1\":{\"22\":1}}],[\"不相似度越高\",{\"1\":{\"94\":1}}],[\"不能满足无害性等要求\",{\"1\":{\"87\":1}}],[\"不能在一个手写\",{\"1\":{\"45\":1}}],[\"不断往\",{\"1\":{\"63\":1}}],[\"不仅在这些数据集上比\",{\"1\":{\"75\":1}}],[\"不仅学会了语言元素间的表面统计关系\",{\"1\":{\"58\":1}}],[\"不仅只在embedding上进行微调\",{\"1\":{\"26\":1}}],[\"不同\",{\"1\":{\"46\":1}}],[\"不同的注意力头可能会关注不同类型的信息\",{\"1\":{\"76\":1}}],[\"不同的信息可能是相关的\",{\"1\":{\"76\":1}}],[\"不同的是\",{\"1\":{\"24\":1}}],[\"不同的prefix同时加在编码器和解码器的开头\",{\"1\":{\"24\":1}}],[\"不针对b\",{\"1\":{\"33\":1}}],[\"不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新\",{\"1\":{\"24\":1}}],[\"不过使用了更\",{\"1\":{\"7\":1}}],[\"不再像prompt是人为构造的\",{\"1\":{\"24\":1}}],[\"不太重要的增量矩阵被修剪为具有较低的秩\",{\"1\":{\"22\":1}}],[\"以使模型具备人类倾向的回答问题能力\",{\"1\":{\"86\":1}}],[\"以上的注意力性能\",{\"1\":{\"78\":1}}],[\"以上描述的过程是对称量化\",{\"1\":{\"34\":1}}],[\"以确保编码过程前后都有足够的上下文\",{\"1\":{\"77\":1}}],[\"以进行进一步改进\",{\"1\":{\"75\":1}}],[\"以进一步提高性能\",{\"1\":{\"42\":1}}],[\"以在测试时接受无限长度的输入\",{\"1\":{\"75\":1}}],[\"以在目标token上产生输出分布\",{\"1\":{\"54\":1}}],[\"以获得最终transformer块的激活\",{\"1\":{\"55\":1}}],[\"以\",{\"1\":{\"45\":1}}],[\"以模拟svd\",{\"1\":{\"22\":1}}],[\"以奇异值分解的形式对权重矩阵的增量更新进行参数化\",{\"1\":{\"22\":1}}],[\"以防止过度拟合并节省计算预算\",{\"1\":{\"22\":1}}],[\"以控制其预算\",{\"1\":{\"22\":1}}],[\"以提高参数高效微调的性能\",{\"1\":{\"22\":1}}],[\"自然就短得多\",{\"1\":{\"62\":1}}],[\"自然语言提示本身十分脆弱\",{\"1\":{\"26\":1}}],[\"自动化地寻找连续空间中的知识模板\",{\"1\":{\"27\":1}}],[\"自适应的低秩自适应\",{\"1\":{\"22\":1}}],[\"自主生成的\",{\"1\":{\"7\":1}}],[\"自主生成\",{\"1\":{\"7\":1}}],[\"为新的推理可能性打开了大门\",{\"1\":{\"67\":1}}],[\"为界划分为两种实现方式\",{\"1\":{\"45\":1}}],[\"为消除这一影响\",{\"1\":{\"26\":1}}],[\"为每一个任务\",{\"1\":{\"25\":1}}],[\"为了解决采样时间开销大的问题\",{\"1\":{\"94\":1}}],[\"为了解决这个问题\",{\"1\":{\"42\":1,\"46\":1}}],[\"为了将比模型的上下文窗口长度更长的输入序列进行编码\",{\"1\":{\"77\":1}}],[\"为了增加模型参数的利用效率\",{\"1\":{\"64\":1}}],[\"为了优化\",{\"1\":{\"45\":1}}],[\"为了量化三元组的重要性\",{\"1\":{\"22\":1}}],[\"为了正则化p和q的正交性\",{\"1\":{\"22\":1}}],[\"为了回答这个问题\",{\"1\":{\"22\":1}}],[\"为方便读者阅读\",{\"1\":{\"7\":1}}],[\"如式3\",{\"1\":{\"94\":1}}],[\"如式2\",{\"1\":{\"94\":1}}],[\"如\",{\"1\":{\"75\":1}}],[\"如书籍摘要\",{\"1\":{\"75\":1}}],[\"如果$a\",{\"1\":{\"94\":2}}],[\"如果我们更严谨地来看\",{\"1\":{\"62\":1}}],[\"如果不理解这个概念\",{\"1\":{\"62\":1}}],[\"如果\",{\"1\":{\"62\":1}}],[\"如果大语言模型具备越强的数据压缩能力\",{\"1\":{\"61\":1}}],[\"如果提供了任务说明\",{\"1\":{\"7\":1}}],[\"如此简单的操作\",{\"1\":{\"60\":1}}],[\"如涉侵权\",{\"1\":{\"58\":1}}],[\"如tensorflow\",{\"1\":{\"42\":1}}],[\"如上图所示\",{\"1\":{\"26\":1,\"64\":1}}],[\"如下图b所示\",{\"1\":{\"26\":1}}],[\"如下图所示\",{\"1\":{\"26\":2}}],[\"如下图中的红色块所示\",{\"1\":{\"24\":1}}],[\"如何根据模块的重要性自适应地分配参数预算\",{\"1\":{\"22\":1}}],[\"因为它把\",{\"1\":{\"94\":1}}],[\"因为这样就可以衡量策略是好还是坏\",{\"1\":{\"94\":1}}],[\"因为原生注意力机制具有平方级的复杂度\",{\"1\":{\"75\":1}}],[\"因为在它的最上层会给出\",{\"1\":{\"63\":1}}],[\"因为在前向计算的时候\",{\"1\":{\"21\":1}}],[\"因为从线程角度看\",{\"1\":{\"46\":1}}],[\"因为共享内存大小限制\",{\"1\":{\"45\":1}}],[\"因为可以把\",{\"1\":{\"45\":1}}],[\"因为大模型参数量大\",{\"1\":{\"24\":1}}],[\"因此考虑将kl散度加入到优化目标式3\",{\"1\":{\"94\":1}}],[\"因此必须有一个约束\",{\"1\":{\"94\":1}}],[\"因此非对称量化的w\",{\"1\":{\"34\":1}}],[\"因此对显存来说相当于多存了t的对角元素\",{\"1\":{\"33\":1}}],[\"因此t完全由w决定\",{\"1\":{\"33\":1}}],[\"因此作者在每层都加了prompt的参数\",{\"1\":{\"24\":1}}],[\"因此奇异值被清零\",{\"1\":{\"22\":1}}],[\"因此\",{\"1\":{\"22\":1,\"24\":2,\"55\":1,\"76\":1}}],[\"因此论文提出了以下问题\",{\"1\":{\"22\":1}}],[\"甚至会损害模型性能\",{\"1\":{\"22\":1}}],[\"甚至超过了在\",{\"1\":{\"7\":1}}],[\"即后面这一项效果太弱了\",{\"1\":{\"94\":1}}],[\"即后面这一项效果太强了\",{\"1\":{\"94\":1}}],[\"即采样的次数足够多的情况下式1\",{\"1\":{\"94\":1}}],[\"即用prompt格式的训练数据进行finetune\",{\"1\":{\"86\":1}}],[\"即可\",{\"1\":{\"46\":1}}],[\"即自适应预算分配以实现参数有效的微调\",{\"1\":{\"22\":1}}],[\"即参数高效微调\",{\"1\":{\"19\":1}}],[\"引入自适应kl惩罚\",{\"1\":{\"94\":1}}],[\"引入了重要性采样\",{\"1\":{\"94\":1}}],[\"引入lora部分的参数\",{\"1\":{\"21\":1}}],[\"引入task\",{\"1\":{\"7\":1}}],[\"梯度计算量少了很多\",{\"1\":{\"21\":1}}],[\"尽管参数看起来增加了\",{\"1\":{\"21\":1}}],[\"θ\",{\"1\":{\"21\":3}}],[\"表1\",{\"1\":{\"87\":1}}],[\"表现更好\",{\"1\":{\"75\":1}}],[\"表现出了良好的效果\",{\"1\":{\"7\":1}}],[\"表示\",{\"1\":{\"21\":1}}],[\"φ部分还是需要参与计算的\",{\"1\":{\"21\":1}}],[\"φ\",{\"1\":{\"21\":4}}],[\"从策略梯度算法到近端策略优化算法\",{\"0\":{\"93\":1}}],[\"从此它就开始不断进化\",{\"1\":{\"85\":1}}],[\"从第二个式子可以看到\",{\"1\":{\"21\":1}}],[\"从论文中的公式来看\",{\"1\":{\"21\":1}}],[\"从而使gpt产生的答案更偏向于标注人员的喜好\",{\"1\":{\"87\":1}}],[\"从而将参数量从\",{\"1\":{\"21\":1}}],[\"从而大大降低了计算和存储成本\",{\"1\":{\"18\":1,\"19\":1}}],[\"理论上都可以用到这样的分解\",{\"1\":{\"21\":1}}],[\"理解\",{\"1\":{\"7\":1}}],[\"而后半部分\",{\"1\":{\"94\":1}}],[\"而不是比较谁更好\",{\"1\":{\"94\":1}}],[\"而不是截断关键字\",{\"1\":{\"78\":1}}],[\"而在调试的过程中gpt3\",{\"1\":{\"87\":1}}],[\"而一个\",{\"1\":{\"64\":1}}],[\"而模型若能给出越短的描述\",{\"1\":{\"62\":1}}],[\"而零填充会引入大量的额外计算开销\",{\"1\":{\"42\":1}}],[\"而非对称量化是把每一行的最大值变换到127\",{\"1\":{\"34\":1}}],[\"而保留了w\",{\"1\":{\"33\":1}}],[\"而p\",{\"1\":{\"27\":1}}],[\"而prefix则是可以学习的\",{\"1\":{\"24\":1}}],[\"而且更加注重生成答案的无害性和对话性\",{\"1\":{\"87\":1}}],[\"而且本文还发现\",{\"1\":{\"75\":1}}],[\"而且这篇文章证明了\",{\"1\":{\"63\":1}}],[\"而且学到了人类语言甚至包括物理世界的内在运行规律\",{\"1\":{\"58\":1}}],[\"而且冻结模型所有参数去学习插入token\",{\"1\":{\"26\":1}}],[\"而且从优化角度无法达到最优\",{\"1\":{\"26\":1}}],[\"而transformer中的其他部分参数固定\",{\"1\":{\"24\":1}}],[\"而自动化生成prompt又分为离散提示\",{\"1\":{\"23\":1}}],[\"而正交矩阵p和q表示∆的左\",{\"1\":{\"22\":1}}],[\"而是使用裁剪函数clip\",{\"1\":{\"94\":1}}],[\"而是连续向量\",{\"1\":{\"27\":1}}],[\"而是将连续型token插入每一层\",{\"1\":{\"26\":1}}],[\"而是将∆参数化为∆=p∧q\",{\"1\":{\"22\":1}}],[\"而是所有出现大矩阵的地方\",{\"1\":{\"21\":1}}],[\"而θ部分是凭空增加了的参数\",{\"1\":{\"21\":1}}],[\"而根据假设\",{\"1\":{\"21\":1}}],[\"而加入了lora之后\",{\"1\":{\"21\":1}}],[\"而lora保留了原来的矩阵w\",{\"1\":{\"21\":1}}],[\"而无需微调模型的所有参数\",{\"1\":{\"18\":1,\"19\":1}}],[\"这构成了一个新的监督训练数据集\",{\"1\":{\"87\":1}}],[\"这一步得到的模型是davinci\",{\"1\":{\"87\":1}}],[\"这一方面说明大模型确实可以学习一些抽象概念\",{\"1\":{\"62\":1}}],[\"这对于目前许多文本摘要数据集\",{\"1\":{\"75\":1}}],[\"这类神经元被称为\",{\"1\":{\"64\":1}}],[\"这类\",{\"1\":{\"64\":1}}],[\"这点对应该是个新知识\",{\"1\":{\"63\":1}}],[\"这里需要注意一下\",{\"1\":{\"63\":1}}],[\"这个值越小越好\",{\"1\":{\"94\":1}}],[\"这个值越大越好\",{\"1\":{\"94\":1}}],[\"这个式子前半部分的数学期望\",{\"1\":{\"94\":1}}],[\"这个文章指出了\",{\"1\":{\"64\":1}}],[\"这个信息在高层\",{\"1\":{\"63\":1}}],[\"这个操作也发生在\",{\"1\":{\"63\":1}}],[\"这个单词对应层数的\",{\"1\":{\"63\":1}}],[\"这个算法源自字节跳动\",{\"1\":{\"44\":1}}],[\"这是一种基于检索的方法\",{\"1\":{\"75\":1}}],[\"这是第一个步骤\",{\"1\":{\"63\":1}}],[\"这是目前\",{\"1\":{\"62\":1}}],[\"这是transformer的变体\",{\"1\":{\"54\":1}}],[\"这种方法增强了预训练的语言模型\",{\"1\":{\"75\":1}}],[\"这种方法可以有效地提高模型性能和参数效率\",{\"1\":{\"22\":1}}],[\"这种编码机制被称为\",{\"1\":{\"64\":1}}],[\"这种\",{\"1\":{\"58\":1}}],[\"这些资源随着上下文窗口大小的增加而增加\",{\"1\":{\"75\":1}}],[\"这些算法在保证运算正确性的前提下\",{\"1\":{\"41\":1}}],[\"这些自然语言指令清楚而完整地描述了一项任务\",{\"1\":{\"7\":1}}],[\"这两种prompt的含义如下\",{\"1\":{\"23\":1}}],[\"这保留了未来恢复的可能性\",{\"1\":{\"22\":1}}],[\"这样我们可以对$\",{\"1\":{\"94\":1}}],[\"这样就能从整个输入序列中检索关键字\",{\"1\":{\"78\":1}}],[\"这样相当于训练了不同\",{\"1\":{\"25\":1}}],[\"这样的参数化避免了svd的密集计算\",{\"1\":{\"22\":1}}],[\"这样的操作可以显式地操纵秩\",{\"1\":{\"22\":1}}],[\"这样它们可以捕获更细粒度和特定于任务的信息\",{\"1\":{\"22\":1}}],[\"这样一来就可以在单卡低显存的情况下训练大模型了\",{\"1\":{\"21\":1}}],[\"这样一来需要训练的参数量就减少了很多\",{\"1\":{\"21\":1}}],[\"这就使得训练过程中\",{\"1\":{\"21\":1}}],[\"其监督微调\",{\"1\":{\"86\":1}}],[\"其实是\",{\"1\":{\"62\":1}}],[\"其实并未具备智能\",{\"1\":{\"58\":1}}],[\"其二是跨层参数共享\",{\"1\":{\"21\":1}}],[\"其一是embedding矩阵分解\",{\"1\":{\"21\":1}}],[\"其中每个实例由一系列输入token以及标签\",{\"1\":{\"55\":1}}],[\"其中每个三元组gi包含第i个奇异值和相应的奇异向量\",{\"1\":{\"22\":1}}],[\"其中u=\",{\"1\":{\"54\":1}}],[\"其中k是上下文窗口的大小\",{\"1\":{\"54\":1}}],[\"其中t是一个对角矩阵\",{\"1\":{\"33\":1}}],[\"其中\",{\"1\":{\"20\":1,\"21\":2,\"46\":1}}],[\"该模型在外部数据存储中执行\",{\"1\":{\"76\":1}}],[\"该模型在输入上下文token上应用multi\",{\"1\":{\"54\":1}}],[\"该文介绍了\",{\"1\":{\"67\":1}}],[\"该方法冻结lm参数\",{\"1\":{\"24\":1}}],[\"该方法其实和构造prompt类似\",{\"1\":{\"24\":1}}],[\"该方法是在输入token之前构造一段任务相关的virtual\",{\"1\":{\"24\":1}}],[\"该方法只需要在保持奇异向量的同时删除不重要的奇异值\",{\"1\":{\"22\":1}}],[\"该方法在类似lora的微调过程中在权重矩阵之间动态分配参数预算\",{\"1\":{\"22\":1}}],[\"该思想与albert的思想有异曲同工之处\",{\"1\":{\"21\":1}}],[\"该项目包含了\",{\"1\":{\"7\":1}}],[\"事实上\",{\"1\":{\"21\":1}}],[\"<\",{\"1\":{\"21\":4,\"63\":1,\"94\":1}}],[\"+\",{\"1\":{\"21\":1,\"62\":2}}],[\"∗\",{\"1\":{\"21\":1}}],[\"dong\",{\"1\":{\"89\":1}}],[\"diogo\",{\"1\":{\"89\":1}}],[\"directly\",{\"1\":{\"88\":1}}],[\"dissecting\",{\"1\":{\"63\":1}}],[\"discrete\",{\"1\":{\"23\":1}}],[\"dm\",{\"1\":{\"75\":1}}],[\"d\",{\"1\":{\"64\":3}}],[\"dtype=torch\",{\"1\":{\"33\":4}}],[\"delta\",{\"1\":{\"94\":1}}],[\"descent\",{\"1\":{\"89\":1}}],[\"december\",{\"1\":{\"89\":1}}],[\"decoder用于语言模型\",{\"1\":{\"54\":1}}],[\"decoder模型上采用\",{\"1\":{\"24\":1}}],[\"decoder的bart\",{\"1\":{\"24\":1}}],[\"deberta\",{\"1\":{\"47\":1}}],[\"device=\",{\"1\":{\"33\":8}}],[\"deep\",{\"1\":{\"27\":1}}],[\"d维降到r\",{\"1\":{\"21\":1}}],[\"dai\",{\"1\":{\"89\":1}}],[\"damai\",{\"1\":{\"89\":1}}],[\"dataset\",{\"2\":{\"12\":1}}],[\"datasets\",{\"1\":{\"7\":4}}],[\"davinci\",{\"1\":{\"7\":1,\"85\":3,\"86\":1,\"87\":1,\"88\":1}}],[\"旁支\",{\"1\":{\"21\":1}}],[\"实体\",{\"1\":{\"63\":1}}],[\"实现\",{\"1\":{\"44\":1,\"45\":1,\"46\":1,\"47\":1}}],[\"实现了\",{\"1\":{\"42\":1}}],[\"实现了端到端的推理过程的大幅优化\",{\"1\":{\"41\":1}}],[\"实际测试下来只作用在embedding层的话交互能力会变弱\",{\"1\":{\"26\":1}}],[\"实际上是增加了右侧的\",{\"1\":{\"21\":1}}],[\"实验结果\",{\"0\":{\"28\":1,\"79\":1}}],[\"实验结果表明\",{\"1\":{\"7\":1}}],[\"实验证实只加到embedding上的效果不太好\",{\"1\":{\"24\":1}}],[\"直白的来说\",{\"1\":{\"21\":1}}],[\"直译为大语言模型的低阶适应\",{\"1\":{\"21\":1}}],[\"所以它处理起来非常困难\",{\"1\":{\"94\":1}}],[\"所以它是被发现存在\",{\"1\":{\"64\":1}}],[\"所以直接基于gpt3\",{\"1\":{\"87\":1}}],[\"所以仅仅通过一个多语义神经元是无法探测当前是对谁在做出响应\",{\"1\":{\"64\":1}}],[\"所以随着信息往上层流动\",{\"1\":{\"63\":1}}],[\"所以就越聪明\",{\"1\":{\"62\":1}}],[\"所以就在低资源的情况下\",{\"1\":{\"21\":1}}],[\"所以在这种情况下量化是一个必然的选择\",{\"1\":{\"32\":1}}],[\"所以理论上\",{\"1\":{\"21\":1}}],[\"所以需要计算梯度的部分就只剩下旁支的a和b两个小矩阵\",{\"1\":{\"21\":1}}],[\"所以将embedding矩阵分解成两个相对较小的矩阵\",{\"1\":{\"21\":1}}],[\"所以\",{\"1\":{\"21\":1,\"58\":1,\"62\":1,\"64\":1}}],[\"冻结预训练好的模型权重参数\",{\"1\":{\"21\":1}}],[\"训练的时间开销集中在了数据采样\",{\"1\":{\"94\":1}}],[\"训练过程可以看成是对数据的无损压缩\",{\"1\":{\"62\":1}}],[\"训练框架\",{\"0\":{\"53\":1}}],[\"训练完只保存mlp变换后的参数就行了\",{\"1\":{\"24\":1}}],[\"训练出任务对应prompt的embedding向量\",{\"1\":{\"20\":1}}],[\"训练集\",{\"1\":{\"7\":1}}],[\"谷歌\",{\"1\":{\"20\":1}}],[\"北京智源\",{\"1\":{\"20\":1}}],[\"清华\",{\"1\":{\"20\":1}}],[\"清华keg\",{\"1\":{\"20\":1}}],[\"斯坦福\",{\"1\":{\"20\":1}}],[\"brian\",{\"1\":{\"89\":1}}],[\"broad\",{\"1\":{\"86\":1}}],[\"bosma\",{\"1\":{\"89\":1}}],[\"block\",{\"1\":{\"63\":2}}],[\"by\",{\"0\":{\"51\":1},\"1\":{\"63\":2}}],[\"bytedance\",{\"1\":{\"41\":1}}],[\"bytetransformer\",{\"0\":{\"41\":1},\"1\":{\"41\":2,\"42\":1,\"44\":1,\"45\":1,\"46\":1,\"47\":1},\"2\":{\"50\":1}}],[\"bias\",{\"1\":{\"45\":1}}],[\"bigscience\",{\"1\":{\"7\":12}}],[\"batch\",{\"1\":{\"44\":1,\"46\":4}}],[\"bartbase\",{\"1\":{\"81\":1}}],[\"bart\",{\"1\":{\"24\":1,\"75\":2}}],[\"b\",{\"1\":{\"25\":1,\"33\":2}}],[\"b2\",{\"1\":{\"25\":1}}],[\"b1\",{\"1\":{\"25\":1}}],[\"begin\",{\"1\":{\"94\":3}}],[\"beta$值\",{\"1\":{\"94\":2}}],[\"beta$乘以$\",{\"1\":{\"94\":1}}],[\"beta\",{\"1\":{\"87\":1,\"94\":1}}],[\"beta和text\",{\"1\":{\"86\":1}}],[\"beltagy\",{\"1\":{\"75\":1}}],[\"beat\",{\"1\":{\"63\":2}}],[\"beats\",{\"1\":{\"63\":1}}],[\"bert\",{\"1\":{\"47\":1,\"75\":1}}],[\"be\",{\"1\":{\"20\":1}}],[\"budget\",{\"1\":{\"20\":1}}],[\"微软\",{\"1\":{\"20\":2}}],[\"微调大型\",{\"1\":{\"18\":1}}],[\"微调技术\",{\"0\":{\"37\":1},\"1\":{\"4\":1},\"2\":{\"30\":1,\"35\":1,\"38\":1,\"40\":1}}],[\"meta\",{\"1\":{\"89\":1}}],[\"memorizing\",{\"1\":{\"75\":1}}],[\"min\",{\"1\":{\"94\":2}}],[\"mit\",{\"1\":{\"89\":1}}],[\"mishkin\",{\"1\":{\"89\":1}}],[\"model\",{\"1\":{\"87\":1}}],[\"models\",{\"1\":{\"20\":1,\"21\":1,\"63\":1,\"86\":1,\"89\":3}}],[\"music\",{\"1\":{\"63\":6}}],[\"multi\",{\"1\":{\"45\":2}}],[\"mlp\",{\"1\":{\"42\":1}}],[\"max\",{\"1\":{\"94\":1}}],[\"ma\",{\"1\":{\"89\":1}}],[\"maarten\",{\"1\":{\"89\":1}}],[\"mask\",{\"1\":{\"26\":2,\"44\":1}}],[\"main\",{\"1\":{\"7\":2}}],[\"同样使用该算法去除对\",{\"1\":{\"44\":1}}],[\"同样是使用\",{\"1\":{\"7\":1}}],[\"同时通常还能保留\",{\"1\":{\"78\":1}}],[\"同时对模型架构所需的更改最小\",{\"1\":{\"51\":1}}],[\"同时利用lstm进行reparamerization加速训练\",{\"1\":{\"26\":1}}],[\"同时\",{\"1\":{\"25\":1}}],[\"同时freeze预训练模型进行训练\",{\"1\":{\"25\":1}}],[\"同时是hugging\",{\"1\":{\"19\":1}}],[\"技术\",{\"1\":{\"19\":1}}],[\"额外添加一个或多个\",{\"1\":{\"25\":1}}],[\"额外\",{\"1\":{\"18\":1,\"19\":1}}],[\"最近邻\",{\"1\":{\"76\":1}}],[\"最近的peft技术实现了与完全微调相当的性能\",{\"1\":{\"19\":1}}],[\"最长的输入比\",{\"1\":{\"75\":1}}],[\"最后介绍了上下文学习\",{\"1\":{\"84\":1}}],[\"最后\",{\"1\":{\"77\":1}}],[\"最后一个单词对应的\",{\"1\":{\"63\":1}}],[\"最后把输出拼接在一起作为总输出\",{\"1\":{\"45\":1}}],[\"最后再将左右两部分的结果相加融合\",{\"1\":{\"21\":1}}],[\"最小值变换到−128\",{\"1\":{\"34\":1}}],[\"最小化结果矩阵和原始矩阵之间的差异\",{\"1\":{\"22\":1}}],[\"最开始应用在nlg任务上\",{\"1\":{\"26\":1}}],[\"最关键的就是引入prefix\",{\"1\":{\"26\":1}}],[\"最终我们的优化目标确定了\",{\"1\":{\"94\":1}}],[\"最终追上了精调的效果\",{\"1\":{\"25\":1}}],[\"最终得到了\",{\"1\":{\"7\":1}}],[\"最先进的参数高效微调方法\",{\"0\":{\"18\":1}}],[\"epsilon\",{\"1\":{\"94\":2}}],[\"end\",{\"1\":{\"94\":3}}],[\"ensembling\",{\"1\":{\"25\":1}}],[\"eta\",{\"1\":{\"94\":1}}],[\"et\",{\"1\":{\"75\":7,\"76\":1,\"77\":4,\"89\":3}}],[\"effect\",{\"1\":{\"88\":1}}],[\"effective\",{\"1\":{\"42\":1,\"44\":1}}],[\"efficient\",{\"1\":{\"19\":1,\"20\":2}}],[\"embedding\",{\"1\":{\"25\":2,\"63\":2}}],[\"embedded\",{\"1\":{\"2\":1}}],[\"eval\",{\"2\":{\"16\":1}}],[\"对我们来说\",{\"1\":{\"94\":2}}],[\"对其它无关输入保持沉默\",{\"1\":{\"64\":1}}],[\"对应位置\",{\"1\":{\"63\":1}}],[\"对应的属性\",{\"1\":{\"63\":1}}],[\"对应的\",{\"1\":{\"7\":1,\"63\":1}}],[\"对同一个输入\",{\"1\":{\"46\":1}}],[\"对\",{\"1\":{\"46\":1}}],[\"对称量化把每一行的绝对值的最大值变换到127\",{\"1\":{\"34\":1}}],[\"对每个额外任务产生非常小的开销\",{\"1\":{\"24\":1}}],[\"对角矩阵∧包含奇异值\",{\"1\":{\"22\":1}}],[\"对于每个交叉注意头\",{\"1\":{\"78\":1}}],[\"对于长\",{\"1\":{\"45\":1}}],[\"对于短\",{\"1\":{\"45\":1}}],[\"对于\",{\"1\":{\"45\":1}}],[\"对于单向语言模型采用\",{\"1\":{\"26\":1}}],[\"对于bert类双向语言模型采用模版\",{\"1\":{\"26\":1}}],[\"对于encoder\",{\"1\":{\"24\":1}}],[\"对于decoder\",{\"1\":{\"24\":1}}],[\"对于微调大型模型\",{\"1\":{\"22\":1}}],[\"对于左右两个部分\",{\"1\":{\"21\":1}}],[\"对话指令\",{\"1\":{\"7\":1}}],[\"将式1\",{\"1\":{\"94\":1}}],[\"将语言建模作为微调的辅助目标\",{\"1\":{\"55\":1}}],[\"将数据从r变回d维\",{\"1\":{\"21\":1}}],[\"将数据从\",{\"1\":{\"21\":1}}],[\"将flan\",{\"1\":{\"7\":1}}],[\"将结构化数据序列化并嵌入到prompt中\",{\"1\":{\"7\":1}}],[\"简介\",{\"1\":{\"7\":1}}],[\"作为\",{\"1\":{\"44\":1}}],[\"作者优化了以下目标\",{\"1\":{\"55\":1}}],[\"作者还发现\",{\"1\":{\"55\":1}}],[\"作者将参数调整为受监督的目标任务\",{\"1\":{\"55\":1}}],[\"作者将多层transformer\",{\"1\":{\"54\":1}}],[\"作者使用标准语言建模目标来最大化以下概率\",{\"1\":{\"54\":1}}],[\"作者证明了通过在大量未标注文本上对语言模型进行生成式预训练\",{\"1\":{\"51\":1}}],[\"作者发现直接优化prompt参数不太稳定\",{\"1\":{\"24\":1}}],[\"作者考虑到词表的维度很大\",{\"1\":{\"21\":1}}],[\"作者通过两个策略降低了训练的参数量\",{\"1\":{\"21\":1}}],[\"作者\",{\"1\":{\"7\":1}}],[\"作用巨大\",{\"1\":{\"6\":1}}],[\"组织\",{\"1\":{\"7\":1}}],[\"组织之一\",{\"1\":{\"7\":1}}],[\"上下文学习\",{\"0\":{\"88\":1},\"1\":{\"88\":1}}],[\"上浪费精力\",{\"1\":{\"76\":1}}],[\"上图中蓝色部分\",{\"1\":{\"64\":1}}],[\"上面讲述内容是以数据压缩的视角来看待\",{\"1\":{\"62\":1}}],[\"上开源了\",{\"1\":{\"47\":1}}],[\"上进行指令微调的尝试\",{\"1\":{\"7\":1}}],[\"上述数据集可以总结概括为以下表格\",{\"1\":{\"7\":1}}],[\"上你可以找到\",{\"1\":{\"7\":1}}],[\"模型并不能正确回答\",{\"1\":{\"88\":1}}],[\"模型训练步骤\",{\"1\":{\"87\":1}}],[\"模型训练的优化表示为\",{\"1\":{\"21\":1}}],[\"模型在\",{\"1\":{\"63\":1}}],[\"模型在对话任务上的表现强于在超大规模任务集上的结果\",{\"1\":{\"7\":1}}],[\"模型对知识的提取过程\",{\"0\":{\"63\":1}}],[\"模型压缩效率越高\",{\"1\":{\"62\":1}}],[\"模型智能程度越高\",{\"1\":{\"62\":1}}],[\"模型仅仅学会了语言中的单词共现等浅层的表面统计关系\",{\"1\":{\"58\":1}}],[\"模型架构\",{\"0\":{\"52\":1}}],[\"模型发展调研\",{\"1\":{\"29\":1}}],[\"模型\",{\"1\":{\"25\":1,\"27\":4,\"60\":1,\"85\":1},\"2\":{\"57\":1,\"96\":1}}],[\"模型原有的参数是φ\",{\"1\":{\"21\":1}}],[\"模型的训练方法和数据集\",{\"0\":{\"87\":1}}],[\"模型的两种不同变体\",{\"1\":{\"85\":1}}],[\"模型的有损数据压缩能力\",{\"1\":{\"62\":1}}],[\"模型的优化表示为\",{\"1\":{\"21\":1}}],[\"模型的参数用\",{\"1\":{\"21\":1}}],[\"模型参数\",{\"1\":{\"18\":1,\"19\":1}}],[\"模版\",{\"1\":{\"7\":1}}],[\"rm就是基于第一步生成的sft6b版本\",{\"1\":{\"87\":1}}],[\"rm\",{\"1\":{\"87\":1,\"94\":1}}],[\"range\",{\"1\":{\"75\":1}}],[\"rank\",{\"1\":{\"20\":1,\"21\":1}}],[\"roformer\",{\"1\":{\"47\":1}}],[\"representations\",{\"1\":{\"89\":1}}],[\"reward\",{\"1\":{\"87\":1}}],[\"reinforcement\",{\"1\":{\"85\":1,\"86\":1}}],[\"regressive\",{\"1\":{\"63\":1}}],[\"recall\",{\"1\":{\"63\":1}}],[\"remove\",{\"0\":{\"44\":1},\"1\":{\"44\":1}}],[\"requires\",{\"1\":{\"33\":2}}],[\"research\",{\"1\":{\"7\":1}}],[\"r\",{\"1\":{\"21\":3,\"94\":18}}],[\"rlhf\",{\"1\":{\"7\":5,\"86\":1}}],[\"9个\",{\"1\":{\"87\":1}}],[\"99\",{\"1\":{\"78\":1}}],[\"98\",{\"1\":{\"33\":1}}],[\"95\",{\"1\":{\"33\":1}}],[\"9\",{\"1\":{\"7\":1,\"89\":1}}],[\"使得训练更加稳定\",{\"1\":{\"93\":1}}],[\"使得这些可以在同一个unifiedskg\",{\"1\":{\"7\":1}}],[\"使其可以忽略不计\",{\"1\":{\"46\":1}}],[\"使前缀调优模块化并节省空间\",{\"1\":{\"24\":1}}],[\"使用rm来更新ppo策略\",{\"1\":{\"87\":1}}],[\"使用人工标注prompt数据集的答案用来finetune模型\",{\"1\":{\"87\":1}}],[\"使用长范围训练方法的试验结果\",{\"1\":{\"80\":1}}],[\"使用编码器对完整输入进行块编码\",{\"1\":{\"78\":1}}],[\"使用编码后的数据进行数据传输\",{\"1\":{\"61\":1}}],[\"使用随机梯度下降训练这些参数\",{\"1\":{\"54\":1}}],[\"使用peft微调llms\",{\"1\":{\"29\":1}}],[\"使用bilstm对pi序列进行表征\",{\"1\":{\"27\":1}}],[\"使用llms生成prompt进行instruct\",{\"1\":{\"7\":1}}],[\"使用gpt3生成64k的instruction\",{\"1\":{\"7\":1}}],[\"使用\",{\"1\":{\"7\":2,\"46\":1}}],[\"做打破彼此任务之间的边界的第一次简单尝试\",{\"1\":{\"7\":1}}],[\"ouyang\",{\"1\":{\"89\":1}}],[\"output\",{\"1\":{\"7\":1}}],[\"online\",{\"1\":{\"89\":1}}],[\"only\",{\"1\":{\"88\":1}}],[\"only的gpt\",{\"1\":{\"24\":1}}],[\"on\",{\"1\":{\"86\":1,\"88\":1,\"89\":1}}],[\"own\",{\"1\":{\"63\":2}}],[\"optimization\",{\"1\":{\"93\":1}}],[\"optimizers\",{\"1\":{\"89\":1}}],[\"optimizing\",{\"1\":{\"20\":1,\"29\":1}}],[\"openreview\",{\"1\":{\"89\":1}}],[\"openai官网的chatgpt的训练流程和instructgpt基本一致\",{\"1\":{\"87\":1}}],[\"openai发布了模型索引为的davinci的初代gpt\",{\"1\":{\"85\":1}}],[\"openai\",{\"1\":{\"62\":1,\"86\":1},\"2\":{\"91\":1}}],[\"orleans\",{\"1\":{\"89\":1}}],[\"org\",{\"1\":{\"41\":1,\"75\":1}}],[\"orhonovich\",{\"1\":{\"7\":2}}],[\"offsets\",{\"1\":{\"44\":2}}],[\"of\",{\"1\":{\"20\":2,\"21\":1,\"63\":1,\"86\":1,\"89\":1}}],[\"问题提出\",{\"0\":{\"75\":1}}],[\"问题是为何模型压缩能力越强\",{\"1\":{\"62\":1}}],[\"问题\",{\"1\":{\"7\":1}}],[\"和人类的决策相似\",{\"1\":{\"88\":1}}],[\"和t\",{\"1\":{\"33\":1}}],[\"和连续提示\",{\"1\":{\"23\":1}}],[\"和规模在100m\",{\"1\":{\"7\":1}}],[\"和1600个nlp任务\",{\"1\":{\"7\":1}}],[\"和\",{\"1\":{\"7\":1,\"29\":1,\"42\":1,\"64\":1,\"75\":1,\"81\":1}}],[\"和法国\",{\"1\":{\"7\":1}}],[\"follow\",{\"1\":{\"86\":1,\"89\":1}}],[\"focus\",{\"1\":{\"86\":1}}],[\"for\",{\"1\":{\"20\":3,\"29\":1}}],[\"feedback\",{\"1\":{\"85\":1,\"86\":1,\"89\":1}}],[\"ffn\",{\"1\":{\"63\":1}}],[\"fused\",{\"1\":{\"45\":2}}],[\"fusion\",{\"1\":{\"42\":1}}],[\"faiss\",{\"1\":{\"77\":1}}],[\"factual\",{\"1\":{\"63\":1}}],[\"face开源的peft库目前支持5种方法\",{\"1\":{\"20\":1}}],[\"face开源的一个高效微调大模型的库\",{\"1\":{\"19\":1}}],[\"face\",{\"1\":{\"7\":2},\"2\":{\"31\":1}}],[\"fastertransformer\",{\"1\":{\"44\":1}}],[\"frac\",{\"1\":{\"94\":12}}],[\"framework下进行学习并在这些任务上取得不错的结果\",{\"1\":{\"7\":1}}],[\"from\",{\"1\":{\"85\":1,\"86\":1}}],[\"free\",{\"1\":{\"42\":2,\"44\":1,\"46\":1}}],[\"float16\",{\"1\":{\"33\":3}}],[\"flan\",{\"1\":{\"7\":5}}],[\"fn=<addbackward0>\",{\"1\":{\"33\":1}}],[\"finding\",{\"1\":{\"64\":1}}],[\"finetuned\",{\"1\":{\"89\":1}}],[\"finetune\",{\"2\":{\"39\":1}}],[\"finetuning更新所有参数的方式不同\",{\"1\":{\"24\":1}}],[\"fine\",{\"1\":{\"7\":1,\"19\":1,\"20\":2,\"86\":2}}],[\"filtering等概念\",{\"1\":{\"7\":1}}],[\"filtering\",{\"1\":{\"7\":1}}],[\"生成式问答中的开放域任务可以从更大的输入中综合信息\",{\"1\":{\"75\":1}}],[\"生成式摘要任务\",{\"1\":{\"24\":1}}],[\"生成几组就是几个头\",{\"1\":{\"45\":1}}],[\"生成任务\",{\"1\":{\"24\":1,\"27\":1}}],[\"生成\",{\"1\":{\"7\":1}}],[\"生成了\",{\"1\":{\"7\":1}}],[\"sim\",{\"1\":{\"94\":7}}],[\"size\",{\"1\":{\"44\":3,\"46\":6}}],[\"shuming\",{\"1\":{\"89\":1}}],[\"shot\",{\"1\":{\"67\":1,\"89\":1}}],[\"sum\",{\"1\":{\"94\":6}}],[\"sui\",{\"1\":{\"89\":1}}],[\"sun\",{\"1\":{\"89\":1}}],[\"supervised\",{\"1\":{\"85\":1,\"86\":1}}],[\"superposition\",{\"1\":{\"64\":4}}],[\"super\",{\"1\":{\"7\":5}}],[\"systems\",{\"1\":{\"89\":1}}],[\"sft阶段\",{\"1\":{\"87\":1}}],[\"sft\",{\"1\":{\"86\":1}}],[\"sled\",{\"1\":{\"75\":1}}],[\"specifically\",{\"1\":{\"86\":1}}],[\"specific向量添加到input前面\",{\"1\":{\"24\":1}}],[\"sparse\",{\"1\":{\"64\":1}}],[\"studies\",{\"1\":{\"64\":1}}],[\"state\",{\"1\":{\"21\":1}}],[\"stanford\",{\"1\":{\"7\":1}}],[\"seq2seq\",{\"1\":{\"75\":2,\"78\":2}}],[\"seqlen\",{\"1\":{\"44\":2,\"45\":3,\"46\":3}}],[\"self\",{\"1\":{\"7\":5,\"42\":1,\"54\":1}}],[\"softmax\",{\"1\":{\"45\":2}}],[\"soft\",{\"1\":{\"23\":1,\"26\":2,\"27\":2,\"29\":2}}],[\"svd\",{\"1\":{\"22\":1}}],[\"scale\",{\"1\":{\"20\":1}}],[\"scales\",{\"1\":{\"20\":1}}],[\"73\",{\"1\":{\"33\":1}}],[\"7\",{\"0\":{\"27\":1},\"1\":{\"7\":1,\"29\":1}}],[\"等库对数据存储中的编码输入进行索引\",{\"1\":{\"77\":1}}],[\"等强长程\",{\"1\":{\"75\":1}}],[\"等等\",{\"1\":{\"47\":1}}],[\"等操作\",{\"1\":{\"45\":1}}],[\"等\",{\"1\":{\"7\":1}}],[\"等概念被引入\",{\"1\":{\"7\":1}}],[\"等模型\",{\"1\":{\"7\":1}}],[\"等数据上进行微调的\",{\"1\":{\"7\":1}}],[\"等联合组织\",{\"1\":{\"7\":1}}],[\"中存在很多单个的神经元\",{\"1\":{\"64\":1}}],[\"中读取子问题参数进行了性能优化\",{\"1\":{\"46\":1}}],[\"中进行计算\",{\"1\":{\"46\":1}}],[\"中每个注意力头都会从全部输入中选择一个单独的上下文窗口\",{\"1\":{\"76\":1}}],[\"中每个\",{\"1\":{\"46\":1}}],[\"中的神经元被称为\",{\"1\":{\"64\":1}}],[\"中的分布\",{\"0\":{\"64\":1}}],[\"中的两次矩阵乘操作\",{\"1\":{\"46\":1}}],[\"中的\",{\"1\":{\"46\":1}}],[\"中完成多个独立矩阵乘问题的计算\",{\"1\":{\"46\":1}}],[\"中完成所有操作\",{\"1\":{\"45\":1}}],[\"中实现了融合的多头注意力\",{\"1\":{\"45\":1}}],[\"中也有集成\",{\"1\":{\"44\":1}}],[\"中\",{\"1\":{\"7\":1,\"45\":1,\"47\":1,\"74\":1,\"75\":1,\"78\":1}}],[\"论文中icl的测试数据\",{\"1\":{\"88\":1}}],[\"论文中显示\",{\"1\":{\"7\":1}}],[\"论文链接\",{\"1\":{\"75\":1}}],[\"论文\",{\"1\":{\"63\":1}}],[\"论文地址\",{\"1\":{\"41\":1}}],[\"论文信息\",{\"1\":{\"41\":1}}],[\"论文没有精确计算svd\",{\"1\":{\"22\":1}}],[\"论文提出了一套优化算法\",{\"1\":{\"41\":1}}],[\"论文提出了一种新的方法\",{\"1\":{\"22\":1}}],[\"论文提出了字节跳动的gpu\",{\"1\":{\"41\":1}}],[\"论文提出了两种重要性度量的方式\",{\"1\":{\"22\":1}}],[\"论文分享\",{\"0\":{\"3\":1},\"2\":{\"5\":1}}],[\"条件概率p使用具有参数θ的神经网络来建模\",{\"1\":{\"54\":1}}],[\"条\",{\"1\":{\"7\":1}}],[\"并将其作为一个外在约束\",{\"1\":{\"94\":1}}],[\"并将其存储在数据存储中\",{\"1\":{\"78\":1}}],[\"并将可训练的秩分解矩阵注入到transformer层的每个权重中\",{\"1\":{\"21\":1}}],[\"并只关注这前\",{\"1\":{\"78\":1}}],[\"并只训练这些\",{\"1\":{\"25\":1}}],[\"并仅对输入序列中的前\",{\"1\":{\"78\":1}}],[\"并关注前\",{\"1\":{\"75\":1}}],[\"并不是生成所需上下文长度的上限\",{\"1\":{\"75\":1}}],[\"并不单单靠\",{\"1\":{\"62\":1}}],[\"并不会在推理阶段加速\",{\"1\":{\"21\":1}}],[\"并把\",{\"1\":{\"45\":1}}],[\"并加入锚字符\",{\"1\":{\"27\":1}}],[\"并引入少量自然语言提示的锚字符\",{\"1\":{\"26\":1}}],[\"并且可以被注入到任何预训练的\",{\"1\":{\"78\":1}}],[\"并且可以在没有任何进一步训练的情况下改进现有的\",{\"1\":{\"75\":1}}],[\"并且上述各种优化手段也可以方便地应用到变种\",{\"1\":{\"47\":1}}],[\"并且实现了全面的\",{\"1\":{\"42\":1}}],[\"并且lora与全参数微调的差距不超过0\",{\"1\":{\"28\":1}}],[\"并且只优化prefix\",{\"1\":{\"24\":1}}],[\"并且无法更新参数\",{\"1\":{\"24\":1}}],[\"并且它不再局限于embedding层\",{\"1\":{\"21\":1}}],[\"并进行微调\",{\"1\":{\"22\":1}}],[\"并进行微调训练\",{\"1\":{\"20\":1}}],[\"并稳定了训练\",{\"1\":{\"22\":1}}],[\"并针对拥有自由参数的prefix部分进行微调训练\",{\"1\":{\"20\":1}}],[\"并使用同样的模型将\",{\"1\":{\"7\":1}}],[\"并采用了\",{\"1\":{\"7\":1}}],[\"领域较为活跃的一个方向\",{\"1\":{\"7\":1}}],[\"turbo和oasst两个模型的回答结果\",{\"1\":{\"62\":1}}],[\"tunning\",{\"1\":{\"29\":1}}],[\"tune\",{\"1\":{\"26\":1,\"86\":1}}],[\"tuning仅在transformer的\",{\"1\":{\"27\":1}}],[\"tuning应用于在nlu任务\",{\"1\":{\"26\":1}}],[\"tuning技术\",{\"1\":{\"26\":1}}],[\"tuning技术应用而生\",{\"1\":{\"26\":1}}],[\"tuning还提出了prompt\",{\"1\":{\"25\":1}}],[\"tuning给每个任务定义了自己的prompt\",{\"1\":{\"25\":1}}],[\"tuning是做生成任务\",{\"1\":{\"24\":1}}],[\"tuning的deep形式\",{\"1\":{\"27\":1}}],[\"tuning的简化\",{\"1\":{\"27\":1}}],[\"tuning的prompt拼接方式\",{\"1\":{\"24\":1}}],[\"tuning的作者提出了prefix\",{\"1\":{\"24\":1}}],[\"tuning的方法\",{\"1\":{\"7\":1,\"27\":1}}],[\"tuning将模板t中的pi\",{\"1\":{\"27\":1}}],[\"tuning将预训练参数固定\",{\"1\":{\"26\":1}}],[\"tuning将一系列连续的task\",{\"1\":{\"24\":1}}],[\"tuning将prompt对应的token替换为可训练的嵌入\",{\"1\":{\"20\":1}}],[\"tuning与full\",{\"1\":{\"24\":1}}],[\"tuning可理解为针对prompt部分的微调\",{\"1\":{\"20\":1}}],[\"tuning针对每一类任务\",{\"1\":{\"20\":1}}],[\"tuning在input前面加入prefix部分\",{\"1\":{\"20\":1}}],[\"tuning\",{\"0\":{\"24\":1,\"25\":1,\"26\":1},\"1\":{\"7\":3,\"19\":1,\"20\":11,\"24\":2,\"25\":2,\"26\":9,\"27\":5,\"29\":9,\"85\":2,\"86\":3},\"2\":{\"9\":2,\"31\":3,\"91\":1}}],[\"t5\",{\"1\":{\"47\":1}}],[\"that\",{\"1\":{\"88\":1}}],[\"thread\",{\"1\":{\"46\":1}}],[\"threadblock\",{\"1\":{\"46\":2}}],[\"theta$的优化梯度的误差\",{\"1\":{\"94\":1}}],[\"theta$的更新梯度了\",{\"1\":{\"94\":1}}],[\"theta$就越\",{\"1\":{\"94\":1}}],[\"theta$和$\",{\"1\":{\"94\":9}}],[\"theta$减小$\",{\"1\":{\"94\":1}}],[\"theta$增大$\",{\"1\":{\"94\":1}}],[\"theta$函数的实际意义是奖励关于完整路径$\",{\"1\":{\"94\":1}}],[\"theta$\",{\"1\":{\"94\":1}}],[\"theta$都要更新\",{\"1\":{\"94\":1}}],[\"theta^\",{\"1\":{\"94\":29}}],[\"theta+\",{\"1\":{\"94\":1}}],[\"theta\",{\"1\":{\"94\":30}}],[\"theta=e\",{\"1\":{\"94\":1}}],[\"the\",{\"1\":{\"20\":1,\"88\":1,\"89\":1}}],[\"tvm以及nvidia\",{\"1\":{\"42\":1}}],[\"trpo算法的公式如式4\",{\"1\":{\"94\":1}}],[\"trpo算法引入了kl散度\",{\"1\":{\"94\":1}}],[\"training\",{\"0\":{\"51\":1},\"1\":{\"89\":1}}],[\"transformers\",{\"1\":{\"75\":2}}],[\"transformer\",{\"0\":{\"47\":1,\"64\":1},\"1\":{\"42\":1,\"44\":1,\"47\":2,\"63\":7,\"64\":2,\"74\":1,\"75\":7,\"76\":1,\"78\":3},\"2\":{\"49\":1,\"83\":1}}],[\"transformer推理库\",{\"1\":{\"41\":1}}],[\"tree\",{\"1\":{\"7\":2}}],[\"t的对角元素\",{\"1\":{\"33\":1}}],[\"tensorcore\",{\"1\":{\"45\":1}}],[\"tensorrt等\",{\"1\":{\"42\":1}}],[\"tensor\",{\"1\":{\"33\":8}}],[\"text生成任务\",{\"1\":{\"24\":1}}],[\"text框架中加入knowledge\",{\"1\":{\"7\":1}}],[\"text\",{\"1\":{\"7\":3}}],[\"tau$出现的概率\",{\"1\":{\"94\":2}}],[\"tau$的数学期望\",{\"1\":{\"94\":1}}],[\"tau\",{\"1\":{\"94\":61}}],[\"takes\",{\"1\":{\"88\":1}}],[\"table\",{\"1\":{\"24\":1}}],[\"tasks\",{\"1\":{\"20\":1}}],[\"task\",{\"1\":{\"7\":1}}],[\"t0\",{\"1\":{\"7\":1}}],[\"toh\",{\"1\":{\"22\":1}}],[\"too\",{\"1\":{\"20\":1}}],[\"toolkits\",{\"1\":{\"7\":1}}],[\"to\",{\"1\":{\"7\":2,\"20\":1,\"24\":1,\"86\":3,\"89\":1}}],[\"tokens作为prefix\",{\"1\":{\"24\":1}}],[\"token\",{\"0\":{\"101\":1},\"1\":{\"4\":1,\"27\":2,\"60\":2,\"63\":2,\"75\":5,\"76\":2,\"78\":3,\"80\":2},\"2\":{\"102\":1,\"103\":1,\"104\":1}}],[\"6月发布的text\",{\"1\":{\"85\":1}}],[\"632795115\",{\"1\":{\"58\":1}}],[\"69\",{\"1\":{\"33\":1}}],[\"64k\",{\"1\":{\"7\":2}}],[\"6\",{\"0\":{\"26\":1},\"1\":{\"7\":1,\"29\":1,\"94\":2}}],[\"链接\",{\"1\":{\"7\":1}}],[\"是一个超参数$\",{\"1\":{\"94\":1}}],[\"是一种策略梯度优化算法\",{\"1\":{\"93\":1}}],[\"是之前3\",{\"1\":{\"94\":1}}],[\"是不影响等式的\",{\"1\":{\"94\":1}}],[\"是从类比中学习\",{\"1\":{\"88\":1}}],[\"是使用的基于人类反馈的强化学习的版本指令微调\",{\"1\":{\"85\":1}}],[\"是时下最强大的\",{\"1\":{\"75\":1}}],[\"是描述这个实体最后的\",{\"1\":{\"63\":1}}],[\"是学会了质数这种抽象概念的\",{\"1\":{\"62\":1}}],[\"是这个逻辑\",{\"1\":{\"62\":1}}],[\"是否意味着它具备越强的\",{\"1\":{\"61\":1}}],[\"是token嵌入矩阵\",{\"1\":{\"54\":1}}],[\"是token的上下文向量\",{\"1\":{\"54\":1}}],[\"是用于\",{\"1\":{\"24\":1}}],[\"是微软与佐治亚理工学院共同提出的一种微调优化方法\",{\"1\":{\"22\":1}}],[\"是微软的研究人员为了解决大语言模型微调而开发的一项技术\",{\"1\":{\"21\":1}}],[\"是在特定的一种任务类型上进行指令微调的尝试\",{\"1\":{\"7\":1}}],[\"是\",{\"1\":{\"7\":1}}],[\"是当下最大的开源\",{\"1\":{\"7\":1}}],[\"8\",{\"1\":{\"7\":2,\"33\":1}}],[\"测试集\",{\"1\":{\"7\":1}}],[\"数量大得多的\",{\"1\":{\"64\":1}}],[\"数量\",{\"1\":{\"7\":1}}],[\"数据存储可以存储在\",{\"1\":{\"75\":1}}],[\"数据无损压缩\",{\"1\":{\"62\":1}}],[\"数据内在规律的描述\",{\"1\":{\"62\":1}}],[\"数据与一些开源的\",{\"1\":{\"7\":1}}],[\"数据是\",{\"1\":{\"7\":1}}],[\"数据\",{\"1\":{\"7\":5}}],[\"数据集中\",{\"1\":{\"75\":1}}],[\"数据集token统计\",{\"1\":{\"75\":1}}],[\"数据集\",{\"0\":{\"10\":1},\"1\":{\"4\":1,\"7\":3},\"2\":{\"8\":1,\"11\":1,\"13\":1}}],[\"5系列进行训练\",{\"1\":{\"87\":1}}],[\"5系列已经训练完成\",{\"1\":{\"87\":1}}],[\"5系列的\",{\"1\":{\"87\":1}}],[\"50\",{\"1\":{\"75\":1}}],[\"512\",{\"1\":{\"75\":1}}],[\"53k\",{\"1\":{\"7\":1}}],[\"55k\",{\"1\":{\"7\":1}}],[\"5\",{\"0\":{\"25\":1,\"89\":1},\"1\":{\"7\":2,\"20\":1,\"29\":1,\"62\":2,\"67\":1,\"81\":1,\"94\":2}}],[\"包含超过\",{\"1\":{\"75\":1}}],[\"包含13个nlp任务\",{\"1\":{\"7\":1}}],[\"包含270个nlp任务的2000多个prompt模版\",{\"1\":{\"7\":1}}],[\"包含61个nlp任务\",{\"1\":{\"7\":1}}],[\"包含的语种个数不定\",{\"1\":{\"7\":1}}],[\"包含中文\",{\"1\":{\"7\":1}}],[\"扩展其\",{\"1\":{\"7\":1}}],[\"的部分对应了davinci\",{\"1\":{\"86\":1}}],[\"的论文\",{\"1\":{\"86\":1}}],[\"的提出来自于google的一篇论文\",{\"1\":{\"86\":1}}],[\"的提示\",{\"1\":{\"24\":2}}],[\"的训练方法中\",{\"1\":{\"80\":1}}],[\"的解码器关注编码器的最终隐状态\",{\"1\":{\"78\":1}}],[\"的输出的中间一半\",{\"1\":{\"77\":1}}],[\"的输入\",{\"1\":{\"75\":1}}],[\"的方法对输入的重叠块进行编码\",{\"1\":{\"77\":1}}],[\"的方式实现\",{\"1\":{\"45\":1}}],[\"的最大输入长度受到限制\",{\"1\":{\"76\":1}}],[\"的最后一个位置\",{\"1\":{\"63\":1}}],[\"的隐藏状态上构建一个数据存储\",{\"1\":{\"75\":1}}],[\"的上下文窗口长\",{\"1\":{\"75\":1}}],[\"的个上下文窗口\",{\"1\":{\"75\":1}}],[\"的信息\",{\"1\":{\"63\":1}}],[\"的信息集成到最后位置\",{\"1\":{\"63\":1}}],[\"的低层\",{\"1\":{\"63\":2}}],[\"的效果\",{\"1\":{\"62\":1}}],[\"的智能水准\",{\"1\":{\"62\":1}}],[\"的标准\",{\"1\":{\"47\":1}}],[\"的前缀和\",{\"1\":{\"44\":1}}],[\"的每一行四舍五入到整型之后最大值为127或者最小值为−127即可\",{\"1\":{\"33\":1}}],[\"的每一层之前都加入了soft\",{\"1\":{\"27\":1}}],[\"的每行乘以一个系数\",{\"1\":{\"33\":1}}],[\"的轻量微调\",{\"1\":{\"24\":1}}],[\"的情况下\",{\"1\":{\"21\":1}}],[\"的成本通常高得令人望而却步\",{\"1\":{\"18\":1}}],[\"的llms的rlhf数据集\",{\"1\":{\"7\":1}}],[\"的指令数据\",{\"1\":{\"7\":1}}],[\"的框架中\",{\"1\":{\"7\":1}}],[\"的框架中加入了\",{\"1\":{\"7\":1}}],[\"的生成流程\",{\"1\":{\"7\":1}}],[\"的思路\",{\"1\":{\"7\":1}}],[\"的\",{\"1\":{\"7\":2,\"42\":1,\"46\":1,\"63\":1,\"64\":1,\"80\":2}}],[\"的主要竞品之一\",{\"1\":{\"7\":1}}],[\"的基础上\",{\"1\":{\"7\":1}}],[\"的数据规模在\",{\"1\":{\"7\":1}}],[\"4k\",{\"1\":{\"80\":2}}],[\"4高\",{\"1\":{\"67\":1}}],[\"4314\",{\"1\":{\"33\":2}}],[\"4820\",{\"1\":{\"33\":1}}],[\"4548\",{\"1\":{\"33\":1}}],[\"4753\",{\"1\":{\"33\":1}}],[\"46种语言的多语言prompt数据\",{\"1\":{\"7\":1}}],[\"46\",{\"1\":{\"7\":1,\"89\":1}}],[\"4\",{\"0\":{\"24\":1,\"29\":1,\"88\":1},\"1\":{\"7\":1,\"20\":1,\"29\":1,\"33\":2,\"58\":2,\"80\":1,\"87\":1,\"94\":2}}],[\"构建了\",{\"1\":{\"7\":1}}],[\"英文全称low\",{\"1\":{\"21\":1}}],[\"英文\",{\"1\":{\"7\":1}}],[\"ppo裁剪实现的功能和kl惩罚一样\",{\"1\":{\"94\":1}}],[\"ppo裁剪\",{\"1\":{\"94\":1}}],[\"ppo的主要思想是\",{\"1\":{\"93\":1}}],[\"ppo\",{\"0\":{\"93\":1},\"1\":{\"93\":1}}],[\"ppo阶段\",{\"1\":{\"87\":1}}],[\"pdf\",{\"1\":{\"75\":2}}],[\"penalty\",{\"1\":{\"94\":1}}],[\"perform\",{\"1\":{\"89\":1}}],[\"pearl\",{\"0\":{\"67\":1},\"1\":{\"67\":2}}],[\"peft分类\",{\"0\":{\"20\":1}}],[\"peft能够将预训练的语言模型\",{\"1\":{\"19\":1}}],[\"peft定义\",{\"0\":{\"19\":1}}],[\"peft方法仅微调少量\",{\"1\":{\"18\":1,\"19\":1}}],[\"peft\",{\"0\":{\"18\":1},\"1\":{\"18\":2,\"19\":1},\"2\":{\"31\":1}}],[\"pamela\",{\"1\":{\"89\":1}}],[\"padding\",{\"0\":{\"44\":1},\"1\":{\"42\":3,\"44\":2,\"46\":1}}],[\"parameter\",{\"1\":{\"19\":1,\"20\":2}}],[\"pytorch\",{\"1\":{\"42\":1}}],[\"psedo\",{\"1\":{\"27\":1}}],[\"p2\",{\"1\":{\"26\":2}}],[\"p1\",{\"1\":{\"26\":2}}],[\"policy\",{\"1\":{\"93\":1}}],[\"polysemanticity\",{\"1\":{\"64\":1}}],[\"power\",{\"1\":{\"20\":1}}],[\"pool和quality\",{\"1\":{\"7\":1}}],[\"pool\",{\"1\":{\"7\":1}}],[\"p\",{\"0\":{\"26\":1},\"1\":{\"20\":4,\"26\":7,\"27\":3,\"29\":3,\"58\":1,\"94\":33},\"2\":{\"31\":1}}],[\"plm时\",{\"1\":{\"19\":1}}],[\"plm\",{\"1\":{\"18\":2,\"19\":1}}],[\"prime$分布差异过大的另一种方法\",{\"1\":{\"94\":1}}],[\"prime$分布的不相似度的值\",{\"1\":{\"94\":1}}],[\"prime$分布的差异程度\",{\"1\":{\"94\":2}}],[\"prime$更新的幅度太小\",{\"1\":{\"94\":1}}],[\"prime$的差距过大\",{\"1\":{\"94\":1}}],[\"prime$的不相似程度\",{\"1\":{\"94\":1}}],[\"prime$的kl散度\",{\"1\":{\"94\":1}}],[\"prime$的分布有差异会带来估算结果差异很大的问题\",{\"1\":{\"94\":1}}],[\"prime$采样来估算$\",{\"1\":{\"94\":1}}],[\"prime$采样来计算$\",{\"1\":{\"94\":1}}],[\"prime$采样的好坏程度\",{\"1\":{\"94\":1}}],[\"prime$采样一次之后\",{\"1\":{\"94\":1}}],[\"prime\",{\"1\":{\"94\":16}}],[\"primera\",{\"1\":{\"75\":1,\"81\":1}}],[\"press\",{\"1\":{\"89\":1}}],[\"prediction\",{\"1\":{\"60\":2}}],[\"pre\",{\"0\":{\"51\":1}}],[\"prefetch\",{\"1\":{\"46\":2}}],[\"prefix不是真实的\",{\"1\":{\"27\":1}}],[\"prefix参数进行微调\",{\"1\":{\"26\":1}}],[\"prefix为前缀\",{\"1\":{\"26\":1}}],[\"prefix只加在句首\",{\"1\":{\"24\":1}}],[\"prefix\",{\"0\":{\"24\":1},\"1\":{\"20\":4,\"24\":12,\"26\":3,\"27\":4,\"29\":3},\"2\":{\"31\":1}}],[\"proximal\",{\"1\":{\"93\":1}}],[\"processing\",{\"1\":{\"89\":1}}],[\"proceedings\",{\"1\":{\"89\":1}}],[\"probing\",{\"1\":{\"64\":1}}],[\"problem\",{\"1\":{\"46\":2}}],[\"projection\",{\"1\":{\"42\":1}}],[\"prompt范式第二阶段｜prefix\",{\"1\":{\"29\":1}}],[\"prompt综述\",{\"1\":{\"29\":1}}],[\"prompt比较依靠模型参数量\",{\"1\":{\"26\":1}}],[\"prompt是只作用在embedding层中\",{\"1\":{\"26\":1}}],[\"prompt的一种改进\",{\"1\":{\"26\":1}}],[\"prompt的制作分为手工创建prompt和自动化生成prompt\",{\"1\":{\"23\":1}}],[\"prompting最初由人工设计prompt\",{\"1\":{\"26\":1}}],[\"prompt两种\",{\"1\":{\"23\":1}}],[\"prompt与soft\",{\"1\":{\"23\":1}}],[\"prompt分为hard\",{\"1\":{\"23\":1}}],[\"prompt分类\",{\"0\":{\"23\":1}}],[\"prompts\",{\"1\":{\"20\":1,\"23\":1,\"29\":1}}],[\"promptsource\",{\"1\":{\"7\":5}}],[\"prompt数据\",{\"1\":{\"7\":1}}],[\"prompt\",{\"0\":{\"25\":1},\"1\":{\"7\":10,\"20\":5,\"23\":3,\"24\":1,\"25\":4,\"27\":7,\"29\":6},\"2\":{\"9\":1,\"31\":1,\"99\":1}}],[\"prakharguptaz\",{\"1\":{\"7\":3}}],[\"p3\",{\"1\":{\"7\":10,\"26\":1}}],[\"3论文\",{\"1\":{\"85\":1}}],[\"34\",{\"1\":{\"75\":1}}],[\"32\",{\"1\":{\"46\":2}}],[\"384\",{\"1\":{\"45\":1}}],[\"3652\",{\"1\":{\"33\":1}}],[\"35\",{\"1\":{\"89\":1}}],[\"3559\",{\"1\":{\"33\":1}}],[\"3591\",{\"1\":{\"33\":1}}],[\"3717\",{\"1\":{\"33\":1}}],[\"3\",{\"0\":{\"23\":1,\"28\":1,\"46\":1,\"47\":1,\"62\":1,\"64\":1,\"79\":1,\"80\":1,\"81\":1,\"87\":1},\"1\":{\"7\":2,\"20\":1,\"24\":1,\"29\":1,\"33\":4,\"62\":1,\"63\":1,\"86\":2,\"87\":1,\"88\":1,\"89\":1,\"94\":3}}],[\"任务\",{\"1\":{\"7\":1,\"60\":1}}],[\"任务的超过\",{\"1\":{\"7\":1}}],[\"任务构建\",{\"1\":{\"7\":1}}],[\"nabla\",{\"1\":{\"94\":9}}],[\"natural\",{\"1\":{\"7\":7}}],[\"november\",{\"1\":{\"89\":1}}],[\"noise\",{\"1\":{\"7\":1}}],[\"new\",{\"1\":{\"89\":1}}],[\"net\",{\"1\":{\"89\":1}}],[\"networks\",{\"1\":{\"64\":1}}],[\"neurips\",{\"1\":{\"89\":1}}],[\"neural\",{\"1\":{\"64\":1,\"89\":1}}],[\"neurons\",{\"1\":{\"64\":1}}],[\"next\",{\"1\":{\"60\":2,\"63\":1}}],[\"ntp\",{\"1\":{\"60\":2,\"62\":1,\"63\":1}}],[\"n是层数\",{\"1\":{\"54\":1}}],[\"num\",{\"1\":{\"46\":3}}],[\"nvidia\",{\"1\":{\"44\":1,\"46\":1}}],[\"nlg\",{\"1\":{\"24\":1}}],[\"nlp\",{\"1\":{\"7\":2}}],[\"n\",{\"1\":{\"21\":4,\"64\":2,\"94\":5}}],[\"个隐状态\",{\"1\":{\"78\":1}}],[\"个输入\",{\"1\":{\"75\":1}}],[\"个特征\",{\"1\":{\"64\":1}}],[\"个子问题参数\",{\"1\":{\"46\":1}}],[\"个独立的矩阵乘子问题\",{\"1\":{\"46\":1}}],[\"个不同的语言的版本\",{\"1\":{\"7\":1}}],[\"个\",{\"1\":{\"7\":3,\"75\":1,\"78\":3}}],[\"项目名称\",{\"1\":{\"7\":1}}],[\"项目主页\",{\"1\":{\"7\":1}}],[\"项目包含了\",{\"1\":{\"7\":1}}],[\"项目链接\",{\"1\":{\"7\":7}}],[\"截止目前\",{\"1\":{\"7\":1}}],[\"帮助研究者基于现有nlp\",{\"1\":{\"7\":1}}],[\"在理想情况\",{\"1\":{\"94\":1}}],[\"在加入重要性采样之后\",{\"1\":{\"94\":1}}],[\"在加入lora之前\",{\"1\":{\"21\":1}}],[\"在一次调用中教会它数学题\",{\"1\":{\"88\":1}}],[\"在一些场景下对能耗和时间的要求\",{\"1\":{\"32\":1}}],[\"在图\",{\"1\":{\"80\":1}}],[\"在计算和\",{\"1\":{\"78\":1}}],[\"在标准的交叉注意力机制中\",{\"1\":{\"78\":1}}],[\"在进入交叉注意力模块之前\",{\"1\":{\"76\":1}}],[\"在每个更新步骤中\",{\"1\":{\"93\":1}}],[\"在每个解码器层中的每个注意力头中选一组\",{\"1\":{\"76\":1}}],[\"在每个解码步骤中\",{\"1\":{\"76\":1}}],[\"在每层transformer\",{\"1\":{\"27\":1}}],[\"在解码过程中\",{\"1\":{\"76\":1}}],[\"在各种长程\",{\"1\":{\"75\":1}}],[\"在模型学习过程中\",{\"1\":{\"64\":1}}],[\"在它们的响应之上做个线性组合\",{\"1\":{\"64\":1}}],[\"在推理过程中\",{\"1\":{\"63\":1}}],[\"在提取这条知识的时候\",{\"1\":{\"63\":1}}],[\"在训练基座模型的时候\",{\"1\":{\"60\":1}}],[\"在训练损失中增加了额外的惩罚\",{\"1\":{\"22\":1}}],[\"在预训练之后\",{\"1\":{\"55\":1}}],[\"在作者的实验中\",{\"1\":{\"54\":1}}],[\"在实际场景中\",{\"1\":{\"42\":1}}],[\"在只训练1个epoch的情况下\",{\"1\":{\"28\":1}}],[\"在参数量超过10b的模型上\",{\"1\":{\"26\":1}}],[\"在没有加额外层的情况下\",{\"1\":{\"25\":1}}],[\"在transformer\",{\"1\":{\"27\":1}}],[\"在t5类的encoder\",{\"1\":{\"24\":1}}],[\"在text\",{\"1\":{\"7\":1}}],[\"在gpt类的自回归模型上采用\",{\"1\":{\"24\":1}}],[\"在下游微调时\",{\"1\":{\"24\":1}}],[\"在现有的矩阵近似文献中\",{\"1\":{\"22\":1}}],[\"在增量矩阵之间动态地分配参数预算\",{\"1\":{\"22\":1}}],[\"在adalora中\",{\"1\":{\"22\":1}}],[\"在albert中\",{\"1\":{\"21\":2}}],[\"在微调大型\",{\"1\":{\"19\":1}}],[\"在这个基础上\",{\"1\":{\"94\":1}}],[\"在这个项目中将自己的\",{\"1\":{\"7\":1}}],[\"在这些超长输入的情况下\",{\"1\":{\"75\":1}}],[\"在这方面\",{\"1\":{\"18\":1}}],[\"在特定的一种任务类型\",{\"1\":{\"7\":1}}],[\"在对话指令数据上微调后\",{\"1\":{\"7\":1}}],[\"在英语\",{\"1\":{\"7\":1}}],[\"在promptsource基础上\",{\"1\":{\"7\":1}}],[\"在\",{\"1\":{\"7\":4,\"21\":1,\"44\":1,\"63\":1,\"67\":1,\"75\":1,\"86\":1}}],[\"limits\",{\"1\":{\"94\":6}}],[\"li\",{\"1\":{\"89\":1}}],[\"liu\",{\"1\":{\"75\":1}}],[\"l\",{\"1\":{\"89\":1}}],[\"learn\",{\"1\":{\"89\":1}}],[\"learners\",{\"1\":{\"89\":1}}],[\"learning\",{\"1\":{\"85\":1,\"86\":1,\"88\":1,\"89\":1},\"2\":{\"91\":1}}],[\"lester\",{\"1\":{\"89\":1}}],[\"lewis\",{\"1\":{\"75\":1}}],[\"length\",{\"1\":{\"75\":1}}],[\"level\",{\"1\":{\"7\":1}}],[\"llm的信息压缩能力与其智能水平的关系\",{\"1\":{\"58\":1}}],[\"llm的信息压缩能力与知识存储方式分享\",{\"0\":{\"58\":1}}],[\"llm如何重映现实世界\",{\"0\":{\"58\":1}}],[\"llm\",{\"0\":{\"61\":1},\"1\":{\"25\":1,\"58\":2,\"60\":1,\"62\":5,\"64\":4},\"2\":{\"66\":1,\"69\":1,\"72\":1}}],[\"llms\",{\"1\":{\"7\":4}}],[\"lm的参数被冻结\",{\"1\":{\"24\":1}}],[\"language\",{\"0\":{\"51\":1},\"1\":{\"20\":1,\"21\":1,\"63\":1,\"86\":1,\"89\":3}}],[\"large\",{\"1\":{\"20\":1,\"21\":1}}],[\"lab\",{\"1\":{\"2\":1}}],[\"log\",{\"1\":{\"94\":5}}],[\"louisiana\",{\"1\":{\"89\":1}}],[\"long\",{\"1\":{\"75\":1,\"89\":1}}],[\"longformer\",{\"1\":{\"75\":3}}],[\"low\",{\"1\":{\"20\":1}}],[\"lora的微调质量与全模型微调相当\",{\"1\":{\"21\":1}}],[\"lora的做法是\",{\"1\":{\"21\":1}}],[\"lora新增的参数是δ\",{\"1\":{\"21\":1}}],[\"lora也是类似的思想\",{\"1\":{\"21\":1}}],[\"lora冻结预训练模型权重\",{\"1\":{\"21\":1}}],[\"lora\",{\"0\":{\"21\":1},\"1\":{\"20\":2,\"21\":1},\"2\":{\"31\":1}}],[\"guu\",{\"1\":{\"89\":1}}],[\"gpu\",{\"1\":{\"75\":1,\"78\":1}}],[\"gpt首先根据演示示例生成元梯度\",{\"1\":{\"88\":1}}],[\"gpt系列模型树\",{\"1\":{\"85\":1}}],[\"gpt系列模型发展历程\",{\"0\":{\"85\":1}}],[\"gpt模型对知识的提取归纳过程示意图\",{\"1\":{\"63\":1}}],[\"gpt对知识的提取与存储方式\",{\"1\":{\"58\":1}}],[\"gpt架构图\",{\"1\":{\"52\":1}}],[\"gpt论文分享\",{\"0\":{\"51\":1}}],[\"gpt\",{\"0\":{\"63\":1},\"1\":{\"7\":1,\"20\":1,\"24\":1,\"58\":1,\"62\":1,\"63\":3,\"86\":1,\"89\":1}}],[\"gpt3\",{\"1\":{\"7\":1,\"62\":1}}],[\"gets\",{\"1\":{\"94\":1}}],[\"gemm\",{\"0\":{\"46\":1},\"1\":{\"45\":3,\"46\":6}}],[\"generative\",{\"0\":{\"51\":1}}],[\"generation\",{\"1\":{\"20\":1,\"29\":1}}],[\"genci\",{\"1\":{\"7\":1}}],[\"google\",{\"1\":{\"7\":4},\"2\":{\"91\":1}}],[\"grouped\",{\"0\":{\"46\":1},\"1\":{\"45\":1,\"46\":6}}],[\"grounding\",{\"1\":{\"7\":2}}],[\"gradient\",{\"1\":{\"89\":1}}],[\"grad\",{\"1\":{\"33\":1}}],[\"grad=true\",{\"1\":{\"33\":2}}],[\"grained\",{\"1\":{\"7\":1}}],[\"github\",{\"1\":{\"7\":5,\"18\":1,\"41\":1,\"47\":1,\"67\":1}}],[\"i=1\",{\"1\":{\"94\":2}}],[\"implicitly\",{\"1\":{\"89\":1}}],[\"improving\",{\"0\":{\"51\":1}}],[\"iclr\",{\"1\":{\"89\":1}}],[\"icl只对attention有影响\",{\"1\":{\"88\":1}}],[\"icl只存在一次前向传播中\",{\"1\":{\"88\":1}}],[\"icl是一个元优化的过程\",{\"1\":{\"88\":1}}],[\"icl和微调的区别\",{\"1\":{\"88\":1}}],[\"icl\",{\"1\":{\"88\":2}}],[\"ivgi\",{\"1\":{\"75\":1,\"77\":2}}],[\"idris\",{\"1\":{\"7\":1}}],[\"information\",{\"1\":{\"89\":1}}],[\"input\",{\"1\":{\"75\":1}}],[\"in\",{\"1\":{\"63\":1,\"64\":2,\"88\":1,\"89\":4},\"2\":{\"91\":1}}],[\"international\",{\"1\":{\"89\":1}}],[\"intelligence\",{\"1\":{\"2\":1}}],[\"int8\",{\"1\":{\"33\":1}}],[\"int8量化技术原理讲解\",{\"0\":{\"32\":1}}],[\"int量化技术是一种节约大模型推理或训练的过程中占用的显存的技术\",{\"1\":{\"32\":1}}],[\"intrauct\",{\"1\":{\"7\":1}}],[\"instructgpt的训练数据构成\",{\"1\":{\"87\":1}}],[\"instructdial\",{\"1\":{\"7\":4}}],[\"instruct\",{\"1\":{\"7\":7,\"86\":1,\"87\":1},\"2\":{\"9\":1}}],[\"instructions\",{\"1\":{\"86\":1,\"89\":1}}],[\"instruction等\",{\"1\":{\"7\":1}}],[\"instruction\",{\"1\":{\"7\":13,\"85\":2,\"86\":2},\"2\":{\"91\":1}}],[\"clip\",{\"1\":{\"94\":2}}],[\"class\",{\"1\":{\"86\":1}}],[\"clark\",{\"1\":{\"76\":1}}],[\"claud\",{\"1\":{\"7\":1}}],[\"chunk\",{\"1\":{\"77\":1}}],[\"checkpoint\",{\"1\":{\"75\":1}}],[\"chatgpt使用了和text\",{\"1\":{\"87\":1}}],[\"chatgpt是如何工作的\",{\"1\":{\"87\":1}}],[\"chatgpt相关技术介绍\",{\"0\":{\"84\":1},\"2\":{\"92\":1}}],[\"chatgpt\",{\"1\":{\"7\":1,\"85\":1},\"2\":{\"91\":1}}],[\"cpu\",{\"1\":{\"75\":1}}],[\"cnn\",{\"1\":{\"75\":1}}],[\"cnrs\",{\"1\":{\"7\":1}}],[\"cta\",{\"1\":{\"46\":1}}],[\"cutlass\",{\"0\":{\"46\":1},\"1\":{\"45\":1}}],[\"cuda\",{\"1\":{\"33\":8,\"46\":1}}],[\"c\",{\"1\":{\"25\":1}}],[\"carroll\",{\"1\":{\"89\":1}}],[\"capacity\",{\"1\":{\"64\":1}}],[\"cases\",{\"1\":{\"94\":2}}],[\"case\",{\"1\":{\"64\":1}}],[\"cai等人\",{\"1\":{\"22\":1}}],[\"can\",{\"1\":{\"20\":1,\"89\":1}}],[\"conference\",{\"1\":{\"89\":1}}],[\"considering\",{\"1\":{\"88\":1}}],[\"context\",{\"1\":{\"88\":1,\"89\":1},\"2\":{\"91\":1}}],[\"continuous\",{\"1\":{\"20\":1,\"23\":1,\"29\":1}}],[\"codebase\",{\"1\":{\"67\":1}}],[\"collection\",{\"1\":{\"7\":2}}],[\"co\",{\"1\":{\"7\":3}}],[\"comparable\",{\"1\":{\"20\":1}}],[\"com\",{\"1\":{\"7\":6,\"18\":1,\"41\":1,\"58\":1}}],[\"由此可以证明icl只存在于一次前向传播\",{\"1\":{\"88\":1}}],[\"由此推测icl并不会被模型记住\",{\"1\":{\"88\":1}}],[\"由sft模型随机生成多个答案\",{\"1\":{\"87\":1}}],[\"由于编码器上下文窗口的大小是固定的\",{\"1\":{\"76\":1}}],[\"由于子问题的数量\",{\"1\":{\"46\":1}}],[\"由于变成了int8整型\",{\"1\":{\"33\":1}}],[\"由于在不太重要的权重矩阵添加更多的参数会产生很少的收益\",{\"1\":{\"22\":1}}],[\"由于不需要对模型的权重参数重新计算梯度\",{\"1\":{\"21\":1}}],[\"由\",{\"1\":{\"7\":1,\"26\":1}}],[\"hao\",{\"1\":{\"89\":1}}],[\"haystack\",{\"1\":{\"64\":1}}],[\"hard+soft\",{\"1\":{\"27\":1}}],[\"hard\",{\"1\":{\"23\":1,\"29\":1}}],[\"harmless\",{\"1\":{\"7\":1}}],[\"headed\",{\"1\":{\"54\":1}}],[\"head\",{\"1\":{\"45\":2,\"46\":4,\"63\":2}}],[\"hidden\",{\"1\":{\"44\":2}}],[\"hi\",{\"1\":{\"27\":1}}],[\"hku\",{\"1\":{\"7\":2}}],[\"hh\",{\"1\":{\"7\":5}}],[\"human\",{\"1\":{\"85\":1,\"86\":1,\"89\":1}}],[\"hub\",{\"1\":{\"7\":1}}],[\"huggingface\",{\"1\":{\"7\":3,\"18\":1}}],[\"hugging\",{\"1\":{\"7\":2,\"20\":1},\"2\":{\"31\":1}}],[\"hust\",{\"1\":{\"2\":1}}],[\"https\",{\"1\":{\"7\":9,\"18\":1,\"41\":2,\"58\":1,\"75\":1}}],[\"2和式2\",{\"1\":{\"94\":1}}],[\"2的$r\",{\"1\":{\"94\":1}}],[\"2所示\",{\"1\":{\"94\":1}}],[\"2换算成式2\",{\"1\":{\"94\":1}}],[\"28\",{\"1\":{\"89\":1}}],[\"29\",{\"1\":{\"89\":1}}],[\"25\",{\"1\":{\"89\":1}}],[\"2571\",{\"1\":{\"33\":1}}],[\"2305\",{\"1\":{\"75\":1}}],[\"2210\",{\"1\":{\"41\":1}}],[\"2612\",{\"1\":{\"33\":1}}],[\"2604\",{\"1\":{\"33\":1}}],[\"27744\",{\"1\":{\"89\":1}}],[\"27730\",{\"1\":{\"89\":1}}],[\"2720\",{\"1\":{\"33\":1}}],[\"270\",{\"1\":{\"7\":1}}],[\"240k\",{\"1\":{\"7\":1}}],[\"2019\",{\"1\":{\"76\":1,\"77\":2}}],[\"2018\",{\"1\":{\"75\":2}}],[\"2011\",{\"1\":{\"22\":1}}],[\"2010\",{\"1\":{\"22\":2}}],[\"2023\",{\"1\":{\"89\":1}}],[\"2023年3月\",{\"1\":{\"20\":1}}],[\"2020年7月\",{\"1\":{\"85\":1}}],[\"2020b\",{\"1\":{\"75\":1}}],[\"2020a\",{\"1\":{\"75\":1}}],[\"2022年5\",{\"1\":{\"85\":1}}],[\"2022年3月20\",{\"1\":{\"20\":1}}],[\"2022\",{\"1\":{\"75\":2,\"77\":2,\"86\":1,\"89\":6}}],[\"2021年9月\",{\"1\":{\"20\":1}}],[\"2021年3月18\",{\"1\":{\"20\":1}}],[\"2021年8月\",{\"1\":{\"20\":1}}],[\"2021年10月\",{\"1\":{\"20\":1}}],[\"2021数据与一些开源的instruction数据\",{\"1\":{\"7\":1}}],[\"2021\",{\"1\":{\"7\":2,\"75\":2}}],[\"2000\",{\"1\":{\"7\":1}}],[\"2\",{\"0\":{\"20\":1,\"21\":1,\"22\":2,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"27\":1,\"34\":1,\"43\":1,\"44\":1,\"45\":2,\"46\":1,\"53\":1,\"54\":1,\"55\":2,\"61\":1,\"63\":1,\"76\":1,\"77\":1,\"78\":2,\"81\":1,\"86\":1},\"1\":{\"7\":1,\"20\":1,\"22\":2,\"23\":1,\"24\":3,\"29\":1,\"33\":2,\"44\":1,\"45\":1,\"46\":2,\"55\":1,\"58\":1,\"62\":1,\"63\":1,\"64\":3,\"78\":1,\"86\":1,\"87\":1,\"89\":1,\"94\":5}}],[\"语言\",{\"1\":{\"7\":1}}],[\"语言说明的模型\",{\"1\":{\"7\":1}}],[\"语言模型\",{\"0\":{\"70\":1},\"1\":{\"4\":1},\"2\":{\"48\":1,\"56\":1,\"65\":1,\"68\":1,\"71\":1,\"73\":1,\"82\":1,\"90\":1,\"95\":1}}],[\"配备\",{\"1\":{\"7\":1}}],[\"1+\",{\"1\":{\"94\":1}}],[\"1中\",{\"1\":{\"94\":1}}],[\"1是严格相等的\",{\"1\":{\"94\":1}}],[\"1所示\",{\"1\":{\"94\":4}}],[\"1式给出的\",{\"1\":{\"94\":1}}],[\"1式\",{\"1\":{\"94\":1}}],[\"1的极大值\",{\"1\":{\"94\":1}}],[\"1为优化目标\",{\"1\":{\"94\":1}}],[\"16k\",{\"1\":{\"80\":2}}],[\"16384\",{\"1\":{\"75\":1}}],[\"161k\",{\"1\":{\"7\":1}}],[\"122\",{\"1\":{\"33\":1}}],[\"127\",{\"1\":{\"33\":2}}],[\"1237\",{\"1\":{\"33\":2}}],[\"10th\",{\"1\":{\"89\":1}}],[\"1024\",{\"1\":{\"75\":1}}],[\"10\",{\"1\":{\"7\":1,\"67\":1,\"75\":1}}],[\"100m\",{\"1\":{\"7\":1}}],[\"13\",{\"1\":{\"7\":1}}],[\"1b之间的p3数据集\",{\"1\":{\"7\":1}}],[\"1b\",{\"1\":{\"7\":1}}],[\"1\",{\"0\":{\"7\":1,\"19\":1,\"21\":1,\"33\":1,\"42\":1,\"44\":1,\"52\":1,\"54\":1,\"59\":1,\"60\":2,\"61\":1,\"62\":1,\"75\":1,\"77\":1,\"80\":1,\"85\":1,\"94\":1},\"1\":{\"7\":1,\"20\":1,\"22\":2,\"23\":1,\"24\":2,\"28\":1,\"29\":1,\"33\":2,\"44\":1,\"45\":1,\"46\":3,\"52\":1,\"55\":1,\"58\":1,\"62\":1,\"63\":1,\"64\":3,\"75\":1,\"86\":1,\"87\":1,\"89\":2,\"94\":14}}],[\"本文的方法也比处理所有输入\",{\"1\":{\"78\":1}}],[\"本文不是只关注输入的这前\",{\"1\":{\"78\":1}}],[\"本文使用\",{\"1\":{\"77\":1}}],[\"本文按照\",{\"1\":{\"77\":1}}],[\"本文证明\",{\"1\":{\"75\":1}}],[\"本文主要分享的内容为以下两点\",{\"1\":{\"58\":1}}],[\"本文针对一些质量较高的指令数据集和提示数据集\",{\"1\":{\"6\":1}}],[\"本页面包含一些论文分享的分类\",{\"1\":{\"4\":1}}],[\"尤其是在模型与人类认识对齐方面\",{\"1\":{\"6\":1}}],[\"评估方法\",{\"0\":{\"14\":1},\"1\":{\"4\":1},\"2\":{\"15\":1,\"17\":1}}],[\"as\",{\"1\":{\"89\":1}}],[\"associations\",{\"1\":{\"63\":1}}],[\"april\",{\"1\":{\"89\":1}}],[\"approx\",{\"1\":{\"94\":3}}],[\"approaches\",{\"1\":{\"86\":1}}],[\"apple\",{\"1\":{\"63\":1}}],[\"auto\",{\"1\":{\"63\":1}}],[\"agi\",{\"1\":{\"61\":1}}],[\"advances\",{\"1\":{\"89\":1}}],[\"add\",{\"1\":{\"45\":1}}],[\"adams\",{\"1\":{\"89\":1}}],[\"adaptive\",{\"1\":{\"20\":1,\"94\":1}}],[\"adaptation\",{\"1\":{\"20\":1,\"21\":1}}],[\"adalora提出了一种新的重要性度量\",{\"1\":{\"22\":1}}],[\"adalora将增量矩阵p∧q划分为三元组\",{\"1\":{\"22\":1}}],[\"adalora通过重要性评分动态调整∆=p\",{\"1\":{\"22\":1}}],[\"adalora根据重要性评分自适应地分配参数预算\",{\"1\":{\"22\":1}}],[\"adalora包含两个重要组成部分\",{\"1\":{\"22\":1}}],[\"adalora调整增量矩阵的秩\",{\"1\":{\"22\":1}}],[\"adalora\",{\"0\":{\"22\":1},\"1\":{\"20\":1,\"22\":2},\"2\":{\"31\":1}}],[\"aml\",{\"1\":{\"42\":1,\"44\":1,\"47\":1}}],[\"attention操作\",{\"1\":{\"54\":1}}],[\"attention\",{\"1\":{\"42\":1,\"44\":2,\"45\":2,\"46\":3,\"63\":8,\"88\":1}}],[\"abs\",{\"1\":{\"41\":1}}],[\"are\",{\"1\":{\"89\":1}}],[\"arxiv\",{\"1\":{\"41\":1,\"75\":1,\"89\":1}}],[\"artificial\",{\"1\":{\"2\":1}}],[\"a\",{\"1\":{\"25\":1,\"64\":1,\"86\":1,\"94\":5}}],[\"a2\",{\"1\":{\"25\":1}}],[\"a1\",{\"1\":{\"25\":1}}],[\"across\",{\"1\":{\"20\":1}}],[\"aligned\",{\"1\":{\"94\":4}}],[\"aligning\",{\"1\":{\"86\":1}}],[\"almeida\",{\"1\":{\"89\":1}}],[\"al\",{\"1\":{\"75\":8,\"76\":1,\"77\":4,\"89\":3}}],[\"albert直接用两个小矩阵替换了原来的大矩阵\",{\"1\":{\"21\":1}}],[\"allocation\",{\"1\":{\"20\":1}}],[\"allen\",{\"1\":{\"7\":2}}],[\"alpaca\",{\"1\":{\"7\":1}}],[\"anchor\",{\"1\":{\"26\":1,\"27\":1}}],[\"anthropics\",{\"1\":{\"7\":1}}],[\"anthropic\",{\"1\":{\"7\":5}}],[\"and\",{\"1\":{\"2\":1,\"7\":1,\"20\":1,\"64\":1,\"88\":1}}],[\"ai\",{\"1\":{\"7\":2}}]],\"serializationVersion\":2},\"/en/\":{\"documentCount\":114,\"nextId\":114,\"documentIds\":{\"0\":\"v-2d0a870d\",\"1\":\"v-2d0a870d@2\",\"2\":\"v-5aa3d8ba\",\"3\":\"v-367b840a\",\"4\":\"v-367b840a@2\",\"5\":\"v-395cd082\",\"6\":\"v-395cd082#catalog\",\"7\":\"v-395cd082@0\",\"8\":\"v-395cd082@2\",\"9\":\"v-70eda030\",\"10\":\"v-70eda030@0\",\"11\":\"v-70eda030@1\",\"12\":\"v-70eda030@2\",\"13\":\"v-3777b6d3\",\"14\":\"v-3777b6d3@0\",\"15\":\"v-3777b6d3@1\",\"16\":\"v-4a2a37eb\",\"17\":\"v-4a2a37eb#markdown-introduction\",\"18\":\"v-4a2a37eb#markdown-config\",\"19\":\"v-4a2a37eb#markdown-extension\",\"20\":\"v-4a2a37eb#vuepress-enhancement\",\"21\":\"v-4a2a37eb#theme-enhancement\",\"22\":\"v-4a2a37eb#custom-container\",\"23\":\"v-4a2a37eb#tabs\",\"24\":\"v-4a2a37eb#code-tabs\",\"25\":\"v-4a2a37eb#superscript-and-subscript\",\"26\":\"v-4a2a37eb#align\",\"27\":\"v-4a2a37eb#attrs\",\"28\":\"v-4a2a37eb#footnote\",\"29\":\"v-4a2a37eb#mark\",\"30\":\"v-4a2a37eb#tasklist\",\"31\":\"v-4a2a37eb#image-enhancement\",\"32\":\"v-4a2a37eb#card\",\"33\":\"v-4a2a37eb#chart\",\"34\":\"v-4a2a37eb#echarts\",\"35\":\"v-4a2a37eb#flowchart\",\"36\":\"v-4a2a37eb#mermaid\",\"37\":\"v-4a2a37eb#tex\",\"38\":\"v-4a2a37eb#include-files\",\"39\":\"v-4a2a37eb#code-demo\",\"40\":\"v-4a2a37eb#stylize\",\"41\":\"v-4a2a37eb#playground\",\"42\":\"v-4a2a37eb#vue-playground\",\"43\":\"v-4a2a37eb#presentation\",\"44\":\"v-4a2a37eb@0\",\"45\":\"v-4a2a37eb@1\",\"46\":\"v-4a2a37eb@2\",\"47\":\"v-0e4acecb\",\"48\":\"v-0e4acecb#page-information\",\"49\":\"v-0e4acecb#page-content\",\"50\":\"v-0e4acecb#page-structure\",\"51\":\"v-0e4acecb@0\",\"52\":\"v-0e4acecb@1\",\"53\":\"v-0e4acecb@2\",\"54\":\"v-fb852992\",\"55\":\"v-fb852992#heading-2\",\"56\":\"v-fb852992#heading-3\",\"57\":\"v-fb852992@0\",\"58\":\"v-fb852992@1\",\"59\":\"v-4fd051a1\",\"60\":\"v-4fd051a1#heading-2\",\"61\":\"v-4fd051a1#heading-3\",\"62\":\"v-4fd051a1@0\",\"63\":\"v-4fd051a1@1\",\"64\":\"v-57615dc1\",\"65\":\"v-57615dc1#heading-2\",\"66\":\"v-57615dc1#heading-3\",\"67\":\"v-57615dc1@0\",\"68\":\"v-57615dc1@1\",\"69\":\"v-285adf66\",\"70\":\"v-285adf66#heading-2\",\"71\":\"v-285adf66#heading-3\",\"72\":\"v-285adf66@0\",\"73\":\"v-285adf66@1\",\"74\":\"v-58aa03b4\",\"75\":\"v-58aa03b4#heading-2\",\"76\":\"v-58aa03b4#heading-3\",\"77\":\"v-58aa03b4@0\",\"78\":\"v-58aa03b4@1\",\"79\":\"v-55405276\",\"80\":\"v-55405276#heading-2\",\"81\":\"v-55405276#heading-3\",\"82\":\"v-55405276@0\",\"83\":\"v-55405276@1\",\"84\":\"v-51d6a138\",\"85\":\"v-51d6a138#heading-2\",\"86\":\"v-51d6a138#heading-3\",\"87\":\"v-51d6a138@0\",\"88\":\"v-51d6a138@1\",\"89\":\"v-4e6ceffa\",\"90\":\"v-4e6ceffa#heading-2\",\"91\":\"v-4e6ceffa#heading-3\",\"92\":\"v-4e6ceffa@0\",\"93\":\"v-4e6ceffa@1\",\"94\":\"v-e748286e\",\"95\":\"v-e748286e#heading-2\",\"96\":\"v-e748286e#heading-3\",\"97\":\"v-e748286e@0\",\"98\":\"v-e748286e@1\",\"99\":\"v-e3de7730\",\"100\":\"v-e3de7730#heading-2\",\"101\":\"v-e3de7730#heading-3\",\"102\":\"v-e3de7730@0\",\"103\":\"v-e3de7730@1\",\"104\":\"v-e074c5f2\",\"105\":\"v-e074c5f2#heading-2\",\"106\":\"v-e074c5f2#heading-3\",\"107\":\"v-e074c5f2@0\",\"108\":\"v-e074c5f2@1\",\"109\":\"v-dd0b14b4\",\"110\":\"v-dd0b14b4#heading-2\",\"111\":\"v-dd0b14b4#heading-3\",\"112\":\"v-dd0b14b4@0\",\"113\":\"v-dd0b14b4@1\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[2,30],\"1\":[null,null,2],\"2\":[2,7],\"3\":[2],\"4\":[null,null,2],\"5\":[2],\"6\":[1,8],\"7\":[null,null,1],\"8\":[null,null,2],\"9\":[4,40],\"10\":[null,null,1],\"11\":[null,null,1],\"12\":[null,null,4],\"13\":[2,10],\"14\":[null,null,1],\"15\":[null,null,1],\"16\":[2,32],\"17\":[2,19],\"18\":[2,26],\"19\":[2,18],\"20\":[2,16],\"21\":[2,18],\"22\":[2,24],\"23\":[1,2],\"24\":[2,2],\"25\":[3,4],\"26\":[1,7],\"27\":[1,6],\"28\":[1,7],\"29\":[1,7],\"30\":[1,6],\"31\":[2,8],\"32\":[1,24],\"33\":[1,2],\"34\":[1,2],\"35\":[1,2],\"36\":[1,2],\"37\":[1,11],\"38\":[2,10],\"39\":[2,2],\"40\":[1,9],\"41\":[1,2],\"42\":[2,2],\"43\":[1,7],\"44\":[null,null,1],\"45\":[null,null,1],\"46\":[null,null,2],\"47\":[2,10],\"48\":[2,27],\"49\":[2,49],\"50\":[2,34],\"51\":[null,null,1],\"52\":[null,null,3],\"53\":[null,null,2],\"54\":[1],\"55\":[2,5],\"56\":[2,5],\"57\":[null,null,1],\"58\":[null,null,3],\"59\":[2],\"60\":[2,5],\"61\":[2,5],\"62\":[null,null,2],\"63\":[null,null,2],\"64\":[1],\"65\":[2,5],\"66\":[2,5],\"67\":[null,null,2],\"68\":[null,null,2],\"69\":[1],\"70\":[2,5],\"71\":[2,5],\"72\":[null,null,1],\"73\":[null,null,2],\"74\":[2],\"75\":[2,5],\"76\":[2,5],\"77\":[null,null,1],\"78\":[null,null,3],\"79\":[2,6],\"80\":[2,5],\"81\":[2,5],\"82\":[null,null,1],\"83\":[null,null,3],\"84\":[2],\"85\":[2,5],\"86\":[2,5],\"87\":[null,null,2],\"88\":[null,null,3],\"89\":[2],\"90\":[2,5],\"91\":[2,5],\"92\":[null,null,2],\"93\":[null,null,3],\"94\":[2],\"95\":[2,5],\"96\":[2,5],\"97\":[null,null,2],\"98\":[null,null,3],\"99\":[2,9],\"100\":[2,5],\"101\":[2,5],\"102\":[null,null,2],\"103\":[null,null,3],\"104\":[2],\"105\":[2,5],\"106\":[2,5],\"107\":[null,null,1],\"108\":[null,null,3],\"109\":[2],\"110\":[2,5],\"111\":[2,5],\"112\":[null,null,1],\"113\":[null,null,3]},\"averageFieldLength\":[1.7944673065512011,12.516996012689509,1.589501752224544],\"storedFields\":{\"0\":{\"h\":\"Blog Home\",\"t\":[\"This is a blog home page demo.\",\"To use this layout, you should set both layout: BlogHome and home: true in the page front matter.\",\"For related configuration docs, please see blog homepage.\"]},\"1\":{\"c\":[\"Blog Home\"]},\"2\":{\"h\":\"Intro Page\",\"t\":[\"Place your introduction and profile here.\"]},\"3\":{\"h\":\"Slide page\"},\"4\":{\"c\":[\"Slide page\"]},\"5\":{\"h\":\"Features demo\"},\"6\":{\"h\":\"Catalog\",\"t\":[\"Markdown Enhance\",\"Page Config\",\"Function Disable\",\"Encryption Demo\"]},\"7\":{\"c\":[\"Guide\"]},\"8\":{\"c\":[\"Features demo\"]},\"9\":{\"h\":\"Disabling layout and features\",\"t\":[\"You can disable some function and layout on the page by setting the Frontmatter of the page.\",\"This page is an demo that disables the following features:\",\"Navbar\",\"Sidebar\",\"Breadcrumb\",\"Page information\",\"Contributors\",\"Edit link\",\"Update time\",\"Prev/Next link\",\"Comment\",\"Footer\",\"Back to top button\"]},\"10\":{\"c\":[\"Guide\"]},\"11\":{\"c\":[\"disable\"]},\"12\":{\"c\":[\"Disabling layout and features\"]},\"13\":{\"h\":\"Encryption Article\",\"t\":[\"The actual article content.\",\"Paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text paragraph 1 text.\",\"Paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text paragraph 2 text.\"]},\"14\":{\"c\":[\"Guide\"]},\"15\":{\"c\":[\"encryption\"]},\"16\":{\"h\":\"Markdown Enhance\",\"t\":[\"VuePress basically generate pages from Markdown files. So you can use it to generate documentation or blog sites easily.\",\"You should create and write Markdown files, so that VuePress can convert them to different pages according to file structure.\"]},\"17\":{\"h\":\"Markdown Introduction\",\"t\":[\"If you are a new learner and don't know how to write Markdown, please read Markdown Intro and Markdown Demo.\"]},\"18\":{\"h\":\"Markdown Config\",\"t\":[\"VuePress introduce configuration for each markdown page using Frontmatter.\",\"Info\",\"Frontmatter is a important concept in VuePress. If you don't know it, you need to read Frontmatter Introduction.\"]},\"19\":{\"h\":\"Markdown Extension\",\"t\":[\"The Markdown content in VuePress will be parsed by markdown-it, which supports syntax extensions via markdown-it plugins.\"]},\"20\":{\"h\":\"VuePress Enhancement\",\"t\":[\"To enrich document writing, VuePress has extended Markdown syntax.\",\"For these extensions, please read Markdown extensions in VuePress.\"]},\"21\":{\"h\":\"Theme Enhancement\",\"t\":[\"By using vuepress-plugin-md-enhance, the theme extends more Markdown syntax and provides richer writing functions.\"]},\"22\":{\"h\":\"Custom Container\",\"t\":[\"Safely use {{ variable }} in Markdown.\",\"Custom Title\",\"A custom information container with code, link.\",\"const a = 1; \",\"Custom Title\",\"A custom tip container\",\"Custom Title\",\"A custom warning container\",\"Custom Title\",\"A custom danger container\",\"Custom Title\",\"A custom details container\",\"View Detail\"]},\"23\":{\"h\":\"Tabs\",\"t\":[\"View Detail\"]},\"24\":{\"h\":\"Code Tabs\",\"t\":[\"View Detail\"]},\"25\":{\"h\":\"Superscript and Subscript\",\"t\":[\"19th H2O\",\"View Detail\"]},\"26\":{\"h\":\"Align\",\"t\":[\"I am center\",\"I am right align\",\"View Detail\"]},\"27\":{\"h\":\"Attrs\",\"t\":[\"A word having id.\",\"View Detail\"]},\"28\":{\"h\":\"Footnote\",\"t\":[\"This text has footnote[1].\",\"View Detail\"]},\"29\":{\"h\":\"Mark\",\"t\":[\"You can mark important words .\",\"View Detail\"]},\"30\":{\"h\":\"Tasklist\",\"t\":[\" Plan A\",\" Plan B\",\"View Detail\"]},\"31\":{\"h\":\"Image Enhancement\",\"t\":[\"Support setting color scheme and size\",\"View Detail\"]},\"32\":{\"h\":\"Card\",\"t\":[\"title: Mr.Hope desc: Where there is light, there is hope logo: https://mrhope.site/logo.svg link: https://mrhope.site color: rgba(253, 230, 138, 0.15) \",\"View Detail\"]},\"33\":{\"h\":\"Chart\",\"t\":[\"View Detail\"]},\"34\":{\"h\":\"Echarts\",\"t\":[\"View Detail\"]},\"35\":{\"h\":\"Flowchart\",\"t\":[\"View Detail\"]},\"36\":{\"h\":\"Mermaid\",\"t\":[\"View Detail\"]},\"37\":{\"h\":\"Tex\",\"t\":[\"∂ωr∂r​(ωyω​)=(ωyω​){(logy)r+i=1∑r​ωi(−1)ir⋯(r−i+1)(logy)r−i​}\",\"View Detail\"]},\"38\":{\"h\":\"Include files\",\"t\":[\"Markdown Enhance\",\"Page Config\",\"Function Disable\",\"Encryption Demo\",\"View Detail\"]},\"39\":{\"h\":\"Code Demo\",\"t\":[\"View Detail\"]},\"40\":{\"h\":\"Stylize\",\"t\":[\"Donate Mr.Hope a cup of coffee. \",\"View Detail\"]},\"41\":{\"h\":\"Playground\",\"t\":[\"View Detail\"]},\"42\":{\"h\":\"Vue Playground\",\"t\":[\"View Detail\"]},\"43\":{\"h\":\"Presentation\",\"t\":[\"View Detail\",\"This is footnote content ↩︎\"]},\"44\":{\"c\":[\"Guide\"]},\"45\":{\"c\":[\"Markdown\"]},\"46\":{\"c\":[\"Markdown Enhance\"]},\"47\":{\"h\":\"Page Config\",\"t\":[\"Content before more comment is regarded as page excerpt.\"]},\"48\":{\"h\":\"Page Information\",\"t\":[\"You can set page information in Markdown's Frontmatter.\",\"The author is Ms.Hope.\",\"The writing date is January 1, 2020\",\"Category is \\\"Guide\\\"\",\"Tags are \\\"Page Config\\\" and \\\"Guide\\\"\"]},\"49\":{\"h\":\"Page Content\",\"t\":[\"You are free to write your Markdown here.\",\"Assets\",\"You can place images besides your Markdown files, but you should use relative links (i.e.: starting with ./) for them.\",\"For images in .vuepress/public directory, please use absolute links (i.e.: starting with /) for them.\",\"The theme contains a custom badge:\",\"A dark blue badge text badge at the end of line. \"]},\"50\":{\"h\":\"Page Structure\",\"t\":[\"This page should contain:\",\"BreadCrumb\",\"Title and information\",\"TOC (Table of Contents)\",\"Meta information including update time and contributors\",\"Comments\",\"Navbar\",\"Sidebar\",\"Footer\",\"Back to top button\",\"You can customize them in theme options and page frontmatter.\"]},\"51\":{\"c\":[\"Guide\"]},\"52\":{\"c\":[\"Page config\",\"Guide\"]},\"53\":{\"c\":[\"Page Config\"]},\"54\":{\"h\":\"Cherry\"},\"55\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"56\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"57\":{\"c\":[\"Cherry\"]},\"58\":{\"c\":[\"red\",\"small\",\"round\"]},\"59\":{\"h\":\"Dragon Fruit\"},\"60\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"61\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"62\":{\"c\":[\"Dragon Fruit\",\"Fruit\"]},\"63\":{\"c\":[\"red\",\"big\"]},\"64\":{\"h\":\"Strawberry\"},\"65\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"66\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"67\":{\"c\":[\"Fruit\",\"Strawberry\"]},\"68\":{\"c\":[\"red\",\"small\"]},\"69\":{\"h\":\"Tomato\"},\"70\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"71\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"72\":{\"c\":[\"Vegetable\"]},\"73\":{\"c\":[\"red\",\"round\"]},\"74\":{\"h\":\"Apple 1\"},\"75\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"76\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"77\":{\"c\":[\"Apple\"]},\"78\":{\"c\":[\"red\",\"big\",\"round\"]},\"79\":{\"h\":\"Apple 2\",\"t\":[\"A apple article being stared.\"]},\"80\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"81\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"82\":{\"c\":[\"Apple\"]},\"83\":{\"c\":[\"red\",\"big\",\"round\"]},\"84\":{\"h\":\"Apple 3\"},\"85\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"86\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"87\":{\"c\":[\"Apple\",\"Fruit\"]},\"88\":{\"c\":[\"red\",\"big\",\"round\"]},\"89\":{\"h\":\"Apple 4\"},\"90\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"91\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"92\":{\"c\":[\"Apple\",\"Fruit\"]},\"93\":{\"c\":[\"red\",\"big\",\"round\"]},\"94\":{\"h\":\"Banana 1\"},\"95\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"96\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"97\":{\"c\":[\"Banana\",\"Fruit\"]},\"98\":{\"c\":[\"yellow\",\"curly\",\"long\"]},\"99\":{\"h\":\"Banana 2\",\"t\":[\"A banana article being stared with number 10.\"]},\"100\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"101\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"102\":{\"c\":[\"Banana\",\"Fruit\"]},\"103\":{\"c\":[\"yellow\",\"curly\",\"long\"]},\"104\":{\"h\":\"Banana 3\"},\"105\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"106\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"107\":{\"c\":[\"Banana\"]},\"108\":{\"c\":[\"yellow\",\"curly\",\"long\"]},\"109\":{\"h\":\"Banana 4\"},\"110\":{\"h\":\"Heading 2\",\"t\":[\"Here is the content.\"]},\"111\":{\"h\":\"Heading 3\",\"t\":[\"Here is the content.\"]},\"112\":{\"c\":[\"Banana\"]},\"113\":{\"c\":[\"yellow\",\"curly\",\"long\"]}},\"dirtCount\":0,\"index\":[[\"yellow\",{\"2\":{\"98\":1,\"103\":1,\"108\":1,\"113\":1}}],[\"your\",{\"1\":{\"2\":1,\"49\":2}}],[\"you\",{\"1\":{\"0\":1,\"9\":1,\"16\":2,\"17\":1,\"18\":2,\"29\":1,\"48\":1,\"49\":3,\"50\":1}}],[\"4\",{\"0\":{\"89\":1,\"109\":1}}],[\"3\",{\"0\":{\"56\":1,\"61\":1,\"66\":1,\"71\":1,\"76\":1,\"81\":1,\"84\":1,\"86\":1,\"91\":1,\"96\":1,\"101\":1,\"104\":1,\"106\":1,\"111\":1}}],[\"january\",{\"1\":{\"48\":1}}],[\"↩︎\",{\"1\":{\"43\":1}}],[\"−1\",{\"1\":{\"37\":1}}],[\"ωyω​\",{\"1\":{\"37\":2}}],[\"∂ωr∂r​\",{\"1\":{\"37\":1}}],[\"0\",{\"1\":{\"32\":1}}],[\"=\",{\"1\":{\"22\":1,\"37\":1}}],[\"round\",{\"2\":{\"58\":1,\"73\":1,\"78\":1,\"83\":1,\"88\":1,\"93\":1}}],[\"r−i​\",{\"1\":{\"37\":1}}],[\"r−i+1\",{\"1\":{\"37\":1}}],[\"r+i=1∑r​ωi\",{\"1\":{\"37\":1}}],[\"rgba\",{\"1\":{\"32\":1}}],[\"right\",{\"1\":{\"26\":1}}],[\"richer\",{\"1\":{\"21\":1}}],[\"red\",{\"2\":{\"58\":1,\"63\":1,\"68\":1,\"73\":1,\"78\":1,\"83\":1,\"88\":1,\"93\":1}}],[\"relative\",{\"1\":{\"49\":1}}],[\"related\",{\"1\":{\"0\":1}}],[\"regarded\",{\"1\":{\"47\":1}}],[\"read\",{\"1\":{\"17\":1,\"18\":1,\"20\":1}}],[\"meta\",{\"1\":{\"50\":1}}],[\"mermaid\",{\"0\":{\"36\":1}}],[\"ms\",{\"1\":{\"48\":1}}],[\"mrhope\",{\"1\":{\"32\":2}}],[\"mr\",{\"1\":{\"32\":1,\"40\":1}}],[\"more\",{\"1\":{\"21\":1,\"47\":1}}],[\"md\",{\"1\":{\"21\":1}}],[\"mark\",{\"0\":{\"29\":1},\"1\":{\"29\":1}}],[\"markdown\",{\"0\":{\"16\":1,\"17\":1,\"18\":1,\"19\":1},\"1\":{\"6\":1,\"16\":2,\"17\":3,\"18\":1,\"19\":3,\"20\":2,\"21\":1,\"22\":1,\"38\":1,\"48\":1,\"49\":2},\"2\":{\"45\":1,\"46\":1}}],[\"matter\",{\"1\":{\"0\":1}}],[\"vegetable\",{\"2\":{\"72\":1}}],[\"vue\",{\"0\":{\"42\":1}}],[\"vuepress\",{\"0\":{\"20\":1},\"1\":{\"16\":2,\"18\":2,\"19\":1,\"20\":2,\"21\":1,\"49\":1}}],[\"view\",{\"1\":{\"22\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"27\":1,\"28\":1,\"29\":1,\"30\":1,\"31\":1,\"32\":1,\"33\":1,\"34\":1,\"35\":1,\"36\":1,\"37\":1,\"38\":1,\"39\":1,\"40\":1,\"41\":1,\"42\":1,\"43\":1}}],[\"via\",{\"1\":{\"19\":1}}],[\"variable\",{\"1\":{\"22\":1}}],[\"where\",{\"1\":{\"32\":1}}],[\"which\",{\"1\":{\"19\":1}}],[\"words\",{\"1\":{\"29\":1}}],[\"word\",{\"1\":{\"27\":1}}],[\"warning\",{\"1\":{\"22\":1}}],[\"with\",{\"1\":{\"22\":1,\"49\":2,\"99\":1}}],[\"will\",{\"1\":{\"19\":1}}],[\"writing\",{\"1\":{\"20\":1,\"21\":1,\"48\":1}}],[\"write\",{\"1\":{\"16\":1,\"17\":1,\"49\":1}}],[\"know\",{\"1\":{\"17\":1,\"18\":1}}],[\"generate\",{\"1\":{\"16\":2}}],[\"guide\",{\"1\":{\"48\":2},\"2\":{\"7\":1,\"10\":1,\"14\":1,\"44\":1,\"51\":1,\"52\":1}}],[\"2020\",{\"1\":{\"48\":1}}],[\"230\",{\"1\":{\"32\":1}}],[\"253\",{\"1\":{\"32\":1}}],[\"2\",{\"0\":{\"55\":1,\"60\":1,\"65\":1,\"70\":1,\"75\":1,\"79\":1,\"80\":1,\"85\":1,\"90\":1,\"95\":1,\"99\":1,\"100\":1,\"105\":1,\"110\":1},\"1\":{\"13\":14}}],[\"10\",{\"1\":{\"99\":1}}],[\"15\",{\"1\":{\"32\":1}}],[\"138\",{\"1\":{\"32\":1}}],[\"19th\",{\"1\":{\"25\":1}}],[\"1\",{\"0\":{\"74\":1,\"94\":1},\"1\":{\"13\":12,\"22\":1,\"28\":1,\"48\":1}}],[\"number\",{\"1\":{\"99\":1}}],[\"need\",{\"1\":{\"18\":1}}],[\"new\",{\"1\":{\"17\":1}}],[\"next\",{\"1\":{\"9\":1}}],[\"navbar\",{\"1\":{\"9\":1,\"50\":1}}],[\"using\",{\"1\":{\"18\":1,\"21\":1}}],[\"use\",{\"1\":{\"0\":1,\"16\":1,\"22\":1,\"49\":2}}],[\"update\",{\"1\":{\"9\":1,\"50\":1}}],[\"long\",{\"2\":{\"98\":1,\"103\":1,\"108\":1,\"113\":1}}],[\"logy\",{\"1\":{\"37\":2}}],[\"logo\",{\"1\":{\"32\":2}}],[\"line\",{\"1\":{\"49\":1}}],[\"links\",{\"1\":{\"49\":2}}],[\"link\",{\"1\":{\"9\":2,\"22\":1,\"32\":1}}],[\"light\",{\"1\":{\"32\":1}}],[\"learner\",{\"1\":{\"17\":1}}],[\"layout\",{\"0\":{\"9\":1},\"1\":{\"0\":2,\"9\":1},\"2\":{\"12\":1}}],[\"e\",{\"1\":{\"49\":2}}],[\"excerpt\",{\"1\":{\"47\":1}}],[\"extends\",{\"1\":{\"21\":1}}],[\"extended\",{\"1\":{\"20\":1}}],[\"extensions\",{\"1\":{\"19\":1,\"20\":2}}],[\"extension\",{\"0\":{\"19\":1}}],[\"echarts\",{\"0\":{\"34\":1}}],[\"each\",{\"1\":{\"18\":1}}],[\"easily\",{\"1\":{\"16\":1}}],[\"edit\",{\"1\":{\"9\":1}}],[\"end\",{\"1\":{\"49\":1}}],[\"enrich\",{\"1\":{\"20\":1}}],[\"encryption\",{\"0\":{\"13\":1},\"1\":{\"6\":1,\"38\":1},\"2\":{\"15\":1}}],[\"enhancement\",{\"0\":{\"20\":1,\"21\":1,\"31\":1}}],[\"enhance\",{\"0\":{\"16\":1},\"1\":{\"6\":1,\"21\":1,\"38\":1},\"2\":{\"46\":1}}],[\"options\",{\"1\":{\"50\":1}}],[\"or\",{\"1\":{\"16\":1}}],[\"of\",{\"1\":{\"9\":1,\"40\":1,\"49\":1,\"50\":1}}],[\"on\",{\"1\":{\"9\":1}}],[\"cherry\",{\"0\":{\"54\":1},\"2\":{\"57\":1}}],[\"chart\",{\"0\":{\"33\":1}}],[\"curly\",{\"2\":{\"98\":1,\"103\":1,\"108\":1,\"113\":1}}],[\"cup\",{\"1\":{\"40\":1}}],[\"customize\",{\"1\":{\"50\":1}}],[\"custom\",{\"0\":{\"22\":1},\"1\":{\"22\":10,\"49\":1}}],[\"center\",{\"1\":{\"26\":1}}],[\"create\",{\"1\":{\"16\":1}}],[\"coffee\",{\"1\":{\"40\":1}}],[\"color\",{\"1\":{\"31\":1,\"32\":1}}],[\"code\",{\"0\":{\"24\":1,\"39\":1},\"1\":{\"22\":1}}],[\"comments\",{\"1\":{\"50\":1}}],[\"comment\",{\"1\":{\"9\":1,\"47\":1}}],[\"const\",{\"1\":{\"22\":1}}],[\"concept\",{\"1\":{\"18\":1}}],[\"convert\",{\"1\":{\"16\":1}}],[\"contain\",{\"1\":{\"50\":1}}],[\"contains\",{\"1\":{\"49\":1}}],[\"container\",{\"0\":{\"22\":1},\"1\":{\"22\":5}}],[\"contents\",{\"1\":{\"50\":1}}],[\"content\",{\"0\":{\"49\":1},\"1\":{\"13\":1,\"19\":1,\"43\":1,\"47\":1,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"contributors\",{\"1\":{\"9\":1,\"50\":1}}],[\"config\",{\"0\":{\"18\":1,\"47\":1},\"1\":{\"6\":1,\"38\":1,\"48\":1},\"2\":{\"52\":1,\"53\":1}}],[\"configuration\",{\"1\":{\"0\":1,\"18\":1}}],[\"category\",{\"1\":{\"48\":1}}],[\"catalog\",{\"0\":{\"6\":1}}],[\"card\",{\"0\":{\"32\":1}}],[\"can\",{\"1\":{\"9\":1,\"16\":2,\"29\":1,\"48\":1,\"49\":1,\"50\":1}}],[\"heading\",{\"0\":{\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"here\",{\"1\":{\"2\":1,\"49\":1,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"https\",{\"1\":{\"32\":2}}],[\"having\",{\"1\":{\"27\":1}}],[\"has\",{\"1\":{\"20\":1,\"28\":1}}],[\"h2o\",{\"1\":{\"25\":1}}],[\"hope\",{\"1\":{\"32\":2,\"40\":1,\"48\":1}}],[\"how\",{\"1\":{\"17\":1}}],[\"homepage\",{\"1\":{\"0\":1}}],[\"home\",{\"0\":{\"0\":1},\"1\":{\"0\":2},\"2\":{\"1\":1}}],[\"public\",{\"1\":{\"49\":1}}],[\"parsed\",{\"1\":{\"19\":1}}],[\"paragraph\",{\"1\":{\"13\":26}}],[\"pages\",{\"1\":{\"16\":2}}],[\"page\",{\"0\":{\"2\":1,\"3\":1,\"47\":1,\"48\":1,\"49\":1,\"50\":1},\"1\":{\"0\":2,\"6\":1,\"9\":4,\"18\":1,\"38\":1,\"47\":1,\"48\":2,\"50\":2},\"2\":{\"4\":1,\"52\":1,\"53\":1}}],[\"presentation\",{\"0\":{\"43\":1}}],[\"prev\",{\"1\":{\"9\":1}}],[\"provides\",{\"1\":{\"21\":1}}],[\"profile\",{\"1\":{\"2\":1}}],[\"playground\",{\"0\":{\"41\":1,\"42\":1}}],[\"plan\",{\"1\":{\"30\":2}}],[\"place\",{\"1\":{\"2\":1,\"49\":1}}],[\"plugin\",{\"1\":{\"21\":1}}],[\"plugins\",{\"1\":{\"19\":1}}],[\"please\",{\"1\":{\"0\":1,\"17\":1,\"20\":1,\"49\":1}}],[\"dragon\",{\"0\":{\"59\":1},\"2\":{\"62\":1}}],[\"dark\",{\"1\":{\"49\":1}}],[\"date\",{\"1\":{\"48\":1}}],[\"danger\",{\"1\":{\"22\":1}}],[\"desc\",{\"1\":{\"32\":1}}],[\"detail\",{\"1\":{\"22\":1,\"23\":1,\"24\":1,\"25\":1,\"26\":1,\"27\":1,\"28\":1,\"29\":1,\"30\":1,\"31\":1,\"32\":1,\"33\":1,\"34\":1,\"35\":1,\"36\":1,\"37\":1,\"38\":1,\"39\":1,\"40\":1,\"41\":1,\"42\":1,\"43\":1}}],[\"details\",{\"1\":{\"22\":1}}],[\"demo\",{\"0\":{\"5\":1,\"39\":1},\"1\":{\"0\":1,\"6\":1,\"9\":1,\"17\":1,\"38\":1},\"2\":{\"8\":1}}],[\"donate\",{\"1\":{\"40\":1}}],[\"don\",{\"1\":{\"17\":1,\"18\":1}}],[\"document\",{\"1\":{\"20\":1}}],[\"documentation\",{\"1\":{\"16\":1}}],[\"docs\",{\"1\":{\"0\":1}}],[\"directory\",{\"1\":{\"49\":1}}],[\"different\",{\"1\":{\"16\":1}}],[\"disabling\",{\"0\":{\"9\":1},\"2\":{\"12\":1}}],[\"disables\",{\"1\":{\"9\":1}}],[\"disable\",{\"1\":{\"6\":1,\"9\":1,\"38\":1},\"2\":{\"11\":1}}],[\"fruit\",{\"0\":{\"59\":1},\"2\":{\"62\":2,\"67\":1,\"87\":1,\"92\":1,\"97\":1,\"102\":1}}],[\"free\",{\"1\":{\"49\":1}}],[\"from\",{\"1\":{\"16\":1}}],[\"frontmatter\",{\"1\":{\"9\":1,\"18\":3,\"48\":1,\"50\":1}}],[\"front\",{\"1\":{\"0\":1}}],[\"flowchart\",{\"0\":{\"35\":1}}],[\"file\",{\"1\":{\"16\":1}}],[\"files\",{\"0\":{\"38\":1},\"1\":{\"16\":2,\"49\":1}}],[\"footnote\",{\"0\":{\"28\":1},\"1\":{\"28\":1,\"43\":1}}],[\"footer\",{\"1\":{\"9\":1,\"50\":1}}],[\"following\",{\"1\":{\"9\":1}}],[\"for\",{\"1\":{\"0\":1,\"18\":1,\"20\":1,\"49\":3}}],[\"functions\",{\"1\":{\"21\":1}}],[\"function\",{\"1\":{\"6\":1,\"9\":1,\"38\":1}}],[\"features\",{\"0\":{\"5\":1,\"9\":1},\"1\":{\"9\":1},\"2\":{\"8\":1,\"12\":1}}],[\"ir⋯\",{\"1\":{\"37\":1}}],[\"images\",{\"1\":{\"49\":2}}],[\"image\",{\"0\":{\"31\":1}}],[\"important\",{\"1\":{\"18\":1,\"29\":1}}],[\"id\",{\"1\":{\"27\":1}}],[\"i\",{\"1\":{\"26\":2,\"49\":2}}],[\"if\",{\"1\":{\"17\":1,\"18\":1}}],[\"it\",{\"1\":{\"16\":1,\"18\":1,\"19\":2}}],[\"including\",{\"1\":{\"50\":1}}],[\"include\",{\"0\":{\"38\":1}}],[\"info\",{\"1\":{\"18\":1}}],[\"information\",{\"0\":{\"48\":1},\"1\":{\"9\":1,\"22\":1,\"48\":1,\"50\":2}}],[\"introduce\",{\"1\":{\"18\":1}}],[\"introduction\",{\"0\":{\"17\":1},\"1\":{\"2\":1,\"18\":1}}],[\"intro\",{\"0\":{\"2\":1},\"1\":{\"17\":1}}],[\"in\",{\"1\":{\"0\":1,\"18\":1,\"19\":1,\"20\":1,\"22\":1,\"48\":1,\"49\":1,\"50\":1}}],[\"is\",{\"1\":{\"0\":1,\"9\":1,\"18\":1,\"32\":2,\"43\":1,\"47\":1,\"48\":3,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"big\",{\"2\":{\"63\":1,\"78\":1,\"83\":1,\"88\":1,\"93\":1}}],[\"blue\",{\"1\":{\"49\":1}}],[\"bloghome\",{\"1\":{\"0\":1}}],[\"blog\",{\"0\":{\"0\":1},\"1\":{\"0\":2,\"16\":1},\"2\":{\"1\":1}}],[\"but\",{\"1\":{\"49\":1}}],[\"button\",{\"1\":{\"9\":1,\"50\":1}}],[\"b\",{\"1\":{\"30\":1}}],[\"being\",{\"1\":{\"79\":1,\"99\":1}}],[\"besides\",{\"1\":{\"49\":1}}],[\"before\",{\"1\":{\"47\":1}}],[\"be\",{\"1\":{\"19\":1}}],[\"banana\",{\"0\":{\"94\":1,\"99\":1,\"104\":1,\"109\":1},\"1\":{\"99\":1},\"2\":{\"97\":1,\"102\":1,\"107\":1,\"112\":1}}],[\"badge\",{\"1\":{\"49\":3}}],[\"basically\",{\"1\":{\"16\":1}}],[\"back\",{\"1\":{\"9\":1,\"50\":1}}],[\"breadcrumb\",{\"1\":{\"9\":1,\"50\":1}}],[\"by\",{\"1\":{\"9\":1,\"19\":1,\"21\":1}}],[\"both\",{\"1\":{\"0\":1}}],[\"small\",{\"2\":{\"58\":1,\"68\":1}}],[\"s\",{\"1\":{\"48\":1}}],[\"stared\",{\"1\":{\"79\":1,\"99\":1}}],[\"starting\",{\"1\":{\"49\":2}}],[\"strawberry\",{\"0\":{\"64\":1},\"2\":{\"67\":1}}],[\"structure\",{\"0\":{\"50\":1},\"1\":{\"16\":1}}],[\"stylize\",{\"0\":{\"40\":1}}],[\"svg\",{\"1\":{\"32\":1}}],[\"scheme\",{\"1\":{\"31\":1}}],[\"subscript\",{\"0\":{\"25\":1}}],[\"support\",{\"1\":{\"31\":1}}],[\"supports\",{\"1\":{\"19\":1}}],[\"superscript\",{\"0\":{\"25\":1}}],[\"safely\",{\"1\":{\"22\":1}}],[\"syntax\",{\"1\":{\"19\":1,\"20\":1,\"21\":1}}],[\"site\",{\"1\":{\"32\":2}}],[\"sites\",{\"1\":{\"16\":1}}],[\"size\",{\"1\":{\"31\":1}}],[\"sidebar\",{\"1\":{\"9\":1,\"50\":1}}],[\"so\",{\"1\":{\"16\":2}}],[\"some\",{\"1\":{\"9\":1}}],[\"slide\",{\"0\":{\"3\":1},\"2\":{\"4\":1}}],[\"see\",{\"1\":{\"0\":1}}],[\"setting\",{\"1\":{\"9\":1,\"31\":1}}],[\"set\",{\"1\":{\"0\":1,\"48\":1}}],[\"should\",{\"1\":{\"0\":1,\"16\":1,\"49\":1,\"50\":1}}],[\"tex\",{\"0\":{\"37\":1}}],[\"text\",{\"1\":{\"13\":26,\"28\":1,\"49\":1}}],[\"table\",{\"1\":{\"50\":1}}],[\"tabs\",{\"0\":{\"23\":1,\"24\":1}}],[\"tags\",{\"1\":{\"48\":1}}],[\"tasklist\",{\"0\":{\"30\":1}}],[\"tip\",{\"1\":{\"22\":1}}],[\"title\",{\"1\":{\"22\":5,\"32\":1,\"50\":1}}],[\"time\",{\"1\":{\"9\":1,\"50\":1}}],[\"t\",{\"1\":{\"17\":1,\"18\":1}}],[\"that\",{\"1\":{\"9\":1,\"16\":1}}],[\"there\",{\"1\":{\"32\":2}}],[\"these\",{\"1\":{\"20\":1}}],[\"theme\",{\"0\":{\"21\":1},\"1\":{\"21\":1,\"49\":1,\"50\":1}}],[\"them\",{\"1\":{\"16\":1,\"49\":2,\"50\":1}}],[\"the\",{\"1\":{\"0\":1,\"9\":4,\"13\":1,\"19\":1,\"21\":1,\"48\":2,\"49\":2,\"55\":1,\"56\":1,\"60\":1,\"61\":1,\"65\":1,\"66\":1,\"70\":1,\"71\":1,\"75\":1,\"76\":1,\"80\":1,\"81\":1,\"85\":1,\"86\":1,\"90\":1,\"91\":1,\"95\":1,\"96\":1,\"100\":1,\"101\":1,\"105\":1,\"106\":1,\"110\":1,\"111\":1}}],[\"this\",{\"1\":{\"0\":2,\"9\":1,\"28\":1,\"43\":1,\"50\":1}}],[\"true\",{\"1\":{\"0\":1}}],[\"tomato\",{\"0\":{\"69\":1}}],[\"toc\",{\"1\":{\"50\":1}}],[\"top\",{\"1\":{\"9\":1,\"50\":1}}],[\"to\",{\"1\":{\"0\":1,\"9\":1,\"16\":3,\"17\":1,\"18\":1,\"20\":1,\"49\":1,\"50\":1}}],[\"apple\",{\"0\":{\"74\":1,\"79\":1,\"84\":1,\"89\":1},\"1\":{\"79\":1},\"2\":{\"77\":1,\"82\":1,\"87\":1,\"92\":1}}],[\"at\",{\"1\":{\"49\":1}}],[\"attrs\",{\"0\":{\"27\":1}}],[\"absolute\",{\"1\":{\"49\":1}}],[\"author\",{\"1\":{\"48\":1}}],[\"assets\",{\"1\":{\"49\":1}}],[\"as\",{\"1\":{\"47\":1}}],[\"am\",{\"1\":{\"26\":2}}],[\"align\",{\"0\":{\"26\":1},\"1\":{\"26\":1}}],[\"are\",{\"1\":{\"17\":1,\"48\":1,\"49\":1}}],[\"article\",{\"0\":{\"13\":1},\"1\":{\"13\":1,\"79\":1,\"99\":1}}],[\"according\",{\"1\":{\"16\":1}}],[\"actual\",{\"1\":{\"13\":1}}],[\"an\",{\"1\":{\"9\":1}}],[\"and\",{\"0\":{\"9\":1,\"25\":1},\"1\":{\"0\":1,\"2\":1,\"9\":1,\"16\":1,\"17\":2,\"21\":1,\"31\":1,\"48\":1,\"50\":3},\"2\":{\"12\":1}}],[\"a\",{\"1\":{\"0\":1,\"17\":1,\"18\":1,\"22\":6,\"27\":1,\"30\":1,\"40\":1,\"49\":2,\"79\":1,\"99\":1}}]],\"serializationVersion\":2},\"/\":{\"documentCount\":120,\"nextId\":120,\"documentIds\":{\"0\":\"v-c8296fee\",\"1\":\"v-c8296fee@2\",\"2\":\"v-0852455e\",\"3\":\"v-0852455e@2\",\"4\":\"v-1d22e941\",\"5\":\"v-1d22e941@2\",\"6\":\"v-5decfa84\",\"7\":\"v-5decfa84@2\",\"8\":\"v-075c6c62\",\"9\":\"v-075c6c62@2\",\"10\":\"v-506407f4\",\"11\":\"v-506407f4@2\",\"12\":\"v-37a8c5a0\",\"13\":\"v-37a8c5a0@2\",\"14\":\"v-0379cba1\",\"15\":\"v-0379cba1@2\",\"16\":\"v-0fe52c37\",\"17\":\"v-0fe52c37@2\",\"18\":\"v-c6edb6ae\",\"19\":\"v-c6edb6ae@2\",\"20\":\"v-54d7ff21\",\"21\":\"v-54d7ff21@2\",\"22\":\"v-2c3ee7f5\",\"23\":\"v-2c3ee7f5@2\",\"24\":\"v-27b02be6\",\"25\":\"v-27b02be6@2\",\"26\":\"v-02c6a6b2\",\"27\":\"v-02c6a6b2@2\",\"28\":\"v-0017792c\",\"29\":\"v-0017792c@2\",\"30\":\"v-2e75e8de\",\"31\":\"v-2e75e8de@2\",\"32\":\"v-6f7bfa04\",\"33\":\"v-6f7bfa04@2\",\"34\":\"v-0e0b961f\",\"35\":\"v-0e0b961f@2\",\"36\":\"v-7e751551\",\"37\":\"v-7e751551@2\",\"38\":\"v-b6ff5888\",\"39\":\"v-b6ff5888@2\",\"40\":\"v-29e33f95\",\"41\":\"v-29e33f95@2\",\"42\":\"v-dbaf7c9c\",\"43\":\"v-dbaf7c9c@2\",\"44\":\"v-1e3e75c0\",\"45\":\"v-1e3e75c0@2\",\"46\":\"v-0564ef99\",\"47\":\"v-0564ef99@2\",\"48\":\"v-3de926ea\",\"49\":\"v-3de926ea@2\",\"50\":\"v-7b34f334\",\"51\":\"v-7b34f334@2\",\"52\":\"v-3c599b43\",\"53\":\"v-3c599b43@2\",\"54\":\"v-fbb94a6e\",\"55\":\"v-fbb94a6e@2\",\"56\":\"v-1e4ce2de\",\"57\":\"v-1e4ce2de@2\",\"58\":\"v-d39aaa20\",\"59\":\"v-d39aaa20@2\",\"60\":\"v-a0d528ce\",\"61\":\"v-a0d528ce@2\",\"62\":\"v-0115d78b\",\"63\":\"v-0115d78b@2\",\"64\":\"v-231414e4\",\"65\":\"v-231414e4@2\",\"66\":\"v-5f9776df\",\"67\":\"v-5f9776df@2\",\"68\":\"v-2ae80a11\",\"69\":\"v-2ae80a11@2\",\"70\":\"v-540234fd\",\"71\":\"v-540234fd@2\",\"72\":\"v-1def6584\",\"73\":\"v-1def6584@2\",\"74\":\"v-62a926ee\",\"75\":\"v-62a926ee@2\",\"76\":\"v-4f52202f\",\"77\":\"v-4f52202f@2\",\"78\":\"v-a5303446\",\"79\":\"v-a5303446@2\",\"80\":\"v-4f1e78a0\",\"81\":\"v-4f1e78a0@2\",\"82\":\"v-521d399c\",\"83\":\"v-521d399c@2\",\"84\":\"v-b2f11bc8\",\"85\":\"v-b2f11bc8@2\",\"86\":\"v-4c8be360\",\"87\":\"v-4c8be360@2\",\"88\":\"v-2d29c23d\",\"89\":\"v-2d29c23d@2\",\"90\":\"v-67ef9756\",\"91\":\"v-67ef9756@2\",\"92\":\"v-366a930c\",\"93\":\"v-366a930c@2\",\"94\":\"v-4729f7b3\",\"95\":\"v-4729f7b3@2\",\"96\":\"v-af0ebf8e\",\"97\":\"v-af0ebf8e@2\",\"98\":\"v-259091a4\",\"99\":\"v-259091a4@2\",\"100\":\"v-0a160bb2\",\"101\":\"v-0a160bb2@2\",\"102\":\"v-6de8295f\",\"103\":\"v-6de8295f@2\",\"104\":\"v-1d9f85f4\",\"105\":\"v-1d9f85f4@2\",\"106\":\"v-0a768313\",\"107\":\"v-0a768313@2\",\"108\":\"v-bdcc4a40\",\"109\":\"v-bdcc4a40@2\",\"110\":\"v-0e85e50e\",\"111\":\"v-0e85e50e@2\",\"112\":\"v-cb6a3c84\",\"113\":\"v-cb6a3c84@2\",\"114\":\"v-21387c08\",\"115\":\"v-21387c08@2\",\"116\":\"v-1434d78e\",\"117\":\"v-1434d78e@2\",\"118\":\"v-1beaf78e\",\"119\":\"v-1beaf78e@2\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[null,null,1],\"2\":[1],\"3\":[null,null,1],\"4\":[1],\"5\":[null,null,1],\"6\":[1],\"7\":[null,null,1],\"8\":[1],\"9\":[null,null,1],\"10\":[1],\"11\":[null,null,1],\"12\":[1],\"13\":[null,null,1],\"14\":[1],\"15\":[null,null,1],\"16\":[2],\"17\":[null,null,2],\"18\":[2],\"19\":[null,null,2],\"20\":[1],\"21\":[null,null,1],\"22\":[1],\"23\":[null,null,1],\"24\":[1],\"25\":[null,null,1],\"26\":[2],\"27\":[null,null,2],\"28\":[2],\"29\":[null,null,2],\"30\":[3],\"31\":[null,null,3],\"32\":[2],\"33\":[null,null,2],\"34\":[2],\"35\":[null,null,2],\"36\":[3],\"37\":[null,null,3],\"38\":[2],\"39\":[null,null,2],\"40\":[2],\"41\":[null,null,2],\"42\":[2],\"43\":[null,null,2],\"44\":[2],\"45\":[null,null,2],\"46\":[2],\"47\":[null,null,2],\"48\":[2],\"49\":[null,null,2],\"50\":[2],\"51\":[null,null,2],\"52\":[2],\"53\":[null,null,2],\"54\":[1],\"55\":[null,null,1],\"56\":[2],\"57\":[null,null,2],\"58\":[2],\"59\":[null,null,2],\"60\":[2],\"61\":[null,null,2],\"62\":[2],\"63\":[null,null,2],\"64\":[2],\"65\":[null,null,2],\"66\":[2],\"67\":[null,null,2],\"68\":[2],\"69\":[null,null,2],\"70\":[1],\"71\":[null,null,1],\"72\":[3],\"73\":[null,null,3],\"74\":[3],\"75\":[null,null,3],\"76\":[2],\"77\":[null,null,2],\"78\":[3],\"79\":[null,null,3],\"80\":[2],\"81\":[null,null,2],\"82\":[2],\"83\":[null,null,2],\"84\":[3],\"85\":[null,null,3],\"86\":[3],\"87\":[null,null,3],\"88\":[2],\"89\":[null,null,2],\"90\":[2],\"91\":[null,null,2],\"92\":[2],\"93\":[null,null,2],\"94\":[2],\"95\":[null,null,2],\"96\":[2],\"97\":[null,null,2],\"98\":[2],\"99\":[null,null,2],\"100\":[2],\"101\":[null,null,2],\"102\":[2],\"103\":[null,null,2],\"104\":[2],\"105\":[null,null,2],\"106\":[2],\"107\":[null,null,2],\"108\":[2],\"109\":[null,null,2],\"110\":[2],\"111\":[null,null,2],\"112\":[3],\"113\":[null,null,3],\"114\":[4],\"115\":[null,null,4],\"116\":[2],\"117\":[null,null,2],\"118\":[2],\"119\":[null,null,2]},\"averageFieldLength\":[1.6673896752143724,null,1.5852452361445728],\"storedFields\":{\"0\":{\"h\":\"Posts\"},\"1\":{\"c\":[\"Posts\"]},\"2\":{\"h\":\"Apple\"},\"3\":{\"c\":[\"Apple\"]},\"4\":{\"h\":\"Banana\"},\"5\":{\"c\":[\"Banana\"]},\"6\":{\"h\":\"Category\"},\"7\":{\"c\":[\"Category\"]},\"8\":{\"h\":\"Tag\"},\"9\":{\"c\":[\"Tag\"]},\"10\":{\"h\":\"Articles\"},\"11\":{\"c\":[\"Articles\"]},\"12\":{\"h\":\"Star\"},\"13\":{\"c\":[\"Star\"]},\"14\":{\"h\":\"Timeline\"},\"15\":{\"c\":[\"Timeline\"]},\"16\":{\"h\":\"Guide Category\"},\"17\":{\"c\":[\"Guide Category\"]},\"18\":{\"h\":\"disable Tag\"},\"19\":{\"c\":[\"disable Tag\"]},\"20\":{\"h\":\"文章\"},\"21\":{\"c\":[\"文章\"]},\"22\":{\"h\":\"收藏\"},\"23\":{\"c\":[\"收藏\"]},\"24\":{\"h\":\"时间轴\"},\"25\":{\"c\":[\"时间轴\"]},\"26\":{\"h\":\"Cherry Category\"},\"27\":{\"c\":[\"Cherry Category\"]},\"28\":{\"h\":\"encryption Tag\"},\"29\":{\"c\":[\"encryption Tag\"]},\"30\":{\"h\":\"Dragon Fruit Category\"},\"31\":{\"c\":[\"Dragon Fruit Category\"]},\"32\":{\"h\":\"Markdown Tag\"},\"33\":{\"c\":[\"Markdown Tag\"]},\"34\":{\"h\":\"Fruit Category\"},\"35\":{\"c\":[\"Fruit Category\"]},\"36\":{\"h\":\"Page config Tag\"},\"37\":{\"c\":[\"Page config Tag\"]},\"38\":{\"h\":\"Strawberry Category\"},\"39\":{\"c\":[\"Strawberry Category\"]},\"40\":{\"h\":\"Guide Tag\"},\"41\":{\"c\":[\"Guide Tag\"]},\"42\":{\"h\":\"Vegetable Category\"},\"43\":{\"c\":[\"Vegetable Category\"]},\"44\":{\"h\":\"red Tag\"},\"45\":{\"c\":[\"red Tag\"]},\"46\":{\"h\":\"Apple Category\"},\"47\":{\"c\":[\"Apple Category\"]},\"48\":{\"h\":\"small Tag\"},\"49\":{\"c\":[\"small Tag\"]},\"50\":{\"h\":\"Banana Category\"},\"51\":{\"c\":[\"Banana Category\"]},\"52\":{\"h\":\"round Tag\"},\"53\":{\"c\":[\"round Tag\"]},\"54\":{\"h\":\"分类\"},\"55\":{\"c\":[\"分类\"]},\"56\":{\"h\":\"big Tag\"},\"57\":{\"c\":[\"big Tag\"]},\"58\":{\"h\":\"数据集 分类\"},\"59\":{\"c\":[\"数据集 分类\"]},\"60\":{\"h\":\"yellow Tag\"},\"61\":{\"c\":[\"yellow Tag\"]},\"62\":{\"h\":\"微调技术 分类\"},\"63\":{\"c\":[\"微调技术 分类\"]},\"64\":{\"h\":\"curly Tag\"},\"65\":{\"c\":[\"curly Tag\"]},\"66\":{\"h\":\"语言模型 分类\"},\"67\":{\"c\":[\"语言模型 分类\"]},\"68\":{\"h\":\"long Tag\"},\"69\":{\"c\":[\"long Tag\"]},\"70\":{\"h\":\"标签\"},\"71\":{\"c\":[\"标签\"]},\"72\":{\"h\":\"Instruct Tuning 标签\"},\"73\":{\"c\":[\"Instruct Tuning 标签\"]},\"74\":{\"h\":\"Prompt Tuning 标签\"},\"75\":{\"c\":[\"Prompt Tuning 标签\"]},\"76\":{\"h\":\"PEFT 标签\"},\"77\":{\"c\":[\"PEFT 标签\"]},\"78\":{\"h\":\"Hugging Face 标签\"},\"79\":{\"c\":[\"Hugging Face 标签\"]},\"80\":{\"h\":\"LoRA 标签\"},\"81\":{\"c\":[\"LoRA 标签\"]},\"82\":{\"h\":\"AdaLoRA 标签\"},\"83\":{\"c\":[\"AdaLoRA 标签\"]},\"84\":{\"h\":\"Prefix Tuning 标签\"},\"85\":{\"c\":[\"Prefix Tuning 标签\"]},\"86\":{\"h\":\"P-Tuning 标签\"},\"87\":{\"c\":[\"P-Tuning 标签\"]},\"88\":{\"h\":\"优化 标签\"},\"89\":{\"c\":[\"优化 标签\"]},\"90\":{\"h\":\"内存 标签\"},\"91\":{\"c\":[\"内存 标签\"]},\"92\":{\"h\":\"机器学习 标签\"},\"93\":{\"c\":[\"机器学习 标签\"]},\"94\":{\"h\":\"transformer 标签\"},\"95\":{\"c\":[\"transformer 标签\"]},\"96\":{\"h\":\"字节 标签\"},\"97\":{\"c\":[\"字节 标签\"]},\"98\":{\"h\":\"模型 标签\"},\"99\":{\"c\":[\"模型 标签\"]},\"100\":{\"h\":\"深度学习 标签\"},\"101\":{\"c\":[\"深度学习 标签\"]},\"102\":{\"h\":\"LLM 标签\"},\"103\":{\"c\":[\"LLM 标签\"]},\"104\":{\"h\":\"推理 标签\"},\"105\":{\"c\":[\"推理 标签\"]},\"106\":{\"h\":\"摘要 标签\"},\"107\":{\"c\":[\"摘要 标签\"]},\"108\":{\"h\":\"OpenAI 标签\"},\"109\":{\"c\":[\"OpenAI 标签\"]},\"110\":{\"h\":\"Google 标签\"},\"111\":{\"c\":[\"Google 标签\"]},\"112\":{\"h\":\"Instruction Tuning 标签\"},\"113\":{\"c\":[\"Instruction Tuning 标签\"]},\"114\":{\"h\":\"In-context Learning 标签\"},\"115\":{\"c\":[\"In-context Learning 标签\"]},\"116\":{\"h\":\"ChatGPT 标签\"},\"117\":{\"c\":[\"ChatGPT 标签\"]},\"118\":{\"h\":\"强化学习 标签\"},\"119\":{\"c\":[\"强化学习 标签\"]}},\"dirtCount\":0,\"index\":[[\"强化学习\",{\"0\":{\"118\":1},\"2\":{\"119\":1}}],[\"in\",{\"0\":{\"114\":1},\"2\":{\"115\":1}}],[\"instruction\",{\"0\":{\"112\":1},\"2\":{\"113\":1}}],[\"instruct\",{\"0\":{\"72\":1},\"2\":{\"73\":1}}],[\"google\",{\"0\":{\"110\":1},\"2\":{\"111\":1}}],[\"guide\",{\"0\":{\"16\":1,\"40\":1},\"2\":{\"17\":1,\"41\":1}}],[\"openai\",{\"0\":{\"108\":1},\"2\":{\"109\":1}}],[\"摘要\",{\"0\":{\"106\":1},\"2\":{\"107\":1}}],[\"推理\",{\"0\":{\"104\":1},\"2\":{\"105\":1}}],[\"learning\",{\"0\":{\"114\":1},\"2\":{\"115\":1}}],[\"llm\",{\"0\":{\"102\":1},\"2\":{\"103\":1}}],[\"lora\",{\"0\":{\"80\":1},\"2\":{\"81\":1}}],[\"long\",{\"0\":{\"68\":1},\"2\":{\"69\":1}}],[\"深度学习\",{\"0\":{\"100\":1},\"2\":{\"101\":1}}],[\"模型\",{\"0\":{\"98\":1},\"2\":{\"99\":1}}],[\"字节\",{\"0\":{\"96\":1},\"2\":{\"97\":1}}],[\"机器学习\",{\"0\":{\"92\":1},\"2\":{\"93\":1}}],[\"内存\",{\"0\":{\"90\":1},\"2\":{\"91\":1}}],[\"优化\",{\"0\":{\"88\":1},\"2\":{\"89\":1}}],[\"face\",{\"0\":{\"78\":1},\"2\":{\"79\":1}}],[\"fruit\",{\"0\":{\"30\":1,\"34\":1},\"2\":{\"31\":1,\"35\":1}}],[\"hugging\",{\"0\":{\"78\":1},\"2\":{\"79\":1}}],[\"标签\",{\"0\":{\"70\":1,\"72\":1,\"74\":1,\"76\":1,\"78\":1,\"80\":1,\"82\":1,\"84\":1,\"86\":1,\"88\":1,\"90\":1,\"92\":1,\"94\":1,\"96\":1,\"98\":1,\"100\":1,\"102\":1,\"104\":1,\"106\":1,\"108\":1,\"110\":1,\"112\":1,\"114\":1,\"116\":1,\"118\":1},\"2\":{\"71\":1,\"73\":1,\"75\":1,\"77\":1,\"79\":1,\"81\":1,\"83\":1,\"85\":1,\"87\":1,\"89\":1,\"91\":1,\"93\":1,\"95\":1,\"97\":1,\"99\":1,\"101\":1,\"103\":1,\"105\":1,\"107\":1,\"109\":1,\"111\":1,\"113\":1,\"115\":1,\"117\":1,\"119\":1}}],[\"语言模型\",{\"0\":{\"66\":1},\"2\":{\"67\":1}}],[\"微调技术\",{\"0\":{\"62\":1},\"2\":{\"63\":1}}],[\"yellow\",{\"0\":{\"60\":1},\"2\":{\"61\":1}}],[\"数据集\",{\"0\":{\"58\":1},\"2\":{\"59\":1}}],[\"big\",{\"0\":{\"56\":1},\"2\":{\"57\":1}}],[\"banana\",{\"0\":{\"4\":1,\"50\":1},\"2\":{\"5\":1,\"51\":1}}],[\"分类\",{\"0\":{\"54\":1,\"58\":1,\"62\":1,\"66\":1},\"2\":{\"55\":1,\"59\":1,\"63\":1,\"67\":1}}],[\"round\",{\"0\":{\"52\":1},\"2\":{\"53\":1}}],[\"red\",{\"0\":{\"44\":1},\"2\":{\"45\":1}}],[\"small\",{\"0\":{\"48\":1},\"2\":{\"49\":1}}],[\"strawberry\",{\"0\":{\"38\":1},\"2\":{\"39\":1}}],[\"star\",{\"0\":{\"12\":1},\"2\":{\"13\":1}}],[\"vegetable\",{\"0\":{\"42\":1},\"2\":{\"43\":1}}],[\"p\",{\"0\":{\"86\":1},\"2\":{\"87\":1}}],[\"prefix\",{\"0\":{\"84\":1},\"2\":{\"85\":1}}],[\"prompt\",{\"0\":{\"74\":1},\"2\":{\"75\":1}}],[\"peft\",{\"0\":{\"76\":1},\"2\":{\"77\":1}}],[\"page\",{\"0\":{\"36\":1},\"2\":{\"37\":1}}],[\"posts\",{\"0\":{\"0\":1},\"2\":{\"1\":1}}],[\"markdown\",{\"0\":{\"32\":1},\"2\":{\"33\":1}}],[\"dragon\",{\"0\":{\"30\":1},\"2\":{\"31\":1}}],[\"disable\",{\"0\":{\"18\":1},\"2\":{\"19\":1}}],[\"encryption\",{\"0\":{\"28\":1},\"2\":{\"29\":1}}],[\"chatgpt\",{\"0\":{\"116\":1},\"2\":{\"117\":1}}],[\"cherry\",{\"0\":{\"26\":1},\"2\":{\"27\":1}}],[\"context\",{\"0\":{\"114\":1},\"2\":{\"115\":1}}],[\"config\",{\"0\":{\"36\":1},\"2\":{\"37\":1}}],[\"curly\",{\"0\":{\"64\":1},\"2\":{\"65\":1}}],[\"category\",{\"0\":{\"6\":1,\"16\":1,\"26\":1,\"30\":1,\"34\":1,\"38\":1,\"42\":1,\"46\":1,\"50\":1},\"2\":{\"7\":1,\"17\":1,\"27\":1,\"31\":1,\"35\":1,\"39\":1,\"43\":1,\"47\":1,\"51\":1}}],[\"时间轴\",{\"0\":{\"24\":1},\"2\":{\"25\":1}}],[\"收藏\",{\"0\":{\"22\":1},\"2\":{\"23\":1}}],[\"文章\",{\"0\":{\"20\":1},\"2\":{\"21\":1}}],[\"transformer\",{\"0\":{\"94\":1},\"2\":{\"95\":1}}],[\"tuning\",{\"0\":{\"72\":1,\"74\":1,\"84\":1,\"86\":1,\"112\":1},\"2\":{\"73\":1,\"75\":1,\"85\":1,\"87\":1,\"113\":1}}],[\"timeline\",{\"0\":{\"14\":1},\"2\":{\"15\":1}}],[\"tag\",{\"0\":{\"8\":1,\"18\":1,\"28\":1,\"32\":1,\"36\":1,\"40\":1,\"44\":1,\"48\":1,\"52\":1,\"56\":1,\"60\":1,\"64\":1,\"68\":1},\"2\":{\"9\":1,\"19\":1,\"29\":1,\"33\":1,\"37\":1,\"41\":1,\"45\":1,\"49\":1,\"53\":1,\"57\":1,\"61\":1,\"65\":1,\"69\":1}}],[\"adalora\",{\"0\":{\"82\":1},\"2\":{\"83\":1}}],[\"articles\",{\"0\":{\"10\":1},\"2\":{\"11\":1}}],[\"apple\",{\"0\":{\"2\":1,\"46\":1},\"2\":{\"3\":1,\"47\":1}}]],\"serializationVersion\":2}}")).map(([e,t])=>[e,Et(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:s,options:n}})=>{e==="suggest"?self.postMessage(st(t,v[s],n)):e==="search"?self.postMessage(et(t,v[s],n)):self.postMessage({suggestions:st(t,v[s],n),results:et(t,v[s],n)})};
//# sourceMappingURL=index.js.map
