const t=JSON.parse('{"key":"v-e7f16b42","path":"/zh/posts/2023-06/Goat.html","title":"在特定任务上微调语言模型","lang":"zh-CN","frontmatter":{"author":"孙鸿博孙鸿博","icon":"pen-to-square","date":"2023-06-12T00:00:00.000Z","category":["LLM"],"tag":["推理"],"star":true,"description":"在特定任务上微调语言模型 论文：Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks【新加坡国立大学】 Goat模型是一个基于7B LLaMA微调的模型，在算术任务上的性能优于GPT-4，在零样本设置中还超越了75倍参数量的540B PaLM","head":[["meta",{"property":"og:url","content":"https://github.com/hust-404/hust-404.github.io/zh/posts/2023-06/Goat.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"在特定任务上微调语言模型"}],["meta",{"property":"og:description","content":"在特定任务上微调语言模型 论文：Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks【新加坡国立大学】 Goat模型是一个基于7B LLaMA微调的模型，在算术任务上的性能优于GPT-4，在零样本设置中还超越了75倍参数量的540B PaLM"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-13T02:14:31.000Z"}],["meta",{"property":"article:author","content":"孙鸿博孙鸿博"}],["meta",{"property":"article:tag","content":"推理"}],["meta",{"property":"article:published_time","content":"2023-06-12T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-13T02:14:31.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"在特定任务上微调语言模型\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-12T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-13T02:14:31.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"孙鸿博孙鸿博\\"}]}"]]},"headers":[],"git":{"createdTime":1686588551000,"updatedTime":1686622471000,"contributors":[{"name":"Liu Xiao","email":"42756849+liuxiaocs7@users.noreply.github.com","commits":2},{"name":"Sun","email":"57407733+shbone@users.noreply.github.com","commits":1}]},"readingTime":{"minutes":1.46,"words":439},"filePathRelative":"zh/posts/2023-06/Goat.md","localizedDate":"2023年6月12日","excerpt":"<h1> 在特定任务上微调语言模型</h1>\\n<p>论文：Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks【新加坡国立大学】</p>\\n<p>Goat模型是一个基于7B LLaMA微调的模型，在算术任务上的性能优于GPT-4，在零样本设置中还超越了75倍参数量的540B PaLM</p>\\n","autoDesc":true}');export{t as data};
