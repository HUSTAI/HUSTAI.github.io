import{_ as a}from"./plugin-vue_export-helper-c27b6911.js";import{r as i,o as s,c as r,e as l,a as t,b as e,d as c,f as n}from"./app-be719245.js";const d="/assets/images/llm/chatgpt1.png",f="/assets/images/llm/chatgpt2.png",u="/assets/images/llm/chatgpt3.png",p="/assets/images/llm/chatgpt4.png",h={},g=t("ul",null,[t("li",null,"GPT系列模型发展历程"),t("li",null,"提示微调"),t("li",null,"上下文学习")],-1),_=n('<h2 id="_1-gpt系列模型发展历程" tabindex="-1"><a class="header-anchor" href="#_1-gpt系列模型发展历程" aria-hidden="true">#</a> 1 GPT系列模型发展历程</h2><figure><img src="'+d+'" alt="图1 GPT系列模型树" tabindex="0" loading="lazy"><figcaption>图1 gpt系列模型树</figcaption></figure><h2 id="_2-提示微调-instruction-tuning" tabindex="-1"><a class="header-anchor" href="#_2-提示微调-instruction-tuning" aria-hidden="true">#</a> 2 提示微调 instruction tuning<sup class="footnote-ref"><a href="#footnote1">[1]</a><a class="footnote-anchor" id="footnote-ref1"></a></sup></h2><p>该概念的提出来自于Google的一篇论文. 提示微调结合了pretrain-finetune和prompt两个范式的优点，即用prompt格式的训练数据进行fine-tune，以使模型具备人类倾向的回答问题能力。<br> 提示微调结合了pretrain-finetune和prompt两个范式的优点，即用prompt格式的训练数据进行fine-tune，以使模型具备人类倾向的回答问题能力。<br> 在 2022 年 3 月，OpenAI 发布了指令微调<sup class="footnote-ref"><a href="#footnote2">[2]</a><a class="footnote-anchor" id="footnote-ref2"></a></sup> 的论文，其监督微调(supervised instruction tuning) 的部分对应了davinci-instruct-beta和text-davinci-001。</p><blockquote><p>We focus on fine-tuning approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF) to fine-tune GPT-3 to follow a broad class of written instructions.</p></blockquote><h2 id="_3-模型的训练方法和数据集" tabindex="-1"><a class="header-anchor" href="#_3-模型的训练方法和数据集" aria-hidden="true">#</a> 3 模型的训练方法和数据集</h2>',6),m=t("img",{src:f,alt:"图2 模型训练步骤",title:"图2 模型训练步骤",loading:"lazy"},null,-1),b=t("br",null,null,-1),T=t("br",null,null,-1),x=t("br",null,null,-1),P=t("br",null,null,-1),G={href:"https://www.assemblyai.com/blog/how-chatgpt-actually-works/",target:"_blank",rel:"noopener noreferrer"},k=t("br",null,null,-1),C=t("br",null,null,-1),I=t("img",{src:u,alt:"图3 instructgpt的训练数据构成",title:"图3 instructgpt的训练数据构成",loading:"lazy"},null,-1),L=t("br",null,null,-1),w=t("br",null,null,-1),v=n('<h2 id="_4-上下文学习-in-context-learning" tabindex="-1"><a class="header-anchor" href="#_4-上下文学习-in-context-learning" aria-hidden="true">#</a> 4 上下文学习(in-context learning)</h2><p>从类比中学习，和人类的决策相似<br> ICL只存在一次前向传播中，还是会被模型记住？<br> 论文中ICL的测试数据，类似于下图所示，每次预测都需要结合之前的几个demonstration，由此推测ICL并不会被模型记住。结合对text-davinci-003API的测试，在一次调用中教会它数学题，之后单独询问，模型并不能正确回答，由此可以证明ICL只存在于一次前向传播。</p><figure><img src="'+p+'" alt="图4 上下文学习和微调的区别" tabindex="0" loading="lazy"><figcaption>图4 上下文学习和微调的区别</figcaption></figure><p>ICL是一个元优化的过程<sup class="footnote-ref"><a href="#footnote3">[3]</a><a class="footnote-anchor" id="footnote-ref3"></a></sup>，可以看做隐性微调。GPT首先根据演示示例生成元梯度，然后将这些元梯度应用于原始GPT以构建ICL模型。</p><blockquote><p>Considering that ICL directly takes effect on only the attention keys and values.</p><p>ICL只对attention有影响。</p></blockquote><h2 id="_5-参考" tabindex="-1"><a class="header-anchor" href="#_5-参考" aria-hidden="true">#</a> 5 参考</h2><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="footnote1" class="footnote-item"><p>Google Finetuned Language Models Are Zero-Shot Learners instruction tuning: finetuning language models on a collection of datasets described via instructions <a href="#footnote-ref1" class="footnote-backref">↩︎</a></p></li><li id="footnote2" class="footnote-item"><p>OpenAI Training language models to follow instructions with human feedback <a href="#footnote-ref2" class="footnote-backref">↩︎</a></p></li><li id="footnote3" class="footnote-item"><p>Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers <a href="#footnote-ref3" class="footnote-backref">↩︎</a></p></li></ol></section>',8);function y(S,z){const o=i("ExternalLinkIcon");return s(),r("div",null,[g,l(" more "),_,t("p",null,[m,b,e(" （1）人工标注prompt集的答案用来finetune模型。监督微调(supervise fine-tuning, SFT)"),T,e(" 这一步得到的模型是davinci-instruct-beta"),x,e(" （2）通过对模型输出答案打分来训练reward model。Reward Model：基于第一步生成的SFT6B版本，去除最后一次反嵌入层。起到了扩充LLM模型高质量训练数据的作用。"),P,e(" 推理打分：选择了一部分prompt，由SFT模型随机生成多个答案（4-9个），人工对这些答案从到坏进行排序。这构成了一个新的监督训练数据集，排序是这些数据的label。新的数据集被用来训练reward模型。--"),t("a",G,[e("chatgpt是如何工作的"),c(o)]),k,e(" （3）使用reward model来更新ppo策略，从而使gpt产生的答案更偏向于标注人员的喜好。"),C,I,L,e(" 据推测，ChatGPT使用了和text-davinci-003相同的训练方法，采用了不同的数据集，而且更加注重生成答案的无害性和对话性"),w,e(" 合理分析：OpenAI官网的ChatGPT的训练流程和InstructGPT基本一致，除了ChatGPT是基于GPT3.5系列的，再根据InstructGPT发布后半年多才发布ChatGPT，推测是因为初始PPO策略训练的模型太过随心所欲，不能满足无害性等要求，而在调试的过程中GPT3.5系列已经训练完成，所以直接基于GPT3.5系列进行训练。")]),v])}const M=a(h,[["render",y],["__file","chatgpt.html.vue"]]);export{M as default};
