const t=JSON.parse('{"key":"v-a4e986e6","path":"/zh/posts/2023-06/LLM%E5%A6%82%E4%BD%95%E9%87%8D%E6%98%A0%E7%8E%B0%E5%AE%9E%E4%B8%96%E7%95%8C%EF%BC%88%E4%B8%80%EF%BC%89%20%E2%80%94%E2%80%94LLM%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E5%8E%8B%E7%BC%A9%E8%83%BD%E5%8A%9B%E4%B8%8E%E6%80%9D%E7%BB%B4%E5%9B%9E%E8%B7%AF%E5%81%87%E8%AE%BE.html","title":"","lang":"zh-CN","frontmatter":{"author":"太阳东升西落","icon":"pen-to-square","date":"2023-06-14T00:00:00.000Z","shortTitle":"LLM如何重映现实世界（一） ——LLM中的信息压缩能力与思维回路假设","category":["大语言模型"],"tag":["transformer","优化","字节"],"description":"LLM如何重映现实世界（一） ——LLM中的信息压缩能力与思维回路假设 知乎原文：https://zhuanlan.zhihu.com/p/632795115 版权归属原作者，如涉侵权，请联系删除 一种观点认为GPT 4 这种 LLM 模型仅仅学会了语言中的单词共现等浅层的表面统计关系，其实并未具备智能，只是类似鹦鹉学舌的语言片段缝合怪而已；另外一种观点则认为：GPT 4 不仅学会了语言元素间的表面统计关系，而且学到了人类语言甚至包括物理世界的内在运行规律，文字是由内在智能产生的，所以 LLM 具备类人智能。 一、预备知识 1.1 什么是NTP任务 目前规模够大的 LLM 模型，在训练基座模型的时候，都采用「Next Token Prediction，NTP」 (后文为了书写简洁，有时会简称为 NTP) 任务。Next Token Prediction 如此简单的操作，就是通过语言中前面的单词，来产生下一个单词 1.2 利用 LLM 进行数据压缩 如果大语言模型具备越强的数据压缩能力，是否意味着它具备越强的 AGI 智能呢？ 可以举个例子来解释这种数据压缩能力\\t 把LLM看做函数，根据已有的token，计算下一个token的在词表中的概率分布，根据输出的下一个token的概率分布进行算术编码，使用编码后的数据进行数据传输 1.3 压缩即智能 如果 GPT 模型智能程度越高，NTP 预测得越准确，则其压缩效率就越高。所以，我们可以根据模型的压缩效率来评估模型的智能程度，模型压缩效率越高，则模型智能程度越高，这是目前 OpenAI 照此思路推进大模型研发方向的一个核心理念。","head":[["meta",{"property":"og:url","content":"https://github.com/hust-404/hust-404.github.io/zh/posts/2023-06/LLM%E5%A6%82%E4%BD%95%E9%87%8D%E6%98%A0%E7%8E%B0%E5%AE%9E%E4%B8%96%E7%95%8C%EF%BC%88%E4%B8%80%EF%BC%89%20%E2%80%94%E2%80%94LLM%E4%B8%AD%E7%9A%84%E4%BF%A1%E6%81%AF%E5%8E%8B%E7%BC%A9%E8%83%BD%E5%8A%9B%E4%B8%8E%E6%80%9D%E7%BB%B4%E5%9B%9E%E8%B7%AF%E5%81%87%E8%AE%BE.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:description","content":"LLM如何重映现实世界（一） ——LLM中的信息压缩能力与思维回路假设 知乎原文：https://zhuanlan.zhihu.com/p/632795115 版权归属原作者，如涉侵权，请联系删除 一种观点认为GPT 4 这种 LLM 模型仅仅学会了语言中的单词共现等浅层的表面统计关系，其实并未具备智能，只是类似鹦鹉学舌的语言片段缝合怪而已；另外一种观点则认为：GPT 4 不仅学会了语言元素间的表面统计关系，而且学到了人类语言甚至包括物理世界的内在运行规律，文字是由内在智能产生的，所以 LLM 具备类人智能。 一、预备知识 1.1 什么是NTP任务 目前规模够大的 LLM 模型，在训练基座模型的时候，都采用「Next Token Prediction，NTP」 (后文为了书写简洁，有时会简称为 NTP) 任务。Next Token Prediction 如此简单的操作，就是通过语言中前面的单词，来产生下一个单词 1.2 利用 LLM 进行数据压缩 如果大语言模型具备越强的数据压缩能力，是否意味着它具备越强的 AGI 智能呢？ 可以举个例子来解释这种数据压缩能力\\t 把LLM看做函数，根据已有的token，计算下一个token的在词表中的概率分布，根据输出的下一个token的概率分布进行算术编码，使用编码后的数据进行数据传输 1.3 压缩即智能 如果 GPT 模型智能程度越高，NTP 预测得越准确，则其压缩效率就越高。所以，我们可以根据模型的压缩效率来评估模型的智能程度，模型压缩效率越高，则模型智能程度越高，这是目前 OpenAI 照此思路推进大模型研发方向的一个核心理念。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-14T07:41:02.000Z"}],["meta",{"property":"article:author","content":"太阳东升西落"}],["meta",{"property":"article:tag","content":"transformer"}],["meta",{"property":"article:tag","content":"优化"}],["meta",{"property":"article:tag","content":"字节"}],["meta",{"property":"article:published_time","content":"2023-06-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-14T07:41:02.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-14T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-14T07:41:02.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"太阳东升西落\\"}]}"]]},"headers":[],"git":{"createdTime":1686728462000,"updatedTime":1686728462000,"contributors":[{"name":"Sun","email":"57407733+shbone@users.noreply.github.com","commits":1}]},"readingTime":{"minutes":6.74,"words":2023},"filePathRelative":"zh/posts/2023-06/LLM如何重映现实世界（一） ——LLM中的信息压缩能力与思维回路假设.md","localizedDate":"2023年6月14日","excerpt":"<p>LLM如何重映现实世界（一）<br>\\n——LLM中的信息压缩能力与思维回路假设<br>\\n知乎原文：<a href=\\"https://zhuanlan.zhihu.com/p/632795115\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://zhuanlan.zhihu.com/p/632795115</a><br>\\n版权归属原作者，如涉侵权，请联系删除<br>\\n一种观点认为GPT 4 这种 LLM 模型仅仅学会了语言中的单词共现等浅层的表面统计关系，其实并未具备智能，只是类似鹦鹉学舌的语言片段缝合怪而已；另外一种观点则认为：GPT 4 不仅学会了语言元素间的表面统计关系，而且学到了人类语言甚至包括物理世界的内在运行规律，文字是由内在智能产生的，所以 LLM 具备类人智能。<br>\\n一、预备知识<br>\\n1.1 什么是NTP任务<br>\\n目前规模够大的 LLM 模型，在训练基座模型的时候，都采用「Next Token Prediction，NTP」 (后文为了书写简洁，有时会简称为 NTP) 任务。Next Token Prediction 如此简单的操作，就是通过语言中前面的单词，来产生下一个单词<br>\\n1.2 利用 LLM 进行数据压缩<br>\\n如果大语言模型具备越强的数据压缩能力，是否意味着它具备越强的 AGI 智能呢？<br>\\n可以举个例子来解释这种数据压缩能力\\t<br>\\n把LLM看做函数，根据已有的token，计算下一个token的在词表中的概率分布，根据输出的下一个token的概率分布进行算术编码，使用编码后的数据进行数据传输<br>\\n1.3 压缩即智能<br>\\n如果 GPT 模型智能程度越高，NTP 预测得越准确，则其压缩效率就越高。所以，我们可以根据模型的压缩效率来评估模型的智能程度，模型压缩效率越高，则模型智能程度越高，这是目前 OpenAI 照此思路推进大模型研发方向的一个核心理念。</p>","autoDesc":true}');export{t as data};
