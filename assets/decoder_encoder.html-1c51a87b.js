const e=JSON.parse('{"key":"v-1f5cb91e","path":"/zh/posts/llm/decoder_encoder.html","title":"基于Encoder和Decoder的三种架构","lang":"zh-CN","frontmatter":{"author":"最后的开神-wkyc","icon":"pen-to-square","date":"2023-07-06T00:00:00.000Z","category":["语言模型"],"tag":["Transformer"],"shortTitle":"Encoder和Decoder","description":"基于Encoder和Decoder的三种架构 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/decoder_encoder.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"基于Encoder和Decoder的三种架构"}],["meta",{"property":"og:description","content":"基于Encoder和Decoder的三种架构 Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-07-06T08:37:45.000Z"}],["meta",{"property":"article:author","content":"最后的开神-wkyc"}],["meta",{"property":"article:tag","content":"Transformer"}],["meta",{"property":"article:published_time","content":"2023-07-06T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-07-06T08:37:45.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"基于Encoder和Decoder的三种架构\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-06T00:00:00.000Z\\",\\"dateModified\\":\\"2023-07-06T08:37:45.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"最后的开神-wkyc\\"}]}"]]},"headers":[{"level":2,"title":"1 Encoder-Decoder","slug":"_1-encoder-decoder","link":"#_1-encoder-decoder","children":[]},{"level":2,"title":"1.1 T5","slug":"_1-1-t5","link":"#_1-1-t5","children":[{"level":3,"title":"1.2 ChatGLM","slug":"_1-2-chatglm","link":"#_1-2-chatglm","children":[]}]},{"level":2,"title":"2 Encoder-only","slug":"_2-encoder-only","link":"#_2-encoder-only","children":[]},{"level":2,"title":"3 Decoder-only","slug":"_3-decoder-only","link":"#_3-decoder-only","children":[{"level":3,"title":"3.1 GPT2","slug":"_3-1-gpt2","link":"#_3-1-gpt2","children":[]},{"level":3,"title":"3.2 Bloom","slug":"_3-2-bloom","link":"#_3-2-bloom","children":[]},{"level":3,"title":"3.3 Llama","slug":"_3-3-llama","link":"#_3-3-llama","children":[]}]},{"level":2,"title":"4 总结","slug":"_4-总结","link":"#_4-总结","children":[]}],"git":{"createdTime":1688628233000,"updatedTime":1688632665000,"contributors":[{"name":"RankKCodeTalker","email":"93417472+RankKCodeTalker@users.noreply.github.com","commits":2}]},"readingTime":{"minutes":2.81,"words":844},"filePathRelative":"zh/posts/llm/decoder_encoder.md","localizedDate":"2023年7月6日","excerpt":"<h1> 基于Encoder和Decoder的三种架构</h1>\\n<p>Transformer由论文《Attention is All You Need》提出，现在是谷歌云TPU推荐的参考模型。论文相关的Tensorflow的代码可以从GitHub获取，其作为Tensor2Tensor包的一部分。哈佛的NLP团队也实现了一个基于PyTorch的版本，并注释该论文。</p>\\n","autoDesc":true}');export{e as data};
