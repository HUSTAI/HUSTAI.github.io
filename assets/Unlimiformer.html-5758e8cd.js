const e=JSON.parse('{"key":"v-2e985ed2","path":"/zh/posts/2023-06/Unlimiformer.html","title":"Unlimiformer 介绍","lang":"zh-CN","frontmatter":{"author":"研究生鱼皮","icon":"pen-to-square","date":"2023-06-14T00:00:00.000Z","category":["大语言模型"],"tag":["摘要","Transformer","机器学习"],"sticky":10,"description":"Unlimiformer 介绍 原文链接 Transformer 是时下最强大的 seq2seq 架构。预训练 transformer 通常具有 512（例如 BERT）或 1024 个（例如 BART）token 的个上下文窗口，这对于目前许多文本摘要数据集（XSum、CNN/DM）来说是足够长的。 但 16384 并不是生成所需上下文长度的上限：涉及长篇叙事的任务，如书籍摘要（Krys-´cinski et al.，2021）或叙事问答（Kociskýet al.，2018），通常输入超过 10 万个 token。维基百科文章生成的挑战集（Liu*et al.，2018）包含超过 50 万个 token 的输入。生成式问答中的开放域任务可以从更大的输入中综合信息，例如回答关于维基百科上所有健在作者的文章的聚合属性的问题。图 1 根据常见的上下文窗口长度绘制了几个流行的摘要和问答数据集的大小；最长的输入比 Longformer 的上下文窗口长 34 倍以上。","head":[["meta",{"property":"og:url","content":"https://github.com/hust-404/hust-404.github.io/zh/posts/2023-06/Unlimiformer.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"Unlimiformer 介绍"}],["meta",{"property":"og:description","content":"Unlimiformer 介绍 原文链接 Transformer 是时下最强大的 seq2seq 架构。预训练 transformer 通常具有 512（例如 BERT）或 1024 个（例如 BART）token 的个上下文窗口，这对于目前许多文本摘要数据集（XSum、CNN/DM）来说是足够长的。 但 16384 并不是生成所需上下文长度的上限：涉及长篇叙事的任务，如书籍摘要（Krys-´cinski et al.，2021）或叙事问答（Kociskýet al.，2018），通常输入超过 10 万个 token。维基百科文章生成的挑战集（Liu*et al.，2018）包含超过 50 万个 token 的输入。生成式问答中的开放域任务可以从更大的输入中综合信息，例如回答关于维基百科上所有健在作者的文章的聚合属性的问题。图 1 根据常见的上下文窗口长度绘制了几个流行的摘要和问答数据集的大小；最长的输入比 Longformer 的上下文窗口长 34 倍以上。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-14T07:39:41.000Z"}],["meta",{"property":"article:author","content":"研究生鱼皮"}],["meta",{"property":"article:tag","content":"摘要"}],["meta",{"property":"article:tag","content":"Transformer"}],["meta",{"property":"article:tag","content":"机器学习"}],["meta",{"property":"article:published_time","content":"2023-06-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-14T07:39:41.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Unlimiformer 介绍\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-14T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-14T07:39:41.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"研究生鱼皮\\"}]}"]]},"headers":[],"git":{"createdTime":1686728381000,"updatedTime":1686728381000,"contributors":[{"name":"heiheiyoyo","email":"543425864@qq.com","commits":1}]},"readingTime":{"minutes":5.03,"words":1509},"filePathRelative":"zh/posts/2023-06/Unlimiformer.md","localizedDate":"2023年6月14日","excerpt":"<h1> Unlimiformer 介绍</h1>\\n<p><a href=\\"https://mp.weixin.qq.com/s/VktrpfEUK99Zrm3AJJwW-g\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">原文链接</a></p>\\n<p>Transformer 是时下最强大的 seq2seq 架构。预训练 transformer 通常具有 512（例如 BERT）或 1024 个（例如 BART）token 的个上下文窗口，这对于目前许多文本摘要数据集（XSum、CNN/DM）来说是足够长的。</p>\\n<p>但 16384 并不是生成所需上下文长度的上限：涉及长篇叙事的任务，如书籍摘要（Krys-´cinski et al.，2021）或叙事问答（Kociskýet al.，2018），通常输入超过 10 万个 token。维基百科文章生成的挑战集（Liu*et al.，2018）包含超过 50 万个 token 的输入。生成式问答中的开放域任务可以从更大的输入中综合信息，例如回答关于维基百科上所有健在作者的文章的聚合属性的问题。图 1 根据常见的上下文窗口长度绘制了几个流行的摘要和问答数据集的大小；最长的输入比 Longformer 的上下文窗口长 34 倍以上。</p>","autoDesc":true}');export{e as data};
