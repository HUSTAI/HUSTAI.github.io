const e=JSON.parse('{"key":"v-2f77b9dc","path":"/zh/posts/llm/Token-Crisis.html","title":"是重复还是不重复：在令牌危机下扩展LLM的见解","lang":"zh-CN","frontmatter":{"author":"研究生鱼皮-yjf","icon":"pen-to-square","date":"2023-05-31T00:00:00.000Z","shortTitle":"Token危机","category":["语言模型"],"tag":["模型","深度学习","机器学习"],"description":"是重复还是不重复：在令牌危机下扩展LLM的见解 新加坡国立大学的研究人员发布了一篇全新的论文《To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis》，研究了大语言模型的Epoch次数设置问题。文章讨论了在重复的数据集上进行多次训练对大语言模型性能的影响。作者指出，随着大语言模型的规模和训练数据集中Token数量的增加，模型性能受到很大的影响。然而，现有的数据集中的Token数量有限，模型参数规模的增长可能会导致Token不足的情况，被称为\\"Token危机\\"。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/Token-Crisis.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"是重复还是不重复：在令牌危机下扩展LLM的见解"}],["meta",{"property":"og:description","content":"是重复还是不重复：在令牌危机下扩展LLM的见解 新加坡国立大学的研究人员发布了一篇全新的论文《To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis》，研究了大语言模型的Epoch次数设置问题。文章讨论了在重复的数据集上进行多次训练对大语言模型性能的影响。作者指出，随着大语言模型的规模和训练数据集中Token数量的增加，模型性能受到很大的影响。然而，现有的数据集中的Token数量有限，模型参数规模的增长可能会导致Token不足的情况，被称为\\"Token危机\\"。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-08-11T01:55:12.000Z"}],["meta",{"property":"article:author","content":"研究生鱼皮-yjf"}],["meta",{"property":"article:tag","content":"模型"}],["meta",{"property":"article:tag","content":"深度学习"}],["meta",{"property":"article:tag","content":"机器学习"}],["meta",{"property":"article:published_time","content":"2023-05-31T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-08-11T01:55:12.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"是重复还是不重复：在令牌危机下扩展LLM的见解\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-05-31T00:00:00.000Z\\",\\"dateModified\\":\\"2023-08-11T01:55:12.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"研究生鱼皮-yjf\\"}]}"]]},"headers":[{"level":2,"title":"1 问题提出","slug":"_1-问题提出","link":"#_1-问题提出","children":[]},{"level":2,"title":"2 背景","slug":"_2-背景","link":"#_2-背景","children":[]},{"level":2,"title":"3 实验结论","slug":"_3-实验结论","link":"#_3-实验结论","children":[{"level":3,"title":"3.1 模型参数规模与Token数量需要匹配","slug":"_3-1-模型参数规模与token数量需要匹配","link":"#_3-1-模型参数规模与token数量需要匹配","children":[]},{"level":3,"title":"3.2 多轮Epoch的训练会降低模型性能","slug":"_3-2-多轮epoch的训练会降低模型性能","link":"#_3-2-多轮epoch的训练会降低模型性能","children":[]},{"level":3,"title":"3.3 更大规模的数据集会缓解重复Epoch对模型性能下降的影响","slug":"_3-3-更大规模的数据集会缓解重复epoch对模型性能下降的影响","link":"#_3-3-更大规模的数据集会缓解重复epoch对模型性能下降的影响","children":[]},{"level":3,"title":"3.4 提高数据集的质量也无法挽救重复训练带来的过拟合","slug":"_3-4-提高数据集的质量也无法挽救重复训练带来的过拟合","link":"#_3-4-提高数据集的质量也无法挽救重复训练带来的过拟合","children":[]},{"level":3,"title":"3.5参数数量和FLOPs在重复训练上的影响","slug":"_3-5参数数量和flops在重复训练上的影响","link":"#_3-5参数数量和flops在重复训练上的影响","children":[]},{"level":3,"title":"3.6 小计算量模型的过拟合趋势与大计算量的差不多","slug":"_3-6-小计算量模型的过拟合趋势与大计算量的差不多","link":"#_3-6-小计算量模型的过拟合趋势与大计算量的差不多","children":[]},{"level":3,"title":"3.7 多样的训练目标可以减轻多Epoch下降吗？","slug":"_3-7-多样的训练目标可以减轻多epoch下降吗","link":"#_3-7-多样的训练目标可以减轻多epoch下降吗","children":[]},{"level":3,"title":"3.8 Dropout是一个被大语言模型忽视的正则技术，虽然慢，但是可以降低多Epoch的影响","slug":"_3-8-dropout是一个被大语言模型忽视的正则技术-虽然慢-但是可以降低多epoch的影响","link":"#_3-8-dropout是一个被大语言模型忽视的正则技术-虽然慢-但是可以降低多epoch的影响","children":[]},{"level":3,"title":"3.9 在训练过程中逐渐使用Dropout是有效的策略","slug":"_3-9-在训练过程中逐渐使用dropout是有效的策略","link":"#_3-9-在训练过程中逐渐使用dropout是有效的策略","children":[]},{"level":3,"title":"3.10 Dropout对不同规模模型的影响不同","slug":"_3-10-dropout对不同规模模型的影响不同","link":"#_3-10-dropout对不同规模模型的影响不同","children":[]},{"level":3,"title":"3.11 通过MoE扫描确定稠密模型的最佳超参数","slug":"_3-11-通过moe扫描确定稠密模型的最佳超参数","link":"#_3-11-通过moe扫描确定稠密模型的最佳超参数","children":[]}]},{"level":2,"title":"4 总结","slug":"_4-总结","link":"#_4-总结","children":[]}],"git":{"createdTime":1688720971000,"updatedTime":1691718912000,"contributors":[{"name":"heiheiyoyo","email":"543425864@qq.com","commits":2}]},"readingTime":{"minutes":9.43,"words":2830},"filePathRelative":"zh/posts/llm/Token-Crisis.md","localizedDate":"2023年5月31日","excerpt":"<h1> 是重复还是不重复：在令牌危机下扩展LLM的见解</h1>\\n<p>新加坡国立大学的研究人员发布了一篇全新的论文《To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis》，研究了大语言模型的Epoch次数设置问题。文章讨论了在重复的数据集上进行多次训练对大语言模型性能的影响。作者指出，随着大语言模型的规模和训练数据集中Token数量的增加，模型性能受到很大的影响。然而，现有的数据集中的Token数量有限，模型参数规模的增长可能会导致Token不足的情况，被称为\\"Token危机\\"。</p>\\n","autoDesc":true}');export{e as data};
