const t=JSON.parse('{"key":"v-371487d6","path":"/zh/posts/2023-06/qlora.html","title":"让微调更高效","lang":"zh-CN","frontmatter":{"author":"wkyc","icon":"pen-to-square","date":"2023-06-12T00:00:00.000Z","category":["finetinue"],"tag":["lora","gpt"],"star":true,"description":"让微调更高效 低秩自适应（LoRA）技术可以提高微调的效率，可以在单个24GB显存GPU上对70亿参数LLaMA模型进行微调。而最近发布的一个新技术QLoRA（量化LoRA）可以在单个 48GB显存的GPU上训练650亿参数的LLaMA模型，量化的4位参数设置下，训练后得到的65B Guanaco模型保持了完整的16位微调任务性能，并且仅在微调24小时后就达到了ChatGPT性能的99.3%。","head":[["meta",{"property":"og:url","content":"https://github.com/hust-404/hust-404.github.io/zh/posts/2023-06/qlora.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"让微调更高效"}],["meta",{"property":"og:description","content":"让微调更高效 低秩自适应（LoRA）技术可以提高微调的效率，可以在单个24GB显存GPU上对70亿参数LLaMA模型进行微调。而最近发布的一个新技术QLoRA（量化LoRA）可以在单个 48GB显存的GPU上训练650亿参数的LLaMA模型，量化的4位参数设置下，训练后得到的65B Guanaco模型保持了完整的16位微调任务性能，并且仅在微调24小时后就达到了ChatGPT性能的99.3%。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-12T16:40:15.000Z"}],["meta",{"property":"article:author","content":"wkyc"}],["meta",{"property":"article:tag","content":"lora"}],["meta",{"property":"article:tag","content":"gpt"}],["meta",{"property":"article:published_time","content":"2023-06-12T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-12T16:40:15.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"让微调更高效\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-12T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-12T16:40:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"wkyc\\"}]}"]]},"headers":[],"git":{"createdTime":1686588015000,"updatedTime":1686588015000,"contributors":[{"name":"liuxiao","email":"liuxiao2103@qq.com","commits":1}]},"readingTime":{"minutes":0.5,"words":151},"filePathRelative":"zh/posts/2023-06/qlora.md","localizedDate":"2023年6月12日","excerpt":"<h1> 让微调更高效</h1>\\n<p>低秩自适应（LoRA）技术可以提高微调的效率，可以在单个24GB显存GPU上对70亿参数LLaMA模型进行微调。而最近发布的一个新技术QLoRA（量化LoRA）可以在单个 48GB显存的GPU上训练650亿参数的LLaMA模型，量化的4位参数设置下，训练后得到的65B Guanaco模型保持了完整的16位微调任务性能，并且仅在微调24小时后就达到了ChatGPT性能的99.3%。</p>\\n","autoDesc":true}');export{t as data};
