const e=JSON.parse('{"key":"v-2849110f","path":"/zh/posts/finetune/QLORA.html","title":"基于QLoRA微调大语言模型","lang":"zh-CN","frontmatter":{"author":"最后的开神-wkyc","icon":"boxes-packing","date":"2023-07-06T00:00:00.000Z","category":["微调技术"],"tag":["LLaMA","LoRA","LLM"],"description":"基于QLoRA微调大语言模型 LoRA的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。AdaLoRA是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。而本文要讲的QLoRA的核心思想就是在不降低任何性能的情况下微调量化为4 bit的模型。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/finetune/QLORA.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"基于QLoRA微调大语言模型"}],["meta",{"property":"og:description","content":"基于QLoRA微调大语言模型 LoRA的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。AdaLoRA是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。而本文要讲的QLoRA的核心思想就是在不降低任何性能的情况下微调量化为4 bit的模型。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-11-08T03:32:39.000Z"}],["meta",{"property":"article:author","content":"最后的开神-wkyc"}],["meta",{"property":"article:tag","content":"LLaMA"}],["meta",{"property":"article:tag","content":"LoRA"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2023-07-06T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-11-08T03:32:39.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"基于QLoRA微调大语言模型\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-06T00:00:00.000Z\\",\\"dateModified\\":\\"2023-11-08T03:32:39.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"最后的开神-wkyc\\"}]}"]]},"headers":[{"level":2,"title":"1 技术原理","slug":"_1-技术原理","link":"#_1-技术原理","children":[]},{"level":2,"title":"2 环境配置","slug":"_2-环境配置","link":"#_2-环境配置","children":[]},{"level":2,"title":"3 微调时显存占用","slug":"_3-微调时显存占用","link":"#_3-微调时显存占用","children":[]},{"level":2,"title":"4 权重合并推理","slug":"_4-权重合并推理","link":"#_4-权重合并推理","children":[]},{"level":2,"title":"5 推理时显存占用","slug":"_5-推理时显存占用","link":"#_5-推理时显存占用","children":[]},{"level":2,"title":"6 参考","slug":"_6-参考","link":"#_6-参考","children":[]}],"git":{"createdTime":1692602786000,"updatedTime":1699414359000,"contributors":[{"name":"Memory","email":"Memory455@163.com","commits":1},{"name":"RankKCodeTalker","email":"1073931273@qq.com","commits":1}]},"readingTime":{"minutes":4.92,"words":1476},"filePathRelative":"zh/posts/finetune/QLORA.md","localizedDate":"2023年7月6日","excerpt":"<h1> 基于QLoRA微调大语言模型</h1>\\n<p>LoRA的核心思想就是通过低秩分解来模拟参数的改变量，从而以极小的参数量来实现大模型的间接训练。AdaLoRA是对LoRA的一种改进，它根据重要性评分动态分配参数预算给权重矩阵。而本文要讲的QLoRA的核心思想就是在不降低任何性能的情况下微调量化为4 bit的模型。</p>\\n","autoDesc":true}');export{e as data};
