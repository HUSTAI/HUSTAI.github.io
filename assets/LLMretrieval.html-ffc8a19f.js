const e=JSON.parse('{"key":"v-083206d2","path":"/zh/posts/rag/LLMretrieval.html","title":"如何通过大模型实现外挂知识库优化","lang":"zh-CN","frontmatter":{"author":"猞猁-zlj","icon":"blog","date":"2023-09-07T00:00:00.000Z","shortTitle":"大模型外挂知识库优化","category":["rag"],"tag":["LLM","检索","rag"],"description":"如何通过大模型实现外挂知识库优化 大模型时代，通常采用向量召回的方式从文档库里召回和用户问题相关的文档片段，输入到LLM中来增强模型回答质量。本文分享两篇通过大模型的能力增强召回效果的文章，这两篇文章的内容都已经加入了langchain的标准组件，但是都有一些特定的使用场景。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/rag/LLMretrieval.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"如何通过大模型实现外挂知识库优化"}],["meta",{"property":"og:description","content":"如何通过大模型实现外挂知识库优化 大模型时代，通常采用向量召回的方式从文档库里召回和用户问题相关的文档片段，输入到LLM中来增强模型回答质量。本文分享两篇通过大模型的能力增强召回效果的文章，这两篇文章的内容都已经加入了langchain的标准组件，但是都有一些特定的使用场景。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-11-07T01:06:46.000Z"}],["meta",{"property":"article:author","content":"猞猁-zlj"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:tag","content":"检索"}],["meta",{"property":"article:tag","content":"rag"}],["meta",{"property":"article:published_time","content":"2023-09-07T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-11-07T01:06:46.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"如何通过大模型实现外挂知识库优化\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-09-07T00:00:00.000Z\\",\\"dateModified\\":\\"2023-11-07T01:06:46.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"猞猁-zlj\\"}]}"]]},"headers":[{"level":2,"title":"1 HYDE[1]","slug":"_1-hyde-1","link":"#_1-hyde-1","children":[{"level":3,"title":"1.1 框架介绍","slug":"_1-1-框架介绍","link":"#_1-1-框架介绍","children":[]},{"level":3,"title":"1.2 实验结果","slug":"_1-2-实验结果","link":"#_1-2-实验结果","children":[]}]},{"level":2,"title":"2 FLARE[2]","slug":"_2-flare-2","link":"#_2-flare-2","children":[{"level":3,"title":"2.1 策略1-让模型自己决定","slug":"_2-1-策略1-让模型自己决定","link":"#_2-1-策略1-让模型自己决定","children":[]},{"level":3,"title":"2.2 策略2-根据模型生成的token决定","slug":"_2-2-策略2-根据模型生成的token决定","link":"#_2-2-策略2-根据模型生成的token决定","children":[]}]},{"level":2,"title":"3 参考","slug":"_3-参考","link":"#_3-参考","children":[]}],"git":{"createdTime":1698735121000,"updatedTime":1699319206000,"contributors":[{"name":"sheli00","email":"44807582+sheli00@users.noreply.github.com","commits":2}]},"readingTime":{"minutes":6.6,"words":1979},"filePathRelative":"zh/posts/rag/LLMretrieval.md","localizedDate":"2023年9月7日","excerpt":"<h1> 如何通过大模型实现外挂知识库优化</h1>\\n<p>大模型时代，通常采用向量召回的方式从文档库里召回和用户问题相关的文档片段，输入到LLM中来增强模型回答质量。本文分享两篇通过大模型的能力增强召回效果的文章，这两篇文章的内容都已经加入了langchain的标准组件，但是<strong>都有一些特定的使用场景</strong>。</p>\\n","autoDesc":true}');export{e as data};
