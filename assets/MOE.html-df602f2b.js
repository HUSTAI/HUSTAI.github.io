const e=JSON.parse('{"key":"v-401cc49c","path":"/zh/posts/llm/MOE.html","title":"混合专家模型","lang":"zh-CN","frontmatter":{"author":"最后的开神-wkyc","icon":"pen-to-square","date":"2023-07-13T00:00:00.000Z","shortTitle":"混合专家模型","category":["语言模型"],"tag":["模型架构"],"description":"混合专家模型 混合专家模型（Mixture-of-Experts，MoE）为由许多独立网络组成的系统提出了一种新的监督学习过程，每个网络都学习处理完整训练案例集的子集。新过程可以被视为多层监督网络的模块化版本，也可以被视为竞争性学习的关联版本。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/MOE.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"混合专家模型"}],["meta",{"property":"og:description","content":"混合专家模型 混合专家模型（Mixture-of-Experts，MoE）为由许多独立网络组成的系统提出了一种新的监督学习过程，每个网络都学习处理完整训练案例集的子集。新过程可以被视为多层监督网络的模块化版本，也可以被视为竞争性学习的关联版本。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-07-14T03:06:53.000Z"}],["meta",{"property":"article:author","content":"最后的开神-wkyc"}],["meta",{"property":"article:tag","content":"模型架构"}],["meta",{"property":"article:published_time","content":"2023-07-13T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-07-14T03:06:53.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"混合专家模型\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-13T00:00:00.000Z\\",\\"dateModified\\":\\"2023-07-14T03:06:53.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"最后的开神-wkyc\\"}]}"]]},"headers":[{"level":2,"title":"1 专家的适应性混合","slug":"_1-专家的适应性混合","link":"#_1-专家的适应性混合","children":[]},{"level":2,"title":"2 稀疏门控混合专家","slug":"_2-稀疏门控混合专家","link":"#_2-稀疏门控混合专家","children":[{"level":3,"title":"2.1 稀疏门控","slug":"_2-1-稀疏门控","link":"#_2-1-稀疏门控","children":[]},{"level":3,"title":"2.2 token级别","slug":"_2-2-token级别","link":"#_2-2-token级别","children":[]},{"level":3,"title":"2.3 专家平衡","slug":"_2-3-专家平衡","link":"#_2-3-专家平衡","children":[]}]},{"level":2,"title":"3 GShard：Transformer中的MoE","slug":"_3-gshard-transformer中的moe","link":"#_3-gshard-transformer中的moe","children":[]}],"git":{"createdTime":1689239038000,"updatedTime":1689304013000,"contributors":[{"name":"RankKCodeTalker","email":"1073931273@qq.com","commits":2}]},"readingTime":{"minutes":4.65,"words":1395},"filePathRelative":"zh/posts/llm/MOE.md","localizedDate":"2023年7月13日","excerpt":"<h1> 混合专家模型</h1>\\n<p>混合专家模型（Mixture-of-Experts，MoE）为由许多独立网络组成的系统提出了一种新的监督学习过程，每个网络都学习处理完整训练案例集的子集。新过程可以被视为多层监督网络的模块化版本，也可以被视为竞争性学习的关联版本。</p>\\n","autoDesc":true}');export{e as data};
