const e=JSON.parse('{"key":"v-6e4a6b67","path":"/zh/posts/llm/RLpolicy.html","title":"机器学习之强化学习中的策略学习","lang":"zh-CN","frontmatter":{"author":"猞猁-zlj","icon":"pen-to-square","date":"2023-03-28T00:00:00.000Z","shortTitle":"强化学习之策略学习","title":"机器学习之强化学习中的策略学习","category":["语言模型"],"tag":["Reinforcement Learning","Policy-based","OpenAI"],"description":"基于价值的（Policy-Based）方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/RLpolicy.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"机器学习之强化学习中的策略学习"}],["meta",{"property":"og:description","content":"基于价值的（Policy-Based）方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-07-08T03:54:40.000Z"}],["meta",{"property":"article:author","content":"猞猁-zlj"}],["meta",{"property":"article:tag","content":"Reinforcement Learning"}],["meta",{"property":"article:tag","content":"Policy-based"}],["meta",{"property":"article:tag","content":"OpenAI"}],["meta",{"property":"article:published_time","content":"2023-03-28T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-07-08T03:54:40.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"机器学习之强化学习中的策略学习\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-03-28T00:00:00.000Z\\",\\"dateModified\\":\\"2023-07-08T03:54:40.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"猞猁-zlj\\"}]}"]]},"headers":[{"level":2,"title":"1 策略梯度算法","slug":"_1-策略梯度算法","link":"#_1-策略梯度算法","children":[{"level":3,"title":"1.1 算法核心思想","slug":"_1-1-算法核心思想","link":"#_1-1-算法核心思想","children":[]},{"level":3,"title":"1.2 评价标准","slug":"_1-2-评价标准","link":"#_1-2-评价标准","children":[]}]},{"level":2,"title":"2 优势演员-评论家算法","slug":"_2-优势演员-评论家算法","link":"#_2-优势演员-评论家算法","children":[]},{"level":2,"title":"3. TRPO","slug":"_3-trpo","link":"#_3-trpo","children":[]},{"level":2,"title":"4. PPO","slug":"_4-ppo","link":"#_4-ppo","children":[]},{"level":2,"title":"参考","slug":"参考","link":"#参考","children":[]}],"git":{"createdTime":1688788480000,"updatedTime":1688788480000,"contributors":[{"name":"sheli00","email":"44807582+sheli00@users.noreply.github.com","commits":1}]},"readingTime":{"minutes":4.09,"words":1227},"filePathRelative":"zh/posts/llm/RLpolicy.md","localizedDate":"2023年3月28日","excerpt":"<p>基于价值的（Policy-Based）方法直接输出下一步动作的概率，根据概率来选取动作。但不一定概率最高就会选择该动作，还是会从整体进行考虑。适用于非连续和连续的动作。常见的方法有Policy gradients。</p>\\n","autoDesc":true}');export{e as data};
