const t=JSON.parse('{"key":"v-6246dfa8","path":"/zh/posts/llm/GPT2.html","title":"GPT2论文分享与架构分析","lang":"zh-CN","frontmatter":{"author":"最后的开神-wkyc","icon":"pen-to-square","date":"2023-07-05T00:00:00.000Z","category":["语言模型"],"tag":["GPT"],"description":"GPT2论文分享与架构分析 GPT-2 模型由多层单向 Transformer 的解码器部分构成，本质上是自回归模型，自回归的意思是指，每次产生新单词后，将新单词加到原输入句后面，作为新的输入句。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/GPT2.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"GPT2论文分享与架构分析"}],["meta",{"property":"og:description","content":"GPT2论文分享与架构分析 GPT-2 模型由多层单向 Transformer 的解码器部分构成，本质上是自回归模型，自回归的意思是指，每次产生新单词后，将新单词加到原输入句后面，作为新的输入句。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-07-09T11:32:47.000Z"}],["meta",{"property":"article:author","content":"最后的开神-wkyc"}],["meta",{"property":"article:tag","content":"GPT"}],["meta",{"property":"article:published_time","content":"2023-07-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-07-09T11:32:47.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"GPT2论文分享与架构分析\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-07-05T00:00:00.000Z\\",\\"dateModified\\":\\"2023-07-09T11:32:47.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"最后的开神-wkyc\\"}]}"]]},"headers":[{"level":2,"title":"1 语言建模","slug":"_1-语言建模","link":"#_1-语言建模","children":[]},{"level":2,"title":"2 模型架构","slug":"_2-模型架构","link":"#_2-模型架构","children":[]},{"level":2,"title":"3 模型架构解析","slug":"_3-模型架构解析","link":"#_3-模型架构解析","children":[{"level":3,"title":"3.1 LN","slug":"_3-1-ln","link":"#_3-1-ln","children":[]},{"level":3,"title":"3.2 Multi-head Self-Attention","slug":"_3-2-multi-head-self-attention","link":"#_3-2-multi-head-self-attention","children":[]},{"level":3,"title":"3.3 GPT2Attention","slug":"_3-3-gpt2attention","link":"#_3-3-gpt2attention","children":[]},{"level":3,"title":"3.4 参数量计算","slug":"_3-4-参数量计算","link":"#_3-4-参数量计算","children":[]}]}],"git":{"createdTime":1688902367000,"updatedTime":1688902367000,"contributors":[{"name":"Liu Xiao","email":"42756849+liuxiaocs7@users.noreply.github.com","commits":1}]},"readingTime":{"minutes":5.66,"words":1699},"filePathRelative":"zh/posts/llm/GPT2.md","localizedDate":"2023年7月5日","excerpt":"<h1> GPT2论文分享与架构分析</h1>\\n<p>GPT-2 模型由多层单向 Transformer 的解码器部分构成，本质上是自回归模型，自回归的意思是指，每次产生新单词后，将新单词加到原输入句后面，作为新的输入句。</p>\\n","autoDesc":true}');export{t as data};
