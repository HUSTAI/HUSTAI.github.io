import{_ as t}from"./plugin-vue_export-helper-c27b6911.js";import{o as e,c as n,e as l,a,b as s,f as i}from"./app-80a6685a.js";const r="/assets/images/llm/rlvalue1.png",m="/assets/images/llm/rlvalue2.png",o="/assets/images/llm/rlvalue3.png",p="/assets/images/llm/rlvalue4.png",c={},h=a("p",null,"基于价值的（Value-Based）方法输出的是动作的价值，选择价值最高的动作，也就是通过价值选动作。价值学习经典的算法有Sarsa和Q-learning算法。",-1),g=a("h2",{id:"_1-sarsa",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#_1-sarsa","aria-hidden":"true"},"#"),s(" 1 SARSA")],-1),d=a("figure",null,[a("img",{src:r,alt:"Sarsa伪代码",height:"250",tabindex:"0",loading:"lazy"}),a("figcaption",null,"图1.1 Sarsa伪代码")],-1),u=a("p",null,[s("SARSA（State-Action-Reward-State-Action）是一个学习马尔科夫决策过程策略的算法，从名称我们可以看出其学习更新函数依赖的5个值"),a("span",{class:"katex"},[a("span",{class:"katex-mathml"},[a("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[a("semantics",null,[a("mrow",null,[a("mo",{stretchy:"false"},"("),a("mrow",null,[a("mi",{mathvariant:"normal"},"s"),a("mo",{separator:"true"},","),a("mi",{mathvariant:"normal"},"a"),a("mo",{separator:"true"},","),a("mi",{mathvariant:"normal"},"r"),a("mo",{separator:"true"},","),a("msup",null,[a("mi",{mathvariant:"normal"},"s"),a("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),a("mo",{separator:"true"},","),a("msup",null,[a("mi",{mathvariant:"normal"},"a"),a("mo",{mathvariant:"normal",lspace:"0em",rspace:"0em"},"′")]),a("mo",{stretchy:"false"},")")])]),a("annotation",{encoding:"application/x-tex"},"(\\mathrm{s,a,r,s^{\\prime},a^{\\prime})}")])])]),a("span",{class:"katex-html","aria-hidden":"true"},[a("span",{class:"base"},[a("span",{class:"strut",style:{height:"1.0019em","vertical-align":"-0.25em"}}),a("span",{class:"mopen"},"("),a("span",{class:"mord"},[a("span",{class:"mord mathrm"},"s"),a("span",{class:"mpunct"},","),a("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),a("span",{class:"mord mathrm"},"a"),a("span",{class:"mpunct"},","),a("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),a("span",{class:"mord mathrm"},"r"),a("span",{class:"mpunct"},","),a("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),a("span",{class:"mord"},[a("span",{class:"mord mathrm"},"s"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.7519em"}},[a("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mtight"},[a("span",{class:"mord mathrm mtight"},"′")])])])])])])])]),a("span",{class:"mpunct"},","),a("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),a("span",{class:"mord"},[a("span",{class:"mord mathrm"},"a"),a("span",{class:"msupsub"},[a("span",{class:"vlist-t"},[a("span",{class:"vlist-r"},[a("span",{class:"vlist",style:{height:"0.7519em"}},[a("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[a("span",{class:"pstrut",style:{height:"2.7em"}}),a("span",{class:"sizing reset-size6 size3 mtight"},[a("span",{class:"mord mtight"},[a("span",{class:"mord mathrm mtight"},"′")])])])])])])])]),a("span",{class:"mclose"},")")])])])]),s("。SARSA是on-policy的强化学习方法，目标策略与行为策略保持一致。")],-1),f=i('<figure><img src="'+m+'" alt="Sarsa策略更新" tabindex="0" loading="lazy"><figcaption>图1.2 Sarsa策略更新</figcaption></figure><p>根据状态图可以理解SARSA的更新规则。</p><h2 id="_2-q-learning" tabindex="-1"><a class="header-anchor" href="#_2-q-learning" aria-hidden="true">#</a> 2 Q-learning</h2><figure><img src="'+o+'" alt="Q-learning伪代码" height="250" tabindex="0" loading="lazy"><figcaption>图2.1 Q-learning伪代码</figcaption></figure><p>Q-learning同样根据下一步的状态更新Q值，和SARSA的区别在于直接用下一步的最大Q值作为估计来更新。</p><figure><img src="'+p+'" alt="Q-learning策略更新" tabindex="0" loading="lazy"><figcaption>图2.2 Q-learning策略更新</figcaption></figure><h2 id="_3-on-policy和off-policy" tabindex="-1"><a class="header-anchor" href="#_3-on-policy和off-policy" aria-hidden="true">#</a> 3 on-policy和off-policy</h2><p>最后来明确下on-policy和off-policy的概念。强化学习包含两个策略，行为策略，智能体遵循该策略选择动作。与之相对的目标策略是我们优化的对象，也是强化学习模型推断时使用的策略。</p><p>SARSA的目标策略是优化Q值，根据公式我们知道SARSA是通过预估下一步的收益来更新自身的Q值，而且下一步是按照行为策略选出的，所以它的目标策略与行为策略保持一致，我们称SARSA是on-policy算法。</p><p>而Q-learning算法的目标策略是优化下一步的Q表中的最大值，目标策略与行为策略并不一致，我们称Q-learning是off-policy算法。</p><p>简单来说，就是看行为策略和目标策略是否相同。</p>',11);function _(y,S){return e(),n("div",null,[h,l(" more "),g,d,u,f])}const x=t(c,[["render",_],["__file","RLvalue.html.vue"]]);export{x as default};
