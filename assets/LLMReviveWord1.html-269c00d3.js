const e=JSON.parse('{"key":"v-44293e6e","path":"/zh/posts/llm/LLMReviveWord1.html","title":"LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享","lang":"zh-CN","frontmatter":{"author":"shb","icon":"pen-to-square","date":"2023-06-14T00:00:00.000Z","shortTitle":"LLM如何重映现实世界（一）","category":["语言模型"],"tag":["LLM"],"description":"LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享 本文主要分享的内容为以下两点。 (1) LLM的信息压缩能力与其智能水平的关系 (2) GPT对知识的提取与存储方式","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/LLMReviveWord1.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享"}],["meta",{"property":"og:description","content":"LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享 本文主要分享的内容为以下两点。 (1) LLM的信息压缩能力与其智能水平的关系 (2) GPT对知识的提取与存储方式"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-07-08T13:36:30.000Z"}],["meta",{"property":"article:author","content":"shb"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:published_time","content":"2023-06-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-07-08T13:36:30.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-14T00:00:00.000Z\\",\\"dateModified\\":\\"2023-07-08T13:36:30.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"shb\\"}]}"]]},"headers":[{"level":2,"title":"1 预备知识","slug":"_1-预备知识","link":"#_1-预备知识","children":[{"level":3,"title":"1.1 什么是NTP任务","slug":"_1-1-什么是ntp任务","link":"#_1-1-什么是ntp任务","children":[]},{"level":3,"title":"1.2 利用 LLM 进行数据压缩","slug":"_1-2-利用-llm-进行数据压缩","link":"#_1-2-利用-llm-进行数据压缩","children":[]},{"level":3,"title":"1.3 压缩即智能","slug":"_1-3-压缩即智能","link":"#_1-3-压缩即智能","children":[]}]},{"level":2,"title":"2 GPT 模型对知识的提取过程","slug":"_2-gpt-模型对知识的提取过程","link":"#_2-gpt-模型对知识的提取过程","children":[]},{"level":2,"title":"3 知识点在 Transformer 中的分布","slug":"_3-知识点在-transformer-中的分布","link":"#_3-知识点在-transformer-中的分布","children":[]}],"git":{"createdTime":1688823390000,"updatedTime":1688823390000,"contributors":[{"name":"Sun","email":"57407733+shbone@users.noreply.github.com","commits":1}]},"readingTime":{"minutes":7.25,"words":2175},"filePathRelative":"zh/posts/llm/LLMReviveWord1.md","localizedDate":"2023年6月14日","excerpt":"<h1> LLM如何重映现实世界（一）：LLM的信息压缩能力与知识存储方式分享</h1>\\n<p>本文主要分享的内容为以下两点。<br>\\n(1) LLM的信息压缩能力与其智能水平的关系<br>\\n(2) GPT对知识的提取与存储方式</p>\\n","autoDesc":true}');export{e as data};
