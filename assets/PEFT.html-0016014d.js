const t=JSON.parse('{"key":"v-6676e606","path":"/zh/posts/finetune/PEFT.html","title":"PEFT：最先进的参数高效微调方法","lang":"zh-CN","frontmatter":{"author":"hyb","icon":"boxes-packing","date":"2023-06-13T00:00:00.000Z","shortTitle":"PEFT","category":["微调技术"],"tag":["PEFT","Hugging Face","LoRA","AdaLoRA","Prefix Tuning","P-Tuning","Prompt Tuning"],"description":"PEFT：最先进的参数高效微调方法 参数高效微调 （PEFT） 方法能够将预训练的语言模型 （PLM） 有效地适应各种下游应用程序，而无需微调模型的所有参数。微调大型 PLM 的成本通常高得令人望而却步。在这方面，PEFT方法仅微调少量（额外）模型参数，从而大大降低了计算和存储成本。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/finetune/PEFT.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"PEFT：最先进的参数高效微调方法"}],["meta",{"property":"og:description","content":"PEFT：最先进的参数高效微调方法 参数高效微调 （PEFT） 方法能够将预训练的语言模型 （PLM） 有效地适应各种下游应用程序，而无需微调模型的所有参数。微调大型 PLM 的成本通常高得令人望而却步。在这方面，PEFT方法仅微调少量（额外）模型参数，从而大大降低了计算和存储成本。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-19T08:33:31.000Z"}],["meta",{"property":"article:author","content":"hyb"}],["meta",{"property":"article:tag","content":"PEFT"}],["meta",{"property":"article:tag","content":"Hugging Face"}],["meta",{"property":"article:tag","content":"LoRA"}],["meta",{"property":"article:tag","content":"AdaLoRA"}],["meta",{"property":"article:tag","content":"Prefix Tuning"}],["meta",{"property":"article:tag","content":"P-Tuning"}],["meta",{"property":"article:tag","content":"Prompt Tuning"}],["meta",{"property":"article:published_time","content":"2023-06-13T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-19T08:33:31.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PEFT：最先进的参数高效微调方法\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-13T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-19T08:33:31.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"hyb\\"}]}"]]},"headers":[{"level":2,"title":"1 PEFT定义","slug":"_1-peft定义","link":"#_1-peft定义","children":[]},{"level":2,"title":"2 PEFT分类","slug":"_2-peft分类","link":"#_2-peft分类","children":[{"level":3,"title":"2.1 LoRA","slug":"_2-1-lora","link":"#_2-1-lora","children":[]},{"level":3,"title":"2.2 AdaLoRA","slug":"_2-2-adalora","link":"#_2-2-adalora","children":[]},{"level":3,"title":"2.3 prompt分类","slug":"_2-3-prompt分类","link":"#_2-3-prompt分类","children":[]},{"level":3,"title":"2.4 Prefix Tuning","slug":"_2-4-prefix-tuning","link":"#_2-4-prefix-tuning","children":[]},{"level":3,"title":"2.5 Prompt Tuning","slug":"_2-5-prompt-tuning","link":"#_2-5-prompt-tuning","children":[]},{"level":3,"title":"2.6 P-Tuning","slug":"_2-6-p-tuning","link":"#_2-6-p-tuning","children":[]},{"level":3,"title":"2.7 各类提示微调对比","slug":"_2-7-各类提示微调对比","link":"#_2-7-各类提示微调对比","children":[]}]},{"level":2,"title":"3 实验结果","slug":"_3-实验结果","link":"#_3-实验结果","children":[]},{"level":2,"title":"4 参考文章","slug":"_4-参考文章","link":"#_4-参考文章","children":[]}],"git":{"createdTime":1687095795000,"updatedTime":1687163611000,"contributors":[{"name":"Yibo He","email":"1137195420@qq.com","commits":4},{"name":"cs1137195420","email":"1137195420@qq.com","commits":1}]},"readingTime":{"minutes":13.84,"words":4153},"filePathRelative":"zh/posts/finetune/PEFT.md","localizedDate":"2023年6月13日","excerpt":"<h1> PEFT：最先进的参数高效微调方法</h1>\\n<p>参数高效微调 （PEFT） 方法能够将预训练的语言模型 （PLM） 有效地适应各种下游应用程序，而无需微调模型的所有参数。微调大型 PLM 的成本通常高得令人望而却步。在这方面，PEFT方法仅微调少量（额外）模型参数，从而大大降低了计算和存储成本。</p>\\n","autoDesc":true}');export{t as data};
