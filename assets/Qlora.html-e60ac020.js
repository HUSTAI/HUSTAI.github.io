const t=JSON.parse('{"key":"v-ff41ede2","path":"/zh/posts/finetune/Qlora.html","title":"让微调更高效","lang":"zh-CN","frontmatter":{"author":"最后的开神-wkyc","icon":"boxes-packing","date":"2023-06-12T00:00:00.000Z","category":["微调技术"],"tag":["lora","gpt"],"description":"让微调更高效 低秩自适应（LoRA）技术可以提高微调的效率，可以在单个24GB显存GPU上对70亿参数LLaMA模型进行微调。而最近发布的一个新技术QLoRA（量化LoRA）可以在单个 48GB显存的GPU上训练650亿参数的LLaMA模型，量化的4位参数设置下，训练后得到的65B Guanaco模型保持了完整的16位微调任务性能，并且仅在微调24小时后就达到了ChatGPT性能的99.3%。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/finetune/Qlora.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"让微调更高效"}],["meta",{"property":"og:description","content":"让微调更高效 低秩自适应（LoRA）技术可以提高微调的效率，可以在单个24GB显存GPU上对70亿参数LLaMA模型进行微调。而最近发布的一个新技术QLoRA（量化LoRA）可以在单个 48GB显存的GPU上训练650亿参数的LLaMA模型，量化的4位参数设置下，训练后得到的65B Guanaco模型保持了完整的16位微调任务性能，并且仅在微调24小时后就达到了ChatGPT性能的99.3%。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-16T02:23:19.000Z"}],["meta",{"property":"article:author","content":"最后的开神-wkyc"}],["meta",{"property":"article:tag","content":"lora"}],["meta",{"property":"article:tag","content":"gpt"}],["meta",{"property":"article:published_time","content":"2023-06-12T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-16T02:23:19.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"让微调更高效\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-12T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-16T02:23:19.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"最后的开神-wkyc\\"}]}"]]},"headers":[],"git":{"createdTime":1686823822000,"updatedTime":1686882199000,"contributors":[{"name":"Liu Xiao","email":"42756849+liuxiaocs7@users.noreply.github.com","commits":2}]},"readingTime":{"minutes":0.53,"words":158},"filePathRelative":"zh/posts/finetune/Qlora.md","localizedDate":"2023年6月12日","excerpt":"<h1> 让微调更高效</h1>\\n<p>低秩自适应（LoRA）技术可以提高微调的效率，可以在单个24GB显存GPU上对70亿参数LLaMA模型进行微调。而最近发布的一个新技术QLoRA（量化LoRA）可以在单个 48GB显存的GPU上训练650亿参数的LLaMA模型，量化的4位参数设置下，训练后得到的65B Guanaco模型保持了完整的16位微调任务性能，并且仅在微调24小时后就达到了ChatGPT性能的99.3%。</p>\\n","autoDesc":true}');export{t as data};
