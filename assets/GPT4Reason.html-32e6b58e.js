import{_ as n}from"./plugin-vue_export-helper-c27b6911.js";import{r,o as t,c as s,e as o,a,b as i,d as g,f as p}from"./app-7c842960.js";const l="/assets/images/llm/gpt4reason1.png",d="/assets/images/llm/gpt4reason2.png",c="/assets/images/llm/gpt4reason3.png",h="/assets/images/llm/gpt4reason4.png",f="/assets/images/llm/gpt4reason5.png",_="/assets/images/llm/gpt4reason6.png",u="/assets/images/llm/gpt4reason7.png",m="/assets/images/llm/gpt4reason8.png",b="/assets/images/llm/gpt4reason9.png",P="/assets/images/llm/gpt4reason10.png",T="/assets/images/llm/gpt4reason11.png",G="/assets/images/llm/gpt4reason12.png",x="/assets/images/llm/gpt4reason13.png",L="/assets/images/llm/gpt4reason14.png",A={},M=a("p",null,[i("今年三月，OpenAI重磅发布了"),a("strong",null,"GPT-4"),i("大模型，带来了比GPT-3.5更强的推理、计算、逻辑能力。然而8月7日Konstantine Arkoudas撰写了一篇标题为"),a("strong",null,"GPT-4 Can't Reason"),i("的预印本论文，在业界引起轩然大波。该论文得出结论："),a("strong",null,"尽管GPT-4偶尔会闪现出分析的才华，但它目前是完全无法推理的"),i("。而另一篇来自UCLA和华盛顿大学的研究也发现，GPT-4在"),a("strong",null,"大学的数学、物理、化学"),i("任务的推理上，表现不佳。")],-1),C={href:"https://www.preprints.org/manuscript/202308.0148/v1",target:"_blank",rel:"noopener noreferrer"},y=p('<h2 id="_1-什么是推理" tabindex="-1"><a class="header-anchor" href="#_1-什么是推理" aria-hidden="true">#</a> 1 什么是推理？</h2><p>其实在今年一月初，论文作者 Konstantine Arkoudas 就在 Medium 平台上分享了一篇有关 ChatGPT 的非正式评估，评估涉及的学科非常广泛，包括传统 NLU、民间物理、信息检索、心理理论、空间推理、简单逻辑推理和数学。<br> 当时其得到的主要结论是：ChatGPT 是一项开创性的突破；基于 LLM 的系统并不只是“随机鹦鹉”，而是建立了真正的抽象，并能展现创造力；这类系统将带来大量令人兴奋的新应用；尽管取得了上述的成就，但这些系统在推理能力上仍然受到严重限制。<br> 在他看来，如今升级版的 GPT-4 依然如此，甚至完全没有推理能力。<br> 在论文中，Konstantine Arkoudas 指出，业界关于“LLM 是否有推理能力”的争论已经持续了很长时间。</p><ul><li>一方面，是 LLM 支持派。他们对大模型美好推理能力预测往往会依赖不断变化的“定律”，而这些所谓的“定律”，Konstantine Arkoudas 认为，实际上就是站不住脚的经验证据、大量有问题的建模假设、理解不清的概念（LLM 特性），以及甚至包含一点教条信念，即在庞大的语料库中最大限度地减少下一个标记预测的交叉熵损失，就能通过迁移学习的魔力和通用高级表征的构建，提供一个通用的推理引擎。</li><li>另一方面，则是 LLM 怀疑派。他们往往有着严谨的论据，但是这些论点大多是基于过往经验和分析，有些含糊不清(例如，LLM 缺乏“世界模型”，即关于世界如何运作的内部模型)。</li></ul><p>基于这两方面考虑，Konstantine Arkoudas 认为，对于可靠的鲁棒 LLM 推理的合理性，最令人信服的先验考虑是计算复杂性的结果。推理是一个非常难以计算的问题。事实上，在一般情况下，它在算法上是不可判定的。<br> Konstantine Arkoudas 表示，“任何 LLM，无论规模有多大，经过多么广泛和巧都无法破解任意推理问题。这与机器学习中著名的 &quot;没有免费的午餐&quot;定理是一致的，后者指出了模型通用性与性能之间类似的反比关系”。<br> 因此，为了验证“GPT-4 是否具有推理能力”，首先要做的是统一理念，即什么是推理，以及判定推理能力所采用的具体方法。<br> 对于<strong>推理的定义</strong>，Konstantine Arkoudas 表示，<strong>「推理不是不择手段地得出正确的答案，而是根据正确的理由得出正确的答案。」</strong><br> 更准确地说，推理是提出论点，更重要的是证明论点的过程。一个论证包括一个结论和一系列前提，结论就是由这些前提推导出来的。前提代表了为论证目的而被视为既定的信息，即使只是暂时的。结论和前提通常是陈述句，用自然语言或符号逻辑的符号来表达，可真可假，但也可以用图表等其他符号来表示。如果 S 中的所有句子都为真，则 p 为真，在这种情况下，这个论点被认为是有效的。<br> 对于方法论，Konstantine Arkoudas 在论文中所采用的评估不是基于一个语料库或一组语料库。相反，其对 GPT-4 在广泛领域的 21 个简单推理问题上的性能进行了详细的定性分析，其中大部分是从头开始编写的，而其余的则经过手动调整，使模型不易识别它们，这样做的部分原因也是为了避免数据污染。</p><h2 id="_2-用测试问题验证-gpt-4-的推理性" tabindex="-1"><a class="header-anchor" href="#_2-用测试问题验证-gpt-4-的推理性" aria-hidden="true">#</a> 2 用测试问题验证 GPT-4 的推理性</h2><h3 id="_2-1-简单算术" tabindex="-1"><a class="header-anchor" href="#_2-1-简单算术" aria-hidden="true">#</a> 2.1 简单算术</h3><p>Konstantine Arkoudas 表示，执行基本算术运算的能力是通用推理的必要组成部分，尤其是在科学和工程应用领域。为了确保 GPT-4 不会死记硬背，他提出了让 GPT-4 在其选择的范围内随机选择两个随机整数，然后对选择的值执行操作。</p><figure><img src="'+l+'" alt="图2.1 简单算术测试结果" tabindex="0" loading="lazy"><figcaption>图2.1 简单算术测试结果</figcaption></figure><p>但实际上，正确答案是1385*1432=1983320。<br> 事实证明，GPT-4 仍然无法可靠地执行基本算术运算，如加法和乘法。</p><h3 id="_2-2-简单计数" tabindex="-1"><a class="header-anchor" href="#_2-2-简单计数" aria-hidden="true">#</a> 2.2 简单计数</h3><p>给 GPT-4 一个命题变量，在它前面有 27 个否定符号，并要求它计算否定的数量。对于人类来说，这是个很容易的任务，尤其是因为否定符号是分五块写的，每块有五个小点，最后是一对否定符号，但是 GPT-4 的表现如何呢？</p><figure><img src="'+d+'" alt="图2.2 简单计数测试结果" tabindex="0" loading="lazy"><figcaption>图2.2 简单计数测试结果</figcaption></figure><p>根据结果，GPT-4多数了几个否定符号带来的差别似乎并不严重，直到我们意识到它在逻辑输入上的所有差别，正如 GPT-4 自己的解释所强调的那样。即使在明确告诉 GPT-4 要慢慢来、仔细数的情况下，多次重复这个实验也得到了大相径庭的结果。</p><h3 id="_2-3-常识性问题" tabindex="-1"><a class="header-anchor" href="#_2-3-常识性问题" aria-hidden="true">#</a> 2.3 常识性问题</h3><figure><img src="'+c+'" alt="图2.3 常识性问题测试结果" tabindex="0" loading="lazy"><figcaption>图2.3 常识性问题测试结果</figcaption></figure><p>在目前的情况下，其实可以将常识论证视为从给定信息加上未说明的前提得出的直接推导结论，这些前提构成了普遍接受的背景知识。在这种特殊情况下，这种常识性知识就是 &quot;人在死前是活着的，死后就不会再活着 &quot;这样的命题。GPT-4竟回答：根据所提供的信息，无法确定Mable中午是否还活着。</p><h3 id="_2-4-初级逻辑" tabindex="-1"><a class="header-anchor" href="#_2-4-初级逻辑" aria-hidden="true">#</a> 2.4 初级逻辑</h3><p>如果P(x)包含Q(x)，而Q(a)不成立，那么我们就可以根据模型推论出P(a)也不成立（因为如果P(a)成立，那么Q(a)也会成立）。<br> 这是一个最基本的同义反复，但GPT-4却完全提出一个反模型：</p><figure><img src="'+h+'" alt="图2.4 初级逻辑测试结果" tabindex="0" loading="lazy"><figcaption>图2.4 初级逻辑测试结果</figcaption></figure><p>仅仅几句话之后， GPT-4就声称P(x)在给定的解释下确实蕴含Q(x)，这与它自己之前的说法相矛盾。<br> 说明， GPT-4还会出现内部不一致的问题。</p><h3 id="_2-5-简单量词语义" tabindex="-1"><a class="header-anchor" href="#_2-5-简单量词语义" aria-hidden="true">#</a> 2.5 简单量词语义</h3><figure><img src="'+f+'" alt="图2.5 简单量词语义测试结果" tabindex="0" loading="lazy"><figcaption>图2.5 简单量词语义测试结果</figcaption></figure><p>显然，这三个句子都是共同可满足的，一个简单的模型是具有P(a1)、Q(a1)、¬P(a2) 和 ¬Q(a2)的域{a1, a2}，然而GPT-4得出的结论确与之相反。</p><h3 id="_2-6-子集和" tabindex="-1"><a class="header-anchor" href="#_2-6-子集和" aria-hidden="true">#</a> 2.6 子集和</h3><p>S = {2, 8, 6, 32, 22, 44, 28, 12, 18, 10, 14}。那么S有多少个子集的总和是37？<br> 这个问题中，S的子集都是偶数，而偶数之和不可能是奇数，因此答案为0。然而，GPT-4没有停下来考虑S包含的内容，而是转用编程的方式解决。</p><figure><img src="'+_+'" alt="图2.6 子集和测试结果" tabindex="0" loading="lazy"><figcaption>图2.6 子集和测试结果</figcaption></figure><h3 id="_2-7-积木世界" tabindex="-1"><a class="header-anchor" href="#_2-7-积木世界" aria-hidden="true">#</a> 2.7 积木世界</h3><p>这是一个简单的推理任务，需要对倒数第三个积木B3进行案例分析。<br> 首先，B3要么是绿色的，要么不是。<br> 如果是绿色的，那么B3就在非绿色积木B4的上面，所以结论成立。<br> 如果不是，那么从上数的第二个绿色积木B2，就在非绿色积木B3上面，因此结论仍然成立。<br> 然而，结果显示，GPT-4的表现并不理想。</p><figure><img src="'+u+'" alt="图2.7 积木世界测试结果" tabindex="0" loading="lazy"><figcaption>图2.7 积木世界测试结果</figcaption></figure><h3 id="_2-8-谋杀还是自杀" tabindex="-1"><a class="header-anchor" href="#_2-8-谋杀还是自杀" aria-hidden="true">#</a> 2.8 谋杀还是自杀</h3><p>作者构思了一个逻辑谜题，列出了9个条件要求GPT-4找出真正杀害Agatha姨妈的凶手。</p><figure><img src="'+m+'" alt="图2.8 谋杀还是自杀测试结果" tabindex="0" loading="lazy"><figcaption>图2.8 谋杀还是自杀测试结果</figcaption></figure><p>正确的答案是Agatha姨妈杀了自己。<br> GPT-4做出的另一个关键错误是：由于Agatha姨妈讨厌所有除管家以外的人（条件5），这意味着她至少不讨厌她自己。<br> 这是一个奇怪的错误，从第5个条件就可以得出Agatha姨妈讨厌她自己。</p><h3 id="_2-9-wason选择问题" tabindex="-1"><a class="header-anchor" href="#_2-9-wason选择问题" aria-hidden="true">#</a> 2.9 Wason选择问题</h3><p>Wason 选择任务是推理心理学的主要内容。</p><figure><img src="'+b+'" alt="图2.9 Wason选择问题测试结果" tabindex="0" loading="lazy"><figcaption>图2.9 Wason选择问题测试结果</figcaption></figure><p>事实上，只有 16、红色和绿色需要翻转。因此，在精确度方面，这些回答再次表明，GPT-4 并不理解物质条件式的语义。这再次说明了这些例子中出现的另一个重要主题：GPT-4 的回答，无论对错，往往都存在内在的不一致。</p><h2 id="_3-推理测试结论" tabindex="-1"><a class="header-anchor" href="#_3-推理测试结论" aria-hidden="true">#</a> 3 推理测试结论</h2><p>最终种种验证无疑证明了 GPT-4 推理能力的惨淡画面。<br> 结果表明，该模型存在内部不一致性、不能正确应用基本推理技术和缺乏对推理中起基础性作用的概念(如物质条件)的理解等问题。<br> 但是现实中，这些问题往往归纳为大模型带来的误差与“幻觉”，实则其实是它不具备推理能力。<br> 鉴于 GPT-4 是目前最有能力的 LLM，Konstantine Arkoudas 从这些发现中得出三个主要结论：</p><p>1）在软件开发(或一般的科学和工程)中使用生成式人工智能来完成乏味的任务(作为一种针对知识密集型编码问题的涡轮增压自动补全)之外的任何任务都充满了严重的风险。正确性的规范标准是至关重要的，在这些领域，目前的 LLM 不能满足这样的标准。就像生成人工智能已经开始用糟糕的广告污染网络一样，它有可能大规模地增加 Bug 代码。<br> 2）如果 LLM 推理继续改进，严格的证明检查就可能变得越来越重要。对于应用程序来说，对系统推理的正确性有信心是必不可少的，尤其是在科学、医学和工程领域，而验证检查是一种能够提供这种信任的技术。这种方法可以通过要求 LLMS 将其推理正规化(用易于验证检查的符号表示法来表示)，或者可能通过培训其他 LLMS 检查用自然语言表示的一段推理来实现。<br> 3）就目前情况来看，反乌托邦的场景涉及一个让人类屈服的流氓人工智能，甚至其他人类使用人工智能来达到邪恶的目的，是非常牵强的。当最先进的人工智能系统在空间推理过程中甚至无法区分左右时，行业中还有那么多呼吁制定政策和机构来保护人类免受其 AI 侵害的做法显然是不成熟的。</p><h2 id="_4-大学数理化-gpt-4得分35-8" tabindex="-1"><a class="header-anchor" href="#_4-大学数理化-gpt-4得分35-8" aria-hidden="true">#</a> 4 大学数理化，GPT-4得分35.8%</h2><p>UCLA的研究中，主要评估了GPT-4，以及GPT-3.5在数学、化学、物理方面的推理能力。<br> 当前，为了增强LLM解决数学等任务的能力，有人提出了思维连CoT策略，指导大模型逐步生成答案，从而更深入思考问题。<br> 然而，即使这样的方法有其特定的优势，也难以完全解决复杂的科学问题。<br> 如下，是大学物理化学的一个示例问题，以及在两种提示策略下生成的解决方案。<br> 有CoT加持的GPT-4出现明显的计算错误，而提示用Python作为外部工具的GPT-4，也会误解数学方程。</p><figure><img src="'+P+'" alt="图4.1 大学物理化学的一个示例问题" tabindex="0" loading="lazy"><figcaption>图4.1 大学物理化学的一个示例问题</figcaption></figure><p>对此，研究中引入了一个大学水平的科学问题基准SCIBENCH。<br> 其中，「开放数据集」包括从大学课程广泛使用的教科书中收集的5个问题，涵盖了基础物理、热力学、经典力学、量子化学、物理化学、微积分、统计学和微分方程。</p><figure><img src="'+T+'" alt="图4.2 开放教科书问题摘要" tabindex="0" loading="lazy"><figcaption>图4.2 开放教科书问题摘要</figcaption></figure><p>另一个是「封闭数据集」，为了模拟真实世界的评估，其中包含了计算机科学和数学三门大学课程的7套期中和期末考试题。</p><figure><img src="'+G+'" alt="图4.3 封闭考试数据集" tabindex="0" loading="lazy"><figcaption>图4.3 封闭考试数据集</figcaption></figure><p>与现有基准不同，SCIBENCH中的所有问题都是，开放式、自由回答的问题。<br> 数据集中有了，研究重点评估了两个具有代表性的LLM，GPT-3.5和GPT-4，并采用了不同的提示策略，包括CoT、零样本学习、少样本学习。<br> 另外，研究人员还提示模型使用外部工具，比如Python和Wolfram语言。<br> 实验结果表明，在没有任何复杂提示、或使用外部工具的情况下，GPT-3.5和GPT-4在开放数据集中平均准确率分别为10.62%和16.81%。<br> 那么，在加入CoT和外部工具后，在同一数据集上最高准确率也仅仅是35.8%。不过，相较之前，很大程度提高了准确率。</p><figure><img src="'+x+'" alt="图4.4 开放数据集中准确率的结果" tabindex="0" loading="lazy"><figcaption>图4.4 开放数据集中准确率的结果</figcaption></figure><p>在使用CoT提示+外部工具最强配置下，GPT-4在开放式数据集上取得了35.80%的平均分，在封闭数据集上取得了51.57%的平均分。<br> 这些结果表明，在未来的LLM中，GPT-4有相当大的改进潜力。</p><figure><img src="'+L+'" alt="图4.5 考试数据集上的实验结果" tabindex="0" loading="lazy"><figcaption>图4.5 考试数据集上的实验结果</figcaption></figure><p>最后，通过分析发现：</p><ul><li>虽然CoT显著提高了计算能力，但在其他方面的效果较差；</li><li>使用外部工具的提示可能会损害其他基本技能；</li><li>少样本学习并不能普遍提高科学问题解决能力。<br> 总之，研究结果表明，当前大型语言模型在解决问题能力方面依旧很弱，并且在各种工具帮助下，依旧存在局限性。</li></ul>',53);function k(z,B){const e=r("ExternalLinkIcon");return t(),s("div",null,[M,o(" more "),a("p",null,[i("论文地址："),a("a",C,[i("https://www.preprints.org/manuscript/202308.0148/v1"),g(e)])]),y])}const w=n(A,[["render",k],["__file","GPT4Reason.html.vue"]]);export{w as default};
