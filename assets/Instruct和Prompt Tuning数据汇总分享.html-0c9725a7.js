import{_ as o}from"./plugin-vue_export-helper-c27b6911.js";import{r,o as s,c as i,a as t,b as n,d as l}from"./app-f81d8395.js";const u={},c=t("h1",{id:"instruct-prompt-tuning数据",tabindex:"-1"},[t("a",{class:"header-anchor",href:"#instruct-prompt-tuning数据","aria-hidden":"true"},"#"),n(" Instruct/Prompt Tuning数据")],-1),a=t("br",null,null,-1),h=t("br",null,null,-1),_={href:"https://instructions.apps.allenai.org/",target:"_blank",rel:"noopener noreferrer"},d=t("br",null,null,-1),p={href:"https://github.com/bigscience-workshop/promptsource",target:"_blank",rel:"noopener noreferrer"},g=t("br",null,null,-1),b=t("br",null,null,-1),f=t("br",null,null,-1),m={href:"https://huggingface.co/datasets/bigscience/P3",target:"_blank",rel:"noopener noreferrer"},k=t("br",null,null,-1),S=t("br",null,null,-1),C=t("br",null,null,-1),I=t("br",null,null,-1),P=t("br",null,null,-1),G=t("br",null,null,-1),L={href:"https://huggingface.co/datasets/bigscience/P3",target:"_blank",rel:"noopener noreferrer"},T=t("br",null,null,-1),A=t("br",null,null,-1),B={href:"https://github.com/anthropics/hh-rlhf",target:"_blank",rel:"noopener noreferrer"},H=t("br",null,null,-1),x=t("br",null,null,-1),M=t("br",null,null,-1),v=t("br",null,null,-1),F=t("br",null,null,-1),N=t("br",null,null,-1),z={href:"https://huggingface.co/datasets/Anthropic/hh-rlhf",target:"_blank",rel:"noopener noreferrer"},w=t("br",null,null,-1),E=t("br",null,null,-1),y={href:"https://arxiv.org/pdf/2204.05862.pdf",target:"_blank",rel:"noopener noreferrer"},R=t("br",null,null,-1),U=t("br",null,null,-1),D=t("br",null,null,-1),K=t("br",null,null,-1),V=t("br",null,null,-1),J={href:"https://github.com/yizhongw/self-instruct",target:"_blank",rel:"noopener noreferrer"},O=t("br",null,null,-1),Q=t("br",null,null,-1),j=t("br",null,null,-1),q=t("br",null,null,-1),W={href:"https://unifiedskg.com/",target:"_blank",rel:"noopener noreferrer"},X=t("p",null,"解决问题：做打破彼此任务之间的边界的第一次简单尝试，使得这些可以在同一个UnifiedSKG framework下进行学习并在这些任务上取得不错的结果",-1),Y=t("br",null,null,-1),Z={href:"https://github.com/google-research/FLAN/tree/main/flan/v2",target:"_blank",rel:"noopener noreferrer"},$=t("br",null,null,-1),tt=t("br",null,null,-1),nt=t("br",null,null,-1),et={href:"https://github.com/prakharguptaz/Instructdial/tree/main/datasets",target:"_blank",rel:"noopener noreferrer"},lt=t("br",null,null,-1),ot=t("br",null,null,-1),rt=t("br",null,null,-1),st={href:"https://github.com/tatsu-lab/stanford_alpaca",target:"_blank",rel:"noopener noreferrer"},it=t("br",null,null,-1),ut=t("br",null,null,-1),ct={href:"https://baike.baidu.com/item/%E6%97%A7%E9%87%91%E5%B1%B1/29211?fromModule=lemma_inlink%EF%BC%8C",target:"_blank",rel:"noopener noreferrer"},at=t("br",null,null,-1),ht=t("br",null,null,-1),_t=t("br",null,null,-1),dt=t("br",null,null,-1),pt=t("br",null,null,-1),gt=t("br",null,null,-1),bt=t("br",null,null,-1),ft={href:"https://link.zhihu.com/?target=https%3A//github.com/salesforce/CodeGen",target:"_blank",rel:"noopener noreferrer"},mt=t("br",null,null,-1),kt=t("br",null,null,-1),St=t("br",null,null,-1),Ct=t("br",null,null,-1),It=t("br",null,null,-1),Pt=t("br",null,null,-1),Gt=t("br",null,null,-1),Lt=t("br",null,null,-1),Tt={href:"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2204.05999",target:"_blank",rel:"noopener noreferrer"},At=t("br",null,null,-1),Bt=t("br",null,null,-1);function Ht(xt,Mt){const e=r("ExternalLinkIcon");return s(),i("div",null,[c,t("p",null,[n("1.Super-Natural Instruction 【Allen AI】"),a,n(" 这些自然语言指令清楚而完整地描述了一项任务（传统上定义为将输入字符串映射到输出字符串）。配备“理解”语言说明的模型，如果提供了任务说明，应该可以成功解决任何看不见的任务"),h,n(" 项目链接："),t("a",_,[n("https://instructions.apps.allenai.org/"),l(e)])]),t("p",null,[n("2.PromptSource【BigScience】"),d,n(" 项目链接："),t("a",p,[n("https://github.com/bigscience-workshop/promptsource"),l(e)]),g,n(" BigScience 由 Hugging Face 和法国 CNRS，IDRIS，GENCI 等联合组织，是当下最大的开源 LLMs 组织之一。"),b,n(" BigScience 在 2021 年末开发了PromptSource项目，开源了一系列工具 toolkits，帮助研究者基于现有NLP 任务构建 prompt。截止目前，PromptSource 项目包含了 270 个 NLP 任务的超过 2000 个 prompt 模版：")]),t("p",null,[n("3.P3【BigScience】"),f,n(" 项目链接："),t("a",m,[n("https://huggingface.co/datasets/bigscience/P3"),l(e)]),k,n(" 语言：英文"),S,n(" 在promptsource基础上，BigScience 构建了 P3 数据集。在 Hugging Face Hub 上你可以找到 P3 数据，P3 的数据规模在 100M-1B 之间："),C,n(" 4.xMTF 【BigScience，包含中文】"),I,n(" BigScience 在英语 prompt 的基础上，扩展其 prompt 到多种非英语语言。"),P,n(" 该项目包含了 13 个 NLP 任务，并采用了 46 个不同的语言的版本。对应的 prompt 包含的语种个数不定。"),G,n(" 项目链接："),t("a",L,[n("https://huggingface.co/datasets/bigscience/P3"),l(e)]),T,n(" 5.HH-RLHF【Anthropic】"),A,n(" 项目链接："),t("a",B,[n("https://github.com/anthropics/hh-rlhf"),l(e)]),H,n(" 数量："),x,n(" 训练集：161k"),M,n(" 测试集：8.55k"),v,n(" Anthropic 公司旗下的 Claud 是 ChatGPT 的主要竞品之一。"),F,n(" Anthropic 开源了其在自己产品线中使用的 RLHF 数据集："),N,t("a",z,[n("https://huggingface.co/datasets/Anthropic/hh-rlhf"),l(e)]),w,n(" HH-RLHF 项目的初衷在于训练 Helpful and Harmless（HH）的 LLMs。故该项目除了回复质量外，是否为有害信息也体现在了其 human feedback 中："),E,t("a",y,[n("https://arxiv.org/pdf/2204.05862.pdf"),l(e)]),R,n(" 6.Unnatural Instruction【orhonovich】"),U,n(" 使用 LLMs 自主生成 instruction 数据是 instruct-tuning 领域较为活跃的一个方向。"),D,n(" Unnatural Instruction 使用 GPT3（text-davinci-002）生成了 64k 的 instruction prompt 数据。并使用同样的模型将 64k 的 prompt 进行改写，最终得到了 240k 条 instruction 数据。"),K,n(" 论文中显示，在 Instruct-Tuning 中 LLMs 自主生成的 prompt 表现出了良好的效果，甚至超过了在 P3 等数据上进行微调的 T0 等模型。"),V,n(" 7.Self-Instruct【yizhongw】")]),t("p",null,[n("项目链接："),t("a",J,[n("https://github.com/yizhongw/self-instruct"),l(e)]),O,n(" Self-Instruct 同样是使用 LLMs 生成 prompt 进行 instruct-tuning 的思路。不过使用了更 fine-grained 的生成流程。"),Q,n(" Task pool 和 Quality filtering 等概念被引入，部分缓解了 self-intrauct 类型数据的 noise 问题")]),t("p",null,[n("8.UnifiedSKG 【HKU】"),j,n(" UnifiedSKG 在 Text-to-Text 的框架中加入了 knowledge grounding，也就是在 prompt-output 的框架中，加入了结构化数据做辅助，共21个任务数据集，"),q,n(" 项目主页 ："),t("a",W,[n("https://unifiedskg.com/"),l(e)])]),X,t("p",null,[n("9.Flan Collection【Google】"),Y,n(" 项目链接："),t("a",Z,[n("https://github.com/google-research/FLAN/tree/main/flan/v2"),l(e)]),$,n(" Google 在这个项目中将自己的 Flan 2021 数据与一些开源的 instruction 数据（P3，super-natural instruction 等）进行了合并"),tt,n(" 10.InstructDial【prakharguptaz】"),nt,t("a",et,[n("https://github.com/prakharguptaz/Instructdial/tree/main/datasets"),l(e)]),lt,n(" InstructDial 是在特定的一种任务类型上进行指令微调的尝试。实验结果表明，在对话指令数据上微调后，模型在对话任务上的表现强于在超大规模任务集上的结果"),ot,n(" 11.*Alpaca 【Stanford】"),rt,n(" 项目链接:"),t("a",st,[n("https://github.com/tatsu-lab/stanford_alpaca"),l(e)])]),t("p",null,[n("开源可训练的代码生成模型"),it,n(" 1.CodeT5【Saleforce】"),ut,n(" Salesforce是创建于1999年3月的一家客户关系管理(CRM) 软件服务提供商，总部设于美国"),t("a",ct,[n("https://baike.baidu.com/item/旧金山/29211?fromModule=lemma_inlink，"),l(e)]),at,n(" 2021年9月，Saleforce公布了CodeT5模型。目前，Saleforce公开了4个版本的CodeT5模型，均开源可获得。"),ht,n(" CodeT5-small：0.6亿参数"),_t,n(" CodeT5-base：2.2亿参数"),dt,n(" CodeT5-large：7.7亿参数"),pt,n(" CodeT5-large-ntp-py：7.7亿参数"),gt,n(" Saleforce的CodeGen/CodeGen2"),bt,t("a",ft,[n("https://link.zhihu.com/?target=https%3A//github.com/salesforce/CodeGen"),l(e)]),mt,n(" 2022年5月，Saleforce再次发布了一个新的编程模型CodeGen。该模型是一系列模型，参数有4个版本：3.5亿、20亿、60亿和160亿。而训练的数据也有三个："),kt,n(" nl版本：基于Pile数据训练"),St,n(" multi版本：在nl基础上继续在多个编程语言组成的数据集上训练"),Ct,n(" mono版本：继续在multi版本上基于Python代码数据训练"),It,n(" 上述12个模型全部在HuggingFace上开源。"),Pt,n(" 2023年5月3日，Saleforce开源第二代CodeGen：CodeGen2发布。该系列模型包含4个版本，分别是10亿参数、37亿参数、70亿参数和160亿参数四个版本。CodeGen2可以进行infilling，并且支持更多的编程语言。这里的infilling应该是在插入代码的含义。"),Gt,n(" CodeGen2也是全部开源，其中160亿参数版本开源文件大小66GB左右~")]),t("p",null,[n("2.InCoder【MetaAI】"),Lt,t("a",Tt,[n("https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2204.05999"),l(e)]),At,n(" InCoder是MetaAI在2022年4月发布的一个编程大模型。模型训练数据仅包含来自在线来源（如GitHub、GitLab和StackOverflow）的开放许可代码（Apache 2.0、MIT、BSD-2和BSD-3许可），其中重点是Python和JavaScript，但总共包括28种语言 - 总共约200GB的数据。公开的模型预训练结果共2个版本，一个是67亿参数一个是13亿参数。"),Bt,n(" 尽管InCoder的训练数据都是开放许可代码数据，但是MetaAI的InCoder模型确实开源的不可商用的！")])])}const Nt=o(u,[["render",Ht],["__file","Instruct和Prompt Tuning数据汇总分享.html.vue"]]);export{Nt as default};
