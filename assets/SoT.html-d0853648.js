import{_ as i}from"./plugin-vue_export-helper-c27b6911.js";import{r as t,o as p,c as r,a as s,b as a,d as l,e as o,f as n}from"./app-0c1d9c21.js";const c="/assets/images/prompt/SoT1.png",h="/assets/images/prompt/SoT2.png",g="/assets/images/prompt/SoT3.png",u="/assets/images/prompt/SoT4.png",d="/assets/images/prompt/SoT5.png",x="/assets/images/prompt/SoT6.png",y="/assets/images/prompt/SoT7.png",_="/assets/images/prompt/SoT8.png",f="/assets/images/prompt/SoT9.png",b="/assets/images/prompt/SoT10.png",T={},L=s("h1",{id:"skeleton-of-thought-思维骨架",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#skeleton-of-thought-思维骨架","aria-hidden":"true"},"#"),a(" Skeleton-of-Thought: 思维骨架")],-1),S={href:"https://mp.weixin.qq.com/s/9t1opfhUYm3yJuEoKPvVuQ",target:"_blank",rel:"noopener noreferrer"},w=s("strong",null,"思维骨架",-1),M=s("p",null,[a("由于当前先进的LLM采用了"),s("strong",null,"顺序解码"),a("方式，即一次生成一个词语或短语。然而，这种顺序解码可能花费较长生成时间，特别是在处理复杂任务时，会增加系统的延迟。受人类思考和写作过程的启发，来自清华微软的研究人员提出了「思维骨架」（SoT），以减少大模型的端到端的生成延迟。")],-1),v=s("p",null,[a("核心思想：SoT引导LLM，首先生成答案的"),s("strong",null,"骨架"),a("，然后进行并行API调用或分批解码，并行完成每个骨架点的内容。SoT不仅大大提高了速度，在11个不同的LLM中可达2.39倍，而且还可能在多样性和相关性方面提高多个问题类别的答案质量。研究人员称，SoT是以数据为中心优化效率的初步尝试，揭示了推动LLM更像人类一样思考答案质量的潜力。")],-1),k=n('<hr><h2 id="_1-sot-让大模型并行解码" tabindex="-1"><a class="header-anchor" href="#_1-sot-让大模型并行解码" aria-hidden="true">#</a> 1 SoT，让大模型并行解码</h2><h3 id="_1-1-背景" tabindex="-1"><a class="header-anchor" href="#_1-1-背景" aria-hidden="true">#</a> 1.1 背景</h3><p>目前，最先进的LLM的推理过程依旧缓慢，交互能力大大减分。LLM推理慢的3个主要原因：</p><p>（1）大模型需要大量内存，内存访问和计算。比如，GPT-3的FP16权重需要350 GB内存，这意味着仅推理就需要5×80GB A100 GPU。即使有足够多的GPU，繁重的内存访问和计算也会降低推理（以及训练）的速度。<br> （2）主流Transformer架构中的核心注意力操作受I/O约束，其内存和计算复杂度与序列长度成二次方关系。<br> （3）推理中的顺序解码方法逐个生成token，其中每个token都依赖于先前生成的token。这种方法会带来很大的推理延迟，因为token的生成无法并行化。</p><p>先前的研究中，大多将重点放在<strong>大模型规模</strong>，以及<strong>注意力操作</strong>上。这次，研究团队展示了，现成LLM<strong>并行解码</strong>的可行性，而无需对其模型、系统或硬件进行任何改动。</p><blockquote><p>研究人员可以通过Slack使用Claude模型将延迟时间从22秒，减少到12秒（快了1.83倍），通过A100上的Vicuna-33B V1.3将延迟时间从43秒减少到16秒（快了2.69倍）。</p></blockquote><h3 id="_1-2-思路" tabindex="-1"><a class="header-anchor" href="#_1-2-思路" aria-hidden="true">#</a> 1.2 思路</h3><p>这个想法，来源于对人类自身如何回答问题的思考。对于我们来讲，并不总是按顺序思考问题，并写出答案。相反，对于许多类型的问题，首先根据一些策略推导出骨架，然后添加细节来细化和说明每一点。那么，这一点在提供咨询、参加考试、撰写论文等正式场合中，更是如此。我们能够让LLM以同样的方式思考吗？为此，研究人员提出了「思维骨架」（SoT）。具体来说，</p><figure><img src="'+c+'" alt="图1.1 SoT框架" width="650" tabindex="0" loading="lazy"><figcaption>图1.1 SoT框架</figcaption></figure><p>（1）引导LLM首先自己推导出一个骨架。<br> （2）在骨架的基础上，LLM可以并行地完成每个点，从而提高速度。SoT既可用于加速分批解码的开源模型，也可用于加速并行API调用的闭源模型。<br> （3）最后，研究人员在最近发布的11个LLM上测试SoT。结果显示，SoT不仅提供了相当大的加速度（最高可达2.39倍） ，而且它还可以在多样性和相关性方面提高几个问题类别的答案质量。</p><figure><img src="'+h+'" alt="图1.2 SoT效果" width="680" tabindex="0" loading="lazy"><figcaption>图1.2 SoT效果</figcaption></figure><h3 id="_1-3-sot框架" tabindex="-1"><a class="header-anchor" href="#_1-3-sot框架" aria-hidden="true">#</a> 1.3 SoT框架</h3><p>（1）骨架阶段。</p>',14),z=s("p",null,[a("SoT首先使用骨架提示模版"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"T"),s("mi",null,"s")])]),s("annotation",{encoding:"application/x-tex"},"T^{s}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"T"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s")])])])])])])])])])])]),a("，以问题"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"q")]),s("annotation",{encoding:"application/x-tex"},"q")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.625em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q")])])]),a("为参数，组装一个骨架请求 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"T"),s("mi",null,"s")]),s("mo",{stretchy:"false"},"("),s("mi",null,"q"),s("mi",null,"u"),s("mi",null,"e"),s("mi",null,"s"),s("mi",null,"t"),s("mi",null,"i"),s("mi",null,"o"),s("mi",null,"n"),s("mo",null,"="),s("mi",null,"q"),s("mo",{stretchy:"false"},")")]),s("annotation",{encoding:"application/x-tex"},"T^{s}(question = q)")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"T"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s")])])])])])])])]),s("span",{class:"mopen"},"("),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"es"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"i"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03588em"}},"q"),s("span",{class:"mclose"},")")])])]),a(" 。编写骨架提示模板是为了引导LLM输出"),s("strong",null,"简洁的答案骨架"),a("。然后，研究人员从LLM的骨架答案 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"R"),s("mi",null,"s")])]),s("annotation",{encoding:"application/x-tex"},"R^{s}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s")])])])])])])])])])])]),a(" 中提取B点。")],-1),P=s("p",null,"（2）点扩展阶段",-1),B=s("p",null,[a("基于骨架，让LLM在每个点上平行展开。具体地说，对于带有索引 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mi",null,"b")]),s("annotation",{encoding:"application/x-tex"},"b")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6944em"}}),s("span",{class:"mord mathnormal"},"b")])])]),a(" 和骨架 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msubsup",null,[s("mi",null,"R"),s("mi",null,"b"),s("mi",null,"s")])]),s("annotation",{encoding:"application/x-tex"},"R^{s}_{b}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9664em","vertical-align":"-0.2831em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-2.4169em","margin-left":"-0.0077em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"b")])])]),s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2831em"}},[s("span")])])])])])])])]),a(" 的点，SoT使用 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"R"),s("mi",null,"s")]),s("mo",{separator:"true"},","),s("mi",null,"p"),s("mi",null,"o"),s("mi",null,"i"),s("mi",null,"n"),s("mi",null,"t"),s("mtext",null," "),s("mi",null,"i"),s("mi",null,"n"),s("mi",null,"d"),s("mi",null,"e"),s("mi",null,"x"),s("mo",null,"="),s("mi",null,"b"),s("mo",{separator:"true"},","),s("mi",null,"p"),s("mi",null,"o"),s("mi",null,"i"),s("mi",null,"n"),s("mi",null,"t"),s("mtext",null," "),s("mi",null,"s"),s("mi",null,"k"),s("mi",null,"e"),s("mi",null,"l"),s("mi",null,"e"),s("mi",null,"t"),s("mi",null,"o"),s("mi",null,"n"),s("mo",null,"="),s("msubsup",null,[s("mi",null,"R"),s("mi",null,"b"),s("mi",null,"s")])]),s("annotation",{encoding:"application/x-tex"},"R^{s}, point \\ index = b, point \\ skeleton = R_{b}^{s}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s")])])])])])])])]),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal"},"in"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mspace"}," "),s("span",{class:"mord mathnormal"},"in"),s("span",{class:"mord mathnormal"},"d"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord mathnormal"},"x"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.8889em","vertical-align":"-0.1944em"}}),s("span",{class:"mord mathnormal"},"b"),s("span",{class:"mpunct"},","),s("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal"},"in"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mspace"}," "),s("span",{class:"mord mathnormal"},"s"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.03148em"}},"k"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord mathnormal",style:{"margin-right":"0.01968em"}},"l"),s("span",{class:"mord mathnormal"},"e"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.9664em","vertical-align":"-0.2831em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-2.4169em","margin-left":"-0.0077em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"b")])])]),s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"s")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2831em"}},[s("span")])])])])])])])]),a(" 作为LLM的点扩展请求，其中 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("msup",null,[s("mi",null,"T"),s("mrow",null,[s("mi",null,"p"),s("mi",null,"e")])])]),s("annotation",{encoding:"application/x-tex"},"T^{pe}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6833em"}}),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"T"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.6644em"}},[s("span",{style:{top:"-3.063em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"p"),s("span",{class:"mord mathnormal mtight"},"e")])])])])])])])])])])]),a(" 是点扩展提示模板。最后，在完成所有的点之后，研究人员连接点扩展响应 "),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"{"),s("msubsup",null,[s("mi",null,"R"),s("mi",null,"b"),s("mrow",null,[s("mi",null,"p"),s("mi",null,"e")])]),s("msub",null,[s("mo",{stretchy:"false"},"}"),s("mrow",null,[s("mi",null,"b"),s("mo",null,"="),s("mn",null,"1"),s("mo",{separator:"true"},","),s("mo",null,"⋯"),s("mtext",null," "),s("mo",{separator:"true"},","),s("mi",null,"B")])])]),s("annotation",{encoding:"application/x-tex"},"\\{R^{pe}_{b}\\}_{b = 1, \\cdots, B}")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.0836em","vertical-align":"-0.3013em"}}),s("span",{class:"mopen"},"{"),s("span",{class:"mord"},[s("span",{class:"mord mathnormal",style:{"margin-right":"0.00773em"}},"R"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.7823em"}},[s("span",{style:{top:"-2.3987em","margin-left":"-0.0077em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"b")])])]),s("span",{style:{top:"-3.1809em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"p"),s("span",{class:"mord mathnormal mtight"},"e")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3013em"}},[s("span")])])])])]),s("span",{class:"mclose"},[s("span",{class:"mclose"},"}"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3361em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"b"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1"),s("span",{class:"mpunct mtight"},","),s("span",{class:"minner mtight"},"⋯"),s("span",{class:"mspace mtight",style:{"margin-right":"0.1952em"}}),s("span",{class:"mpunct mtight"},","),s("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.05017em"}},"B")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.2861em"}},[s("span")])])])])])])])]),a(" 来得到最终的答案。")],-1),V=n('<p>如下，Prompt 1和 Prompt 2显示了，研究人员当前实现使用的骨架提示模板图片和点扩展提示模板图片。</p><figure><img src="'+g+'" alt="图1.3 使用的骨架提示模板" width="600" tabindex="0" loading="lazy"><figcaption>图1.3 使用的骨架提示模板</figcaption></figure><figure><img src="'+u+'" alt="图1.4 使用的点扩展提示模板" width="600" tabindex="0" loading="lazy"><figcaption>图1.4 使用的点扩展提示模板</figcaption></figure><p>（3）骨架提示模板。</p><p>为了使输出的骨架简短且格式一致，以提高效率和便于提取要点，骨架提示模板（1）精确描述了任务，（2）使用了两个简单的示范，（3）提供了部分答案「1」为LLM继续写作。</p><p>（4）点扩展提示模板。</p><p>点扩展提示模板描述点扩展任务，并提供部分答案。研究人员还提供了指示「在1ー2个句子中非常简短地写出」的说明，以便LLM使答案保持简洁。</p><p>（5）并行点扩展。</p><p>对于只能访问API的专有模型可以发出多个并行的API调用。对于开源模型，让模型将点扩展请求作为批处理。</p><h3 id="_1-4-为什么sot降低了解码延迟" tabindex="-1"><a class="header-anchor" href="#_1-4-为什么sot降低了解码延迟" aria-hidden="true">#</a> 1.4 为什么SoT降低了解码延迟？</h3><p>首先要对SoT为什么能够带来显著的端到端加速有一个高层次的理解。为了简单起见，在这里集中讨论点扩展阶段。</p><p>具有并行API调用的模型。普通方法向服务器发送一个API请求，而 SoT 并行发送多个 API 请求以获得答案的不同部分。</p><p>根据经验，研究人员观察到，在论文中使用的API的延迟与响应中的token数呈正相关。如果请求数量没有达到速率限制，SoT显然会带来加速。</p><p>采用批量解码的开源模型。普通的方法只处理一个问题，并按顺序解码答案，而SoT处理多个点扩展请求和一批答案。</p><h2 id="_2-实验结论" tabindex="-1"><a class="header-anchor" href="#_2-实验结论" aria-hidden="true">#</a> 2 实验结论</h2><p>实验数据集：使用Vicuna-80数据集，它由跨越9个类别的80个问题组成，如编码、数学、写作、角色扮演等。</p><p>模型：对11个最近发布的模型进行SoT测试，其中包括9个开源模型和2个基于API的模型。</p><figure><img src="'+d+'" alt="图2.1 评估模型" width="680" tabindex="0" loading="lazy"><figcaption>图2.1 评估模型</figcaption></figure><h3 id="_2-1-效率评估" tabindex="-1"><a class="header-anchor" href="#_2-1-效率评估" aria-hidden="true">#</a> 2.1 效率评估</h3><p>（1）SoT减少不同模型上的端到端延迟</p><p>应用SoT后，11个模型中，有6个模型速度有2倍以上的提升（即LLaMA2-Chat-7B，LLaMA2-Chat-13B，Vicuna-7B V1.1，OpenChat-13B，Vicuna-33B V1.3，UltraLM-13B）。在ChatGPT-3.5，Vicuna-13B V1.3和Vicuna-7B V1.3上则有1.8倍以上的速度提升。但在StableVicuna-13B和Claude中，速度几乎没有提升。</p><p>如果排除数学和代码的问题类别，速度提升会较排除前略高。</p><figure><img src="'+x+'" alt="图2.2 不同模型加速效果" width="720" tabindex="0" loading="lazy"><figcaption>图2.2 不同模型加速效果</figcaption></figure><p>（2）SoT减少不同类别问题的端到端延迟</p><p>下图显示了每个问题类别在所有模型中的平均速度提升。那些SoT能够提供高质量答案的问题类别标记为绿色，不能的其他问题类别标记为红色。当前的SoT已经可以提升所有类别问题的速度。但对于那些SoT可以提供高质量答案的5个问题类别（即知识、常识、通用、角色扮演、虚拟情景），SoT可以将整体答案生成过程加速1.95倍-2.27倍。</p><figure><img src="'+y+'" alt="图2.3 不同问题类别加速效果" width="680" tabindex="0" loading="lazy"><figcaption>图2.3 不同问题类别加速效果</figcaption></figure><p>（3） SoT和正常生成的延迟对比</p><p>下图显示了模型正常生成和SoT生成的绝对延迟的比较。与正常生成相比，应用SoT的模型生成的速度提升是显而易见的。而<strong>解码阶段</strong>是内容生成端到端延迟的主要原因。因此，尽管SoT在骨架阶段比正常生成具有较高的预填充延迟，但这对总体延迟和总体速度提升几乎没有影响。</p><figure><img src="'+_+'" alt="图2.4 不同方法加速效果" width="720" tabindex="0" loading="lazy"><figcaption>图2.4 不同方法加速效果</figcaption></figure><h3 id="_2-2-质量评估" tabindex="-1"><a class="header-anchor" href="#_2-2-质量评估" aria-hidden="true">#</a> 2.2 质量评估</h3><p>为了比较正常的顺序生成（以下简称为正常）和SoT生成的答案质量，研究采用了两个基于LLM的评估框架: FastChat和LLMZoo。评估过程是向LLM评判器（本研究中为ChatGPT-3.5）展示一个问题和一对答案（由正常和SoT生成），并询问其偏好。回答可能是SoT的答案胜出、与正常答案并列、输给正常答案。</p><p>（1）整体质量：</p><p>下图显示了使用FastChat和LLMZoo两个指标下使用SOT的模型在所有问题下的赢/平/输率。在SoT严格优于基线时，两个指标之间存在差异(49.0% vs.10.4%)。但这两个指标都认为，在超过76%的情况下，SoT并不比基线（正常生成）差。对于FastChat指标，研究人员还展示了排除数学和编码问题（SoT不适用于这些问题，请参见3.2.2节）的比率：在超过90％的情况下，SoT与基准相当。这表明SoT的答案保持着良好的质量。</p><figure><img src="'+f+'" alt="图2.5 生成答案质量比较" width="680" tabindex="0" loading="lazy"><figcaption>图2.5 生成答案质量比较</figcaption></figure><p>（2）SOT在不同类别问题上的表现</p><p>下图计算了所有问题类别的净胜率（胜率-败率）。LLMZoo指标下SoT的质量比FastChat的更好。但不论在哪个框架指标下，SoT在泛型、常识、知识、角色扮演和反事实方面的表现都相对较好，而在写作、费米问题、数学和编码方面表现相对较差。</p><figure><img src="'+b+'" alt="图2.6 生成答案分类比较" width="720" tabindex="0" loading="lazy"><figcaption>图2.6 生成答案分类比较</figcaption></figure><h2 id="_3-局限性" tabindex="-1"><a class="header-anchor" href="#_3-局限性" aria-hidden="true">#</a> 3 局限性</h2><p>由于提示集的限制、现有LLM判断的偏差，以及LLM属性评价的内在困难，研究人员目前对LLM问题的答案质量的评价还远不全面。</p><p>对更可靠的质量评价而言，扩展提示集，以及用人工评价补充基于LLM的评价非常重要。</p><p>然而，目前的研究主要集中在揭示潜在的效率效益上，即通过重新思考现有LLM「全序列解码」的必要性，可以实现相当大的加速。</p><p>因此，研究人员在最后将对答案质量的更彻底的评估留给了未来的工作。</p><h2 id="_4-参考" tabindex="-1"><a class="header-anchor" href="#_4-参考" aria-hidden="true">#</a> 4 参考</h2>',43);function R(q,A){const m=t("ExternalLinkIcon"),e=t("PDF");return p(),r("div",null,[L,s("p",null,[s("a",S,[a("该文"),l(m)]),a(" 介绍了清华与微软合作提出的一种全新"),w,a("(SoT)，大大减少了LLM回答的延迟，并提升了回答的质量。")]),o(" more "),M,v,l(e,{url:"https://arxiv.org/pdf/2307.15337.pdf"}),k,z,P,B,V])}const F=i(T,[["render",R],["__file","SoT.html.vue"]]);export{F as default};
