const e=JSON.parse('{"key":"v-1f54a3f4","path":"/zh/posts/llm/GPT.html","title":"GPT论文分享：Improving Language Understanding by Generative Pre-Training","lang":"zh-CN","frontmatter":{"author":"最后的开神-wkyc","icon":"pen-to-square","date":"2023-06-15T00:00:00.000Z","shortTitle":"GPT论文分享","category":["语言模型"],"tag":["模型","深度学习"],"description":"GPT论文分享：Improving Language Understanding by Generative Pre-Training 作者证明了通过在大量未标注文本上对语言模型进行生成式预训练，然后在每个特定任务上进行歧视性微调，可以在这些任务上实现巨大收益。与以前的方法相比，他们在微调期间利用面向任务的输入转换来实现有效的转移，同时对模型架构所需的更改最小。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/GPT.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"GPT论文分享：Improving Language Understanding by Generative Pre-Training"}],["meta",{"property":"og:description","content":"GPT论文分享：Improving Language Understanding by Generative Pre-Training 作者证明了通过在大量未标注文本上对语言模型进行生成式预训练，然后在每个特定任务上进行歧视性微调，可以在这些任务上实现巨大收益。与以前的方法相比，他们在微调期间利用面向任务的输入转换来实现有效的转移，同时对模型架构所需的更改最小。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-21T03:03:23.000Z"}],["meta",{"property":"article:author","content":"最后的开神-wkyc"}],["meta",{"property":"article:tag","content":"模型"}],["meta",{"property":"article:tag","content":"深度学习"}],["meta",{"property":"article:published_time","content":"2023-06-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-21T03:03:23.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"GPT论文分享：Improving Language Understanding by Generative Pre-Training\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-15T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-21T03:03:23.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"最后的开神-wkyc\\"}]}"]]},"headers":[{"level":2,"title":"1 模型架构","slug":"_1-模型架构","link":"#_1-模型架构","children":[]},{"level":2,"title":"2 训练框架","slug":"_2-训练框架","link":"#_2-训练框架","children":[{"level":3,"title":"2.1 无监督预训练","slug":"_2-1-无监督预训练","link":"#_2-1-无监督预训练","children":[]},{"level":3,"title":"2.2 监督微调","slug":"_2-2-监督微调","link":"#_2-2-监督微调","children":[]}]}],"git":{"createdTime":1686823822000,"updatedTime":1687316603000,"contributors":[{"name":"Liu Xiao","email":"42756849+liuxiaocs7@users.noreply.github.com","commits":2},{"name":"RankKCodeTalker","email":"93417472+RankKCodeTalker@users.noreply.github.com","commits":2},{"name":"liuxiao","email":"liuxiao2103@qq.com","commits":1}]},"readingTime":{"minutes":1.87,"words":560},"filePathRelative":"zh/posts/llm/GPT.md","localizedDate":"2023年6月15日","excerpt":"<h1> GPT论文分享：Improving Language Understanding by Generative Pre-Training</h1>\\n<p>作者证明了通过在大量未标注文本上对语言模型进行生成式预训练，然后在每个特定任务上进行歧视性微调，可以在这些任务上实现巨大收益。与以前的方法相比，他们在微调期间利用面向任务的输入转换来实现有效的转移，同时对模型架构所需的更改最小。</p>\\n","autoDesc":true}');export{e as data};
