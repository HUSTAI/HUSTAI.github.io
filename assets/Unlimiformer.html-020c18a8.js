const e=JSON.parse('{"key":"v-618590a0","path":"/zh/posts/llm/Unlimiformer.html","title":"Unlimiformer 介绍","lang":"zh-CN","frontmatter":{"author":"研究生鱼皮-yjf","icon":"pen-to-square","date":"2023-06-14T00:00:00.000Z","shortTitle":"Unlimiformer介绍","category":["语言模型"],"tag":["摘要","Transformer","机器学习"],"description":"Unlimiformer 介绍 上海人工智能实验室联合商汤科技共同提出一种新的 UniFormer（Unified Transformer）框架， 它能够将卷积与自注意力的优点通过 Transformer 进行无缝集成。UniFormer 模块的相关性聚合在浅层与深层分别配备了局部全局token，能够同时解决冗余与依赖问题，实现了高效的特征学习。","head":[["meta",{"property":"og:url","content":"https://github.com/HUSTAI/HUSTAI.github.io/zh/posts/llm/Unlimiformer.html"}],["meta",{"property":"og:site_name","content":"知识分享"}],["meta",{"property":"og:title","content":"Unlimiformer 介绍"}],["meta",{"property":"og:description","content":"Unlimiformer 介绍 上海人工智能实验室联合商汤科技共同提出一种新的 UniFormer（Unified Transformer）框架， 它能够将卷积与自注意力的优点通过 Transformer 进行无缝集成。UniFormer 模块的相关性聚合在浅层与深层分别配备了局部全局token，能够同时解决冗余与依赖问题，实现了高效的特征学习。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-06-30T08:23:27.000Z"}],["meta",{"property":"article:author","content":"研究生鱼皮-yjf"}],["meta",{"property":"article:tag","content":"摘要"}],["meta",{"property":"article:tag","content":"Transformer"}],["meta",{"property":"article:tag","content":"机器学习"}],["meta",{"property":"article:published_time","content":"2023-06-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-06-30T08:23:27.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Unlimiformer 介绍\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-06-14T00:00:00.000Z\\",\\"dateModified\\":\\"2023-06-30T08:23:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"研究生鱼皮-yjf\\"}]}"]]},"headers":[{"level":2,"title":"1 问题提出","slug":"_1-问题提出","link":"#_1-问题提出","children":[]},{"level":2,"title":"2 Unlimiformer技术原理","slug":"_2-unlimiformer技术原理","link":"#_2-unlimiformer技术原理","children":[{"level":3,"title":"2.1 Unlimiformer编码","slug":"_2-1-unlimiformer编码","link":"#_2-1-unlimiformer编码","children":[]},{"level":3,"title":"2.2 检索增强的交叉注意力机制","slug":"_2-2-检索增强的交叉注意力机制","link":"#_2-2-检索增强的交叉注意力机制","children":[]}]},{"level":2,"title":"3 实验结果","slug":"_3-实验结果","link":"#_3-实验结果","children":[{"level":3,"title":"3.1 长文档摘要","slug":"_3-1-长文档摘要","link":"#_3-1-长文档摘要","children":[]},{"level":3,"title":"3.2 书籍摘要","slug":"_3-2-书籍摘要","link":"#_3-2-书籍摘要","children":[]}]}],"git":{"createdTime":1686823822000,"updatedTime":1688113407000,"contributors":[{"name":"Liu Xiao","email":"42756849+liuxiaocs7@users.noreply.github.com","commits":4},{"name":"heiheiyoyo","email":"543425864@qq.com","commits":3}]},"readingTime":{"minutes":5.58,"words":1675},"filePathRelative":"zh/posts/llm/Unlimiformer.md","localizedDate":"2023年6月14日","excerpt":"<h1> Unlimiformer 介绍</h1>\\n<p>上海人工智能实验室联合商汤科技共同提出一种新的 UniFormer（Unified Transformer）框架， 它能够将卷积与自注意力的优点通过 Transformer 进行无缝集成。UniFormer 模块的相关性聚合在浅层与深层分别配备了局部全局token，能够同时解决冗余与依赖问题，实现了高效的特征学习。</p>\\n","autoDesc":true}');export{e as data};
