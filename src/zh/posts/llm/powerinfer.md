---
author: 最后的开神-wkyc
icon: pen-to-square
date: 2023-12-26
category:
  - 语言模型
tag:
  - Llama
  - 推理
# sticky: 10
---

# PowerInfer：消费级显卡运行大语言模型

上海交大 IPADS 实验室推出的开源推理框架 PowerInfer，让大模型推理速度加快了 11 倍。而且不用量化，就用 FP16 精度，也能让 40B 模型在个人电脑上运行；如果加入量化，2080 Ti 也能流畅运行 70B 模型。

<!-- more -->

论文标题：PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU
论文链接：https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf
代码链接：https://github.com/SJTU-IPADS/PowerInfer

<PDF url="https://arxiv.org/pdf/2312.12456.pdf"/>

---

结合大模型的独特特征，通过 CPU 与 GPU 间的混合计算，PowerInfer 能够在显存有限的个人电脑上实现快速推理。相比于 llama.cpp，PowerInfer 实现了高达 11 倍的加速，让 40B 模型也能在个人电脑上一秒能输出十个 token。PowerInfer 一经发布就引起热烈反响，不到 24 小时就获得了 500+ 星标，其中还有一颗来自 llama.cpp 的作者 Gerganov。

## 1 推理速度最高11倍

在搭载 x86 CPU 和 NVIDIA GPU 的消费级硬件平台上，PowerInfer 以参数量从 7B 到 175B 的一系列 LLM 模型为基准，对 PowerInfer 的端到端推理速度进行了测试，并和同平台上性能最好的推理框架 llama.cpp 进行了对比。 
对于 FP16 精度的模型，在搭载了 13 代 Intel Core i9 和单张 RTX 4090 的高端 PC(PC-High) 上，PowerInfer 平均实现了 7.23 倍的速度提升，其中在 Falcon 40B 上实现了高达 11.69 倍的速度提升。 在所有测试用例上，PowerInfer 平均达到了 8.32 tokens/s，在 OPT 30B 和 Falcon 40B 上最高分别达到 16.06 tokens/s 和 12.94 tokens/s。 借助 PowerInfer，当今的消费级平台可以流畅运行 30-40B 级别的 LLM，并以可以接受的速度运行 70B 级别的 LLM。
模型量化是端侧 LLM 推理非常常用的技术，PowerInfer 也支持了 INT4 量化模型的推理。PowerInfer 分别在高端 PC(PC-High) 和搭载单张 RTX 2080Ti 的中低端 PC(PC-Low) 上测试了一系列 INT4 量化模型的推理速度。 
在 PC-High 上，PowerInfer 能够高速运行 40-70B 规模的模型，最高达到了 29.09 tokens/s 的推理速度，并且实现了平均 2.89 倍，最高 4.28 倍的速度提升。 同时，在消费级硬件上运行 OPT-175B 这种规模的模型也成为可能。在 PC-Low 这种中低端 PC 上，PowerInfer 可以流畅运行 30-70B 规模的模型，并实现平均 5.01 倍，最高 8.00 倍的速度提升，这主要得益于 INT4 量化后模型大部分热神经元得以放置在显存中。

![示意图](/assets/images/llm/powerinfer_5.png "图1.1 PowerInfer在INT4量化模型中的推理速度，纵坐标为加速比，每根柱状图上标注的数字代表了每秒钟能够生成的token数量")

最后，PowerInfer 对比了 PC-High 上运行 PowerInfer 相比于云端顶级计算卡 A100 运行 SOTA 框架 vLLM 的端到端推理速度，测试模型为 FP16 精度的 OPT-30B 和 Falcon-40B(ReLU)。当输入长度为 64 时，PowerInfer 对 A100 的速度差距从 93%-94% 缩小到了 28%-29%；在输入长度为 1 的纯生成场景中，这一差距会被进一步缩小到低至 18%。这代表着 PowerInfer 借助稀疏激活和 CPU/GPU 混合推理，极大地弥合了消费级显卡到顶尖服务端计算卡的推理速度差距。

![示意图](/assets/images/llm/powerinfer_6.png "图1.2 PowerInfer在4090上与vLLM在A100的性能对比")

## 2 充分利用模型和硬件特点
PowerInfer 实现高速推理的秘诀，在于充分利用了稠密模型存在的高局部性的稀疏激活，并与 CPU 和 GPU 的运算特点进行了充分结合。

### 2.1 何谓“稀疏激活”？

最近 Mixtral MoE 大模型引爆了整个 AI 圈，稀疏模型重新进入大家的视野。一个有趣的事实是：像 OPT、LLaMA(ReLU) 这样被视为稠密模型的 LLM，同样存在稀疏激活的特征。

### 2.2 什么是稠密模型的稀疏激活呢？

和 MoE 模型中一个输入 token 只需要激活 FFN layer 其中一个或者两个专家模块类似，以OPT模型的稠密 FFN 层为例，只需要激活一小部分（实验显示约 10%）神经元即可保证输出的正确性。其他的神经元虽然参与了计算，但并没有对输出产生明显贡献。

![示意图](/assets/images/llm/powerinfer_1.png "图2.1 MoE 模型（左）和稠密模型的稀疏激活（右）")

MoE 模型可以在专家FFN 层之前通过路由模块将输入分发给其中一个或者两个专家进行计算，那么稠密模型中的稀疏激活又该如何路由或者在计算之前就知道哪些专家神经元会对结果产生贡献呢？
答案是为稠密模型增加路由预测模块。在模型开始服务前，PowerInfer 首先会对模型进行离线分析，通过将模型在通用数据集中进行推理获取每一层输入与激活神经元之间的对应关系，进而为稠密模型每一层训练一个小的预测路由模块来预测每一个输入会激活的神经元，只计算路由激活的神经元（专家）。
在多个下游任务的测试中，PowerInfer 的路由模块几乎没有引入额外的精度损失。

### 2.3 稀疏激活带来的推理局部性

稀疏激活的另一个有趣事实是，尽管对于不同的输入 token，激活的神经元分布存在差异；但如果在足够多的数据上进行推理，并将每次激活的分布叠加，PowerInfer 发现少部分神经元总体上被激活的概率更高。也就是说，统计意义上大模型神经元的激活符合 Power Law 分布（Power Law 分布是一种统计规律，表示少数事件的发生频率远高于大量其他事件）。
如下图 (a) 所示，对于 OPT-30B 和 LLaMA(ReGLU)-70B 两个模型里的某一层 FFN 网络，统计意义上 26% 和 43% 的神经元分别贡献了 80% 的激活。而在整个模型的尺度上，如下图 (b) 所示，17% 和 26% 的神经元贡献了 80% 的激活。

![示意图](/assets/images/llm/powerinfer_2.png "图2.2 稠密模型的幂律定律")

因此，当只考虑对最终激活有贡献的运算时，LLM 具有推理局部性：对权重的访问倾向于集中在一定的区域，而不是均匀分布在所有的神经元上。在推理运算中它显现为程序的局部性：对内存空间的访问倾向于集中在一定的区域，而不是均匀分布在整个内存空间。
在常见的个人电脑中，GPU 具有较少的显存和更强的计算能力，适合处理频繁访问且计算强度高的任务；而 CPU 拥有更大的内存容量但相对较弱的算力，适合处理少量访问且计算强度低的任务。因此，理想情况下，一小部分经常访问的神经元应该存储在显存中，相比之下更大、访问频率更低的神经元更适合存储在内存中，由 CPU 进行计算。这启发了 PowerInfer 基于局部性特征进行 CPU/GPU 混合推理系统的设计。

### 2.4 CPU/GPU混合推理设计

根据上述神经元的 Power Law 和由此产生的局部性，PowerInfer 通过提前静态分析每一个神经元的冷热性，将少量的热神经元加载在 GPU 显存上，剩余的冷神经元加载到 CPU 的内存中。
以神经元为粒度的模型混合加载，会出现一层内有些神经元在 GPU 上，有些神经元在 CPU 上。为此，PowerInfer 设计了细粒度的 CPU/GPU 混合推理引擎。以下图为例，对于某一层的输入，PowerInfer 会首先预测该输入会激活神经元为 3，4，5。然后 CPU、GPU 会分别根据预测信息，执行位于其内存中的神经元的计算。具体以下图的例子来说，CPU 上会计算第四个神经元，GPU 上会计算第三个、第五个神经元，然后再 GPU 上对两边的计算结果进行合并。

![示意图](/assets/images/llm/powerinfer_3.png "图2.3 PowerInfer混合计算的方式")

### 2.5 PowerInfer的整体架构

总体而言，PowerInfer 利用基于稠密模型的稀疏激活及其引入的局部性特性，开发出了一种创新的 CPU/GPU 混合推理引擎。
在接入一个大型语言模型（LLM）时，PowerInfer 首先在离线阶段对模型的预测路由模块进行训练，并深入分析模型的激活特征。同时，结合目标硬件的带宽和容量等关键信息，计算出最佳的神经元放置策略。在此基础上，PowerInfer 会根据这些计算结果，将神经元优化地分布在内存或显存中。在在线推理阶段，CPU 和 GPU 分别处理存储在其内存中的神经元，随后在 GPU 上对这些独立计算的结果进行高效合并。

![示意图](/assets/images/llm/powerinfer_7.png "图2.4 PowerInfer整体架构图")

## 3 总结与展望
对于端侧用户而言，PowerInfer 的高效推理框架打开了新的可能性。
首先，它使得个人电脑用户能够在本地运行先进的大型语言模型，而无需昂贵的专业硬件。这不仅促进了人工智能应用的普及化，也为爱好者、研究人员和小型企业提供了前所未有的机会。
在云端部署方面，PowerInfer 同样存在巨大的潜力。现有的云端 CPU 也有强大的 AMX 计算单元支持，通过利用 CPU、GPU 间的异构特征，可以乐观地认为 PowerInfer 能够使用更少的高端计算卡，做到更高的服务吞吐。
